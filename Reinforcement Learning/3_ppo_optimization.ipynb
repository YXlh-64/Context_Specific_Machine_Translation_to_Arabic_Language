{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run setup from config notebook\n",
    "%run 0_config_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b14ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import json\n",
    "\n",
    "# Import LoRA/PEFT for safer training\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "print(\"üöÄ PPO Optimization - HPC Optimized\")\n",
    "print(f\"   LoRA enabled: {USE_LORA}\")\n",
    "print(f\"   KL Penalty: {KL_PENALTY_COEF}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ef7032",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacef9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading models...\\n\")\n",
    "\n",
    "# Model name (HuggingFace Hub)\n",
    "model_name = \"ModelSpace/GemmaX2-28-9B-v0.1\"\n",
    "\n",
    "# Configure memory allocation for multi-GPU\n",
    "if NUM_GPUS > 1:\n",
    "    max_memory = {i: GPU_MEMORY_PER_DEVICE for i in range(NUM_GPUS)}\n",
    "    max_memory[\"cpu\"] = \"32GB\"\n",
    "else:\n",
    "    max_memory = None\n",
    "\n",
    "# 1. Load SFT model (policy model for PPO)\n",
    "print(f\"Loading SFT model: {model_name}\")\n",
    "\n",
    "policy_tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "if policy_tokenizer.pad_token is None:\n",
    "    policy_tokenizer.pad_token = policy_tokenizer.eos_token\n",
    "policy_tokenizer.padding_side = \"left\"\n",
    "\n",
    "# ===========================\n",
    "# LoRA CONFIGURATION (RECOMMENDED)\n",
    "# ===========================\n",
    "if USE_LORA:\n",
    "    print(\"üõ°Ô∏è Using LoRA - Original SFT weights will be PRESERVED\")\n",
    "    \n",
    "    # Configure LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        target_modules=LORA_TARGET_MODULES,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    \n",
    "    # Load base model first\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        max_memory=max_memory,\n",
    "        attn_implementation=ATTN_IMPLEMENTATION if USE_FLASH_ATTENTION else None\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA adapters (original weights stay FROZEN)\n",
    "    base_model = get_peft_model(base_model, lora_config)\n",
    "    base_model.print_trainable_parameters()\n",
    "    \n",
    "    # Wrap with value head for PPO\n",
    "    policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "        base_model,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        is_trainable=True\n",
    "    )\n",
    "    print(\"‚úì SFT model loaded with LoRA adapters\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Full fine-tuning mode - SFT weights WILL be modified\")\n",
    "    \n",
    "    # Load with value head for PPO (full fine-tuning)\n",
    "    policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        max_memory=max_memory,\n",
    "        attn_implementation=ATTN_IMPLEMENTATION if USE_FLASH_ATTENTION else None\n",
    "    )\n",
    "    print(\"‚úì SFT model loaded (full fine-tuning)\")\n",
    "\n",
    "# 2. Load reference model (for KL penalty - always frozen)\n",
    "print(f\"\\nLoading reference model (frozen) for KL computation...\")\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    max_memory=max_memory\n",
    ")\n",
    "\n",
    "# Freeze reference model completely\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "ref_model.eval()\n",
    "print(\"‚úì Reference model loaded (frozen)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ef710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load reward model\n",
    "print(f\"\\nLoading reward model from {REWARD_MODEL_COLD_START}...\")\n",
    "\n",
    "# Load tokenizer\n",
    "rm_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_COLD_START)\n",
    "\n",
    "# Load base model\n",
    "rm_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    REWARD_BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Recreate reward model structure (from notebook 2)\n",
    "from torch import nn\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, base_model, hidden_dim=256, head_type='mlp'):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.head_type = head_type\n",
    "        self.hidden_size = base_model.config.hidden_size\n",
    "        \n",
    "        if head_type == 'linear':\n",
    "            self.reward_head = nn.Linear(self.hidden_size, 1)\n",
    "        elif head_type == 'mlp':\n",
    "            self.reward_head = nn.Sequential(\n",
    "                nn.Linear(self.hidden_size, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(hidden_dim, 1)\n",
    "            )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        pooled = hidden_states[torch.arange(batch_size), sequence_lengths]\n",
    "        reward = self.reward_head(pooled)\n",
    "        return reward.squeeze(-1)\n",
    "\n",
    "# Create and load weights\n",
    "reward_model = RewardModel(\n",
    "    base_model=rm_base_model,\n",
    "    hidden_dim=RM_HIDDEN_DIM,\n",
    "    head_type=RM_HEAD_TYPE\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(\n",
    "    REWARD_MODEL_COLD_START / \"reward_model.pt\",\n",
    "    map_location='cpu'\n",
    ")\n",
    "reward_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "reward_model.eval()\n",
    "\n",
    "# Freeze reward model\n",
    "for param in reward_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"‚úì Reward model loaded (frozen)\")\n",
    "print(f\"\\nAll models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acac98e",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6605aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training prompts (no parallel corpus needed)\n",
    "print(\"Loading training prompts...\")\n",
    "\n",
    "try:\n",
    "    all_data = load_test_prompts(TEST_PROMPTS)\n",
    "    print(f\"Loaded {len(all_data)} test prompts\")\n",
    "except:\n",
    "    # Create sample prompts if file doesn't exist\n",
    "    all_data = [\n",
    "        {\"source\": \"Hello, how are you?\", \"source_lang\": \"en\"},\n",
    "        {\"source\": \"Good morning.\", \"source_lang\": \"en\"},\n",
    "        {\"source\": \"Thank you very much.\", \"source_lang\": \"en\"},\n",
    "        {\"source\": \"Bonjour, comment allez-vous?\", \"source_lang\": \"fr\"},\n",
    "        {\"source\": \"Merci beaucoup.\", \"source_lang\": \"fr\"},\n",
    "    ] * 1000  # Replicate for training\n",
    "    print(f\"Created {len(all_data)} sample prompts\")\n",
    "\n",
    "# Create dataset of prompts for PPO\n",
    "prompts = []\n",
    "for item in all_data[:5000]:  # Adjust size based on resources\n",
    "    prompt = format_translation_prompt(item['source'], item['source_lang'])\n",
    "    prompts.append({\n",
    "        'query': prompt,\n",
    "        'source': item['source'],\n",
    "        'source_lang': item['source_lang']\n",
    "    })\n",
    "\n",
    "# Create HuggingFace dataset\n",
    "dataset = Dataset.from_list(prompts)\n",
    "\n",
    "print(f\"Loaded {len(dataset)} training prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a02bcd1",
   "metadata": {},
   "source": [
    "## PPO Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO configuration - HPC Optimized with Safety Measures\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=PPO_LEARNING_RATE,\n",
    "    batch_size=PPO_BATCH_SIZE,\n",
    "    mini_batch_size=PPO_MINI_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=PPO_GRADIENT_ACCUMULATION_STEPS,\n",
    "    ppo_epochs=PPO_EPOCHS,\n",
    "    \n",
    "    # KL penalty to stay close to reference model (INCREASED for safety)\n",
    "    init_kl_coef=KL_PENALTY_COEF,\n",
    "    target_kl=KL_TARGET,\n",
    "    \n",
    "    # PPO clipping\n",
    "    cliprange=CLIP_RANGE,\n",
    "    cliprange_value=VALUE_CLIP_RANGE,\n",
    "    \n",
    "    # GAE parameters\n",
    "    vf_coef=0.1,\n",
    "    gamma=GAMMA,\n",
    "    lam=GAE_LAMBDA,\n",
    "    \n",
    "    # Other settings\n",
    "    seed=SEED,\n",
    "    log_with=\"wandb\" if USE_WANDB else None,\n",
    "    tracker_project_name=WANDB_PROJECT,\n",
    "    tracker_kwargs={\"name\": f\"ppo-coldstart-{'lora' if USE_LORA else 'full'}\"},\n",
    "    \n",
    "    # Optimization settings for HPC\n",
    "    optimize_cuda_cache=True,\n",
    ")\n",
    "\n",
    "print(\"PPO Configuration (HPC Optimized):\")\n",
    "print(f\"  Learning rate: {ppo_config.learning_rate}\")\n",
    "print(f\"  Batch size: {ppo_config.batch_size}\")\n",
    "print(f\"  Mini-batch size: {ppo_config.mini_batch_size}\")\n",
    "print(f\"  KL penalty (init): {ppo_config.init_kl_coef}\")\n",
    "print(f\"  KL target: {ppo_config.target_kl}\")\n",
    "print(f\"  Clip range: {ppo_config.cliprange}\")\n",
    "print(f\"  LoRA: {USE_LORA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37897813",
   "metadata": {},
   "source": [
    "## Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc7ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(source_texts, translations):\n",
    "    \"\"\"\n",
    "    Compute rewards for generated translations using the reward model.\n",
    "    \n",
    "    Args:\n",
    "        source_texts: List of source texts\n",
    "        translations: List of generated translations\n",
    "    \n",
    "    Returns:\n",
    "        List of reward scores (tensors)\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for source, translation in zip(source_texts, translations):\n",
    "        # Format for reward model\n",
    "        text = f\"Source: {source}\\nTranslation: {translation}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = rm_tokenizer(\n",
    "            text,\n",
    "            max_length=RM_MAX_LENGTH,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(reward_model.base_model.device)\n",
    "        \n",
    "        # Get reward\n",
    "        with torch.no_grad():\n",
    "            reward = reward_model(\n",
    "                inputs['input_ids'],\n",
    "                inputs['attention_mask']\n",
    "            )\n",
    "        \n",
    "        rewards.append(reward.cpu())\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "print(\"Reward function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e013fb0f",
   "metadata": {},
   "source": [
    "## Initialize PPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5814993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PPO trainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=policy_model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=policy_tokenizer,\n",
    "    dataset=dataset,\n",
    "    data_collator=None,\n",
    ")\n",
    "\n",
    "print(\"PPO Trainer initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87891d6d",
   "metadata": {},
   "source": [
    "## PPO Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39849c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation settings\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": PPO_MAX_NEW_TOKENS,\n",
    "    \"temperature\": PPO_TEMPERATURE,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": policy_tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": policy_tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "print(\"üöÄ Starting PPO training (HPC Optimized with Safety Measures)...\\n\")\n",
    "print(f\"Total steps: {PPO_STEPS}\")\n",
    "print(f\"Max KL threshold: {MAX_KL_THRESHOLD} (will stop if exceeded)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Track metrics for early stopping\n",
    "best_reward = float('-inf')\n",
    "initial_kl = None\n",
    "rewards_history = []\n",
    "kl_history = []\n",
    "\n",
    "# Training loop with safety measures\n",
    "for step, batch in enumerate(tqdm(ppo_trainer.dataloader, total=PPO_STEPS)):\n",
    "    if step >= PPO_STEPS:\n",
    "        break\n",
    "    \n",
    "    # Get queries (prompts)\n",
    "    query_tensors = batch['input_ids']\n",
    "    \n",
    "    # Generate responses (translations)\n",
    "    response_tensors = ppo_trainer.generate(\n",
    "        query_tensors,\n",
    "        return_prompt=False,\n",
    "        **generation_kwargs\n",
    "    )\n",
    "    \n",
    "    # Decode responses\n",
    "    batch_texts = policy_tokenizer.batch_decode(query_tensors, skip_special_tokens=True)\n",
    "    response_texts = policy_tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
    "    \n",
    "    # Extract source texts (for reward computation)\n",
    "    source_texts = []\n",
    "    for text in batch_texts:\n",
    "        if \"English text to Arabic:\" in text:\n",
    "            source = text.split(\"English text to Arabic:\")[1].split(\"\\n\\nArabic translation:\")[0].strip()\n",
    "        elif \"French text to Arabic:\" in text:\n",
    "            source = text.split(\"French text to Arabic:\")[1].split(\"\\n\\nArabic translation:\")[0].strip()\n",
    "        else:\n",
    "            source = text\n",
    "        source_texts.append(source)\n",
    "    \n",
    "    # Compute rewards\n",
    "    rewards = compute_reward(source_texts, response_texts)\n",
    "    \n",
    "    # Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    \n",
    "    # Track metrics\n",
    "    current_kl = stats['objective/kl']\n",
    "    current_reward = torch.tensor(rewards).mean().item()\n",
    "    rewards_history.append(current_reward)\n",
    "    kl_history.append(current_kl)\n",
    "    \n",
    "    if initial_kl is None:\n",
    "        initial_kl = current_kl\n",
    "    \n",
    "    # ===========================\n",
    "    # SAFETY CHECK: KL Divergence\n",
    "    # ===========================\n",
    "    if current_kl > MAX_KL_THRESHOLD:\n",
    "        print(f\"\\n‚ö†Ô∏è SAFETY STOP: KL divergence ({current_kl:.4f}) exceeded threshold ({MAX_KL_THRESHOLD})\")\n",
    "        print(\"   Model is drifting too far from SFT. Stopping to preserve quality.\")\n",
    "        break\n",
    "    \n",
    "    # Log statistics\n",
    "    if step % 10 == 0:\n",
    "        ppo_trainer.log_stats(\n",
    "            stats,\n",
    "            batch,\n",
    "            rewards,\n",
    "            columns_to_log=[\"query\", \"response\"]\n",
    "        )\n",
    "    \n",
    "    # Print progress\n",
    "    if step % 50 == 0:\n",
    "        mean_reward = current_reward\n",
    "        print(f\"\\nStep {step}:\")\n",
    "        print(f\"  Mean reward: {mean_reward:.4f}\")\n",
    "        print(f\"  Mean KL: {current_kl:.4f} (threshold: {MAX_KL_THRESHOLD})\")\n",
    "        print(f\"  Policy loss: {stats['ppo/loss/policy']:.4f}\")\n",
    "        print(f\"  Value loss: {stats['ppo/loss/value']:.4f}\")\n",
    "        \n",
    "        # KL health indicator\n",
    "        kl_ratio = current_kl / MAX_KL_THRESHOLD\n",
    "        if kl_ratio < 0.5:\n",
    "            kl_status = \"‚úÖ Healthy\"\n",
    "        elif kl_ratio < 0.8:\n",
    "            kl_status = \"‚ö†Ô∏è Moderate\"\n",
    "        else:\n",
    "            kl_status = \"üî¥ High - approaching limit\"\n",
    "        print(f\"  KL status: {kl_status}\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(f\"\\n  Sample translation:\")\n",
    "        print(f\"  Source: {source_texts[0][:100]}...\")\n",
    "        print(f\"  Generated: {response_texts[0][:100]}...\")\n",
    "        print(f\"  Reward: {rewards[0].item():.4f}\")\n",
    "    \n",
    "    # Save checkpoint periodically\n",
    "    if step > 0 and step % CHECKPOINT_EVERY_N_STEPS == 0:\n",
    "        checkpoint_path = PPO_MODEL_COLD_START / f\"checkpoint-{step}\"\n",
    "        checkpoint_path.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        if USE_LORA:\n",
    "            # Save only LoRA adapters (small file)\n",
    "            policy_model.pretrained_model.save_pretrained(checkpoint_path)\n",
    "        else:\n",
    "            ppo_trainer.model.save_pretrained(checkpoint_path)\n",
    "        \n",
    "        policy_tokenizer.save_pretrained(checkpoint_path)\n",
    "        print(f\"\\n‚úì Checkpoint saved to {checkpoint_path}\")\n",
    "        \n",
    "        # Save training metrics\n",
    "        metrics = {\n",
    "            'step': step,\n",
    "            'mean_reward': float(np.mean(rewards_history[-100:])),\n",
    "            'mean_kl': float(np.mean(kl_history[-100:])),\n",
    "            'max_kl_seen': float(max(kl_history))\n",
    "        }\n",
    "        with open(checkpoint_path / \"metrics.json\", 'w') as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ PPO training complete!\")\n",
    "print(f\"   Final mean reward: {np.mean(rewards_history[-50:]):.4f}\")\n",
    "print(f\"   Final mean KL: {np.mean(kl_history[-50:]):.4f}\")\n",
    "print(f\"   Max KL observed: {max(kl_history):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a344da",
   "metadata": {},
   "source": [
    "## Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d87b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final optimized model\n",
    "print(f\"Saving final model to {PPO_MODEL_COLD_START}...\")\n",
    "\n",
    "PPO_MODEL_COLD_START.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "if USE_LORA:\n",
    "    # Save LoRA adapters only (original SFT weights preserved)\n",
    "    policy_model.pretrained_model.save_pretrained(PPO_MODEL_COLD_START)\n",
    "    print(\"‚úì LoRA adapters saved (original SFT weights preserved)\")\n",
    "else:\n",
    "    # Save full model\n",
    "    ppo_trainer.model.save_pretrained(PPO_MODEL_COLD_START)\n",
    "    print(\"‚úì Full model saved\")\n",
    "\n",
    "policy_tokenizer.save_pretrained(PPO_MODEL_COLD_START)\n",
    "\n",
    "# Save training info\n",
    "training_info = {\n",
    "    'base_model': model_name,\n",
    "    'reward_model': str(REWARD_MODEL_COLD_START),\n",
    "    'ppo_steps': PPO_STEPS,\n",
    "    'steps_completed': step + 1,\n",
    "    'ppo_config': {\n",
    "        'learning_rate': PPO_LEARNING_RATE,\n",
    "        'kl_penalty': KL_PENALTY_COEF,\n",
    "        'max_kl_threshold': MAX_KL_THRESHOLD,\n",
    "        'clip_range': CLIP_RANGE\n",
    "    },\n",
    "    'lora_config': {\n",
    "        'enabled': USE_LORA,\n",
    "        'r': LORA_R if USE_LORA else None,\n",
    "        'alpha': LORA_ALPHA if USE_LORA else None,\n",
    "        'target_modules': LORA_TARGET_MODULES if USE_LORA else None\n",
    "    },\n",
    "    'final_metrics': {\n",
    "        'mean_reward': float(np.mean(rewards_history[-50:])) if rewards_history else 0,\n",
    "        'mean_kl': float(np.mean(kl_history[-50:])) if kl_history else 0,\n",
    "        'max_kl': float(max(kl_history)) if kl_history else 0\n",
    "    },\n",
    "    'stage': 'cold_start'\n",
    "}\n",
    "\n",
    "with open(PPO_MODEL_COLD_START / \"training_info.json\", 'w') as f:\n",
    "    json.dump(training_info, f, indent=2)\n",
    "\n",
    "print(\"‚úì Model saved successfully!\")\n",
    "print(f\"\\nPath: {PPO_MODEL_COLD_START}\")\n",
    "print(f\"LoRA adapters: {USE_LORA}\")\n",
    "print(f\"Original SFT weights preserved: {USE_LORA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c924142",
   "metadata": {},
   "source": [
    "## Test Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb175f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the optimized model\n",
    "print(\"Testing optimized model...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_samples = [\n",
    "    {\"text\": \"Hello, how are you today?\", \"lang\": \"en\"},\n",
    "    {\"text\": \"The weather is beautiful this morning.\", \"lang\": \"en\"},\n",
    "    {\"text\": \"Bonjour, comment allez-vous?\", \"lang\": \"fr\"},\n",
    "]\n",
    "\n",
    "ppo_trainer.model.eval()\n",
    "\n",
    "for i, sample in enumerate(test_samples, 1):\n",
    "    prompt = format_translation_prompt(sample['text'], sample['lang'])\n",
    "    \n",
    "    inputs = policy_tokenizer(prompt, return_tensors=\"pt\").to(ppo_trainer.model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = ppo_trainer.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            temperature=0.8,\n",
    "            do_sample=True,\n",
    "            pad_token_id=policy_tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    full_text = policy_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    translation = full_text.split(\"Arabic translation:\")[-1].strip()\n",
    "    \n",
    "    # Compute reward\n",
    "    reward = compute_reward([sample['text']], [translation])[0].item()\n",
    "    \n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Source ({sample['lang']}): {sample['text']}\")\n",
    "    print(f\"Translation: {translation}\")\n",
    "    print(f\"Reward: {reward:.4f}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad05695",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "Proceed to **notebook 4** for inference and user feedback collection."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
