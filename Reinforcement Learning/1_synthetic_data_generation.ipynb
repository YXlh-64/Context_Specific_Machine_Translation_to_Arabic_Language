{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c70776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/aya/anaconda3/lib/python3.13/site-packages (4.57.3)\n",
      "Requirement already satisfied: accelerate in /home/aya/anaconda3/lib/python3.13/site-packages (1.4.0)\n",
      "Requirement already satisfied: torch in /home/aya/anaconda3/lib/python3.13/site-packages (2.9.0)\n",
      "Requirement already satisfied: datasets in /home/aya/anaconda3/lib/python3.13/site-packages (3.0.0)\n",
      "Requirement already satisfied: trl in /home/aya/anaconda3/lib/python3.13/site-packages (0.11.0)\n",
      "Requirement already satisfied: peft in /home/aya/anaconda3/lib/python3.13/site-packages (0.11.1)\n",
      "Requirement already satisfied: bitsandbytes in /home/aya/anaconda3/lib/python3.13/site-packages (0.48.2)\n",
      "Requirement already satisfied: filelock in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: psutil in /home/aya/anaconda3/lib/python3.13/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: setuptools in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (78.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/aya/anaconda3/lib/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/aya/anaconda3/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /home/aya/anaconda3/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/aya/anaconda3/lib/python3.13/site-packages (from datasets) (3.11.10)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /home/aya/anaconda3/lib/python3.13/site-packages (from trl) (1.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp->datasets) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests->transformers) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /home/aya/anaconda3/lib/python3.13/site-packages (from tyro>=0.5.11->trl) (0.17.0)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from tyro>=0.5.11->trl) (4.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/aya/anaconda3/lib/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/aya/anaconda3/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /home/aya/anaconda3/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/aya/anaconda3/lib/python3.13/site-packages (from datasets) (3.11.10)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /home/aya/anaconda3/lib/python3.13/site-packages (from trl) (1.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp->datasets) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests->transformers) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /home/aya/anaconda3/lib/python3.13/site-packages (from tyro>=0.5.11->trl) (0.17.0)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from tyro>=0.5.11->trl) (4.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: comet-ml in /home/aya/anaconda3/lib/python3.13/site-packages (3.54.2)\n",
      "Requirement already satisfied: unbabel-comet in /home/aya/anaconda3/lib/python3.13/site-packages (2.2.7)\n",
      "Requirement already satisfied: bert-score in /home/aya/anaconda3/lib/python3.13/site-packages (0.3.13)\n",
      "Requirement already satisfied: sacrebleu in /home/aya/anaconda3/lib/python3.13/site-packages (2.5.1)\n",
      "Requirement already satisfied: dulwich!=0.20.33,>=0.20.6 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (0.24.10)\n",
      "Requirement already satisfied: everett<3.2.0,>=1.0.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from everett[ini]<3.2.0,>=1.0.1->comet-ml) (3.1.0)\n",
      "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (4.23.0)\n",
      "Requirement already satisfied: psutil>=5.6.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (5.9.0)\n",
      "Requirement already satisfied: python-box<7.0.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (6.1.0)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (2.32.5)\n",
      "Requirement already satisfied: rich>=13.3.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (14.2.0)\n",
      "Requirement already satisfied: semantic-version>=2.8.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (2.10.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.1.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (2.47.0)\n",
      "Requirement already satisfied: setuptools in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (78.1.0)\n",
      "Requirement already satisfied: simplejson in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (3.20.2)\n",
      "Requirement already satisfied: urllib3>=1.26.8 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (2.6.1)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (1.17.0)\n",
      "Requirement already satisfied: wurlitzer>=1.0.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (3.0.2)\n",
      "Requirement already satisfied: configobj in /home/aya/anaconda3/lib/python3.13/site-packages (from everett[ini]<3.2.0,>=1.0.1->comet-ml) (5.0.9)\n",
      "Requirement already satisfied: entmax<2.0,>=1.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (1.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (0.36.0)\n",
      "Requirement already satisfied: jsonargparse==3.13.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (3.13.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.20.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.4.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (2.2.3)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.24.4 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (4.25.8)\n",
      "Requirement already satisfied: pytorch-lightning<3.0.0,>=2.0.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (2.1.4)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.5.4 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (1.15.3)\n",
      "Requirement already satisfied: sentencepiece<0.3.0,>=0.2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (0.2.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (2.9.0)\n",
      "Requirement already satisfied: torchmetrics<0.11.0,>=0.10.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (0.10.3)\n",
      "Requirement already satisfied: transformers<5.0,>=4.17 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (4.57.3)\n",
      "Requirement already satisfied: PyYAML>=3.13 in /home/aya/anaconda3/lib/python3.13/site-packages (from jsonargparse==3.13.1->unbabel-comet) (6.0.2)\n",
      "Requirement already satisfied: portalocker in /home/aya/anaconda3/lib/python3.13/site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: regex in /home/aya/anaconda3/lib/python3.13/site-packages (from sacrebleu) (2024.11.6)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/aya/anaconda3/lib/python3.13/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in /home/aya/anaconda3/lib/python3.13/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /home/aya/anaconda3/lib/python3.13/site-packages (from sacrebleu) (5.3.0)\n",
      "Requirement already satisfied: filelock in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (24.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (1.2.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.15.2)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.7.0)\n",
      "Requirement already satisfied: comet-ml in /home/aya/anaconda3/lib/python3.13/site-packages (3.54.2)\n",
      "Requirement already satisfied: unbabel-comet in /home/aya/anaconda3/lib/python3.13/site-packages (2.2.7)\n",
      "Requirement already satisfied: bert-score in /home/aya/anaconda3/lib/python3.13/site-packages (0.3.13)\n",
      "Requirement already satisfied: sacrebleu in /home/aya/anaconda3/lib/python3.13/site-packages (2.5.1)\n",
      "Requirement already satisfied: dulwich!=0.20.33,>=0.20.6 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (0.24.10)\n",
      "Requirement already satisfied: everett<3.2.0,>=1.0.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from everett[ini]<3.2.0,>=1.0.1->comet-ml) (3.1.0)\n",
      "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (4.23.0)\n",
      "Requirement already satisfied: psutil>=5.6.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (5.9.0)\n",
      "Requirement already satisfied: python-box<7.0.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (6.1.0)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (2.32.5)\n",
      "Requirement already satisfied: rich>=13.3.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (14.2.0)\n",
      "Requirement already satisfied: semantic-version>=2.8.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (2.10.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.1.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (2.47.0)\n",
      "Requirement already satisfied: setuptools in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (78.1.0)\n",
      "Requirement already satisfied: simplejson in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (3.20.2)\n",
      "Requirement already satisfied: urllib3>=1.26.8 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (2.6.1)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (1.17.0)\n",
      "Requirement already satisfied: wurlitzer>=1.0.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (3.0.2)\n",
      "Requirement already satisfied: configobj in /home/aya/anaconda3/lib/python3.13/site-packages (from everett[ini]<3.2.0,>=1.0.1->comet-ml) (5.0.9)\n",
      "Requirement already satisfied: entmax<2.0,>=1.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (1.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (0.36.0)\n",
      "Requirement already satisfied: jsonargparse==3.13.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (3.13.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.20.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.4.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (2.2.3)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.24.4 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (4.25.8)\n",
      "Requirement already satisfied: pytorch-lightning<3.0.0,>=2.0.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (2.1.4)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.5.4 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (1.15.3)\n",
      "Requirement already satisfied: sentencepiece<0.3.0,>=0.2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (0.2.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (2.9.0)\n",
      "Requirement already satisfied: torchmetrics<0.11.0,>=0.10.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (0.10.3)\n",
      "Requirement already satisfied: transformers<5.0,>=4.17 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (4.57.3)\n",
      "Requirement already satisfied: PyYAML>=3.13 in /home/aya/anaconda3/lib/python3.13/site-packages (from jsonargparse==3.13.1->unbabel-comet) (6.0.2)\n",
      "Requirement already satisfied: portalocker in /home/aya/anaconda3/lib/python3.13/site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: regex in /home/aya/anaconda3/lib/python3.13/site-packages (from sacrebleu) (2024.11.6)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/aya/anaconda3/lib/python3.13/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in /home/aya/anaconda3/lib/python3.13/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /home/aya/anaconda3/lib/python3.13/site-packages (from sacrebleu) (5.3.0)\n",
      "Requirement already satisfied: filelock in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (24.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (1.2.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.15.2)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.7.0)\n",
      "Requirement already satisfied: matplotlib in /home/aya/anaconda3/lib/python3.13/site-packages (from bert-score) (3.10.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/aya/anaconda3/lib/python3.13/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (3.11.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (3.7)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/aya/anaconda3/lib/python3.13/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/aya/anaconda3/lib/python3.13/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.22.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas>=1.4.1->unbabel-comet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas>=1.4.1->unbabel-comet) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas>=1.4.1->unbabel-comet) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas>=1.4.1->unbabel-comet) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests>=2.18.4->comet-ml) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests>=2.18.4->comet-ml) (2025.4.26)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from rich>=13.3.2->comet-ml) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from rich>=13.3.2->comet-ml) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet-ml) (0.1.0)\n",
      "Requirement already satisfied: matplotlib in /home/aya/anaconda3/lib/python3.13/site-packages (from bert-score) (3.10.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/aya/anaconda3/lib/python3.13/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (3.11.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (3.7)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/aya/anaconda3/lib/python3.13/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/aya/anaconda3/lib/python3.13/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.22.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas>=1.4.1->unbabel-comet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas>=1.4.1->unbabel-comet) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas>=1.4.1->unbabel-comet) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas>=1.4.1->unbabel-comet) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests>=2.18.4->comet-ml) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests>=2.18.4->comet-ml) (2025.4.26)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from rich>=13.3.2->comet-ml) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from rich>=13.3.2->comet-ml) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet-ml) (0.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.6.0->unbabel-comet) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from jinja2->torch>=1.6.0->unbabel-comet) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from matplotlib->bert-score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/aya/anaconda3/lib/python3.13/site-packages (from matplotlib->bert-score) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from matplotlib->bert-score) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /home/aya/anaconda3/lib/python3.13/site-packages (from matplotlib->bert-score) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from matplotlib->bert-score) (3.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.6.0->unbabel-comet) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from jinja2->torch>=1.6.0->unbabel-comet) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from matplotlib->bert-score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/aya/anaconda3/lib/python3.13/site-packages (from matplotlib->bert-score) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from matplotlib->bert-score) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /home/aya/anaconda3/lib/python3.13/site-packages (from matplotlib->bert-score) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from matplotlib->bert-score) (3.2.0)\n",
      "Requirement already satisfied: wandb in /home/aya/anaconda3/lib/python3.13/site-packages (0.23.1)\n",
      "Requirement already satisfied: pandas in /home/aya/anaconda3/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /home/aya/anaconda3/lib/python3.13/site-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in /home/aya/anaconda3/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: click>=8.0.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: packaging in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (24.2)\n",
      "Requirement already satisfied: platformdirs in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (4.25.8)\n",
      "Requirement already satisfied: pydantic<3 in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (2.12.5)\n",
      "Requirement already satisfied: pyyaml in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (2.32.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (2.47.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from pydantic<3->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from pydantic<3->wandb) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: wandb in /home/aya/anaconda3/lib/python3.13/site-packages (0.23.1)\n",
      "Requirement already satisfied: pandas in /home/aya/anaconda3/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /home/aya/anaconda3/lib/python3.13/site-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in /home/aya/anaconda3/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: click>=8.0.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: packaging in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (24.2)\n",
      "Requirement already satisfied: platformdirs in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (4.25.8)\n",
      "Requirement already satisfied: pydantic<3 in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (2.12.5)\n",
      "Requirement already satisfied: pyyaml in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (2.32.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (2.47.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from pydantic<3->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from pydantic<3->wandb) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "GPU Available: True\n",
      "Number of GPUs: 2\n",
      "  GPU 0: NVIDIA GeForce RTX 5090\n",
      "    Memory: 33.67 GB\n",
      "  GPU 1: NVIDIA GeForce RTX 5090\n",
      "    Memory: 33.67 GB\n",
      "\n",
      "Total VRAM: 67.34 GB\n",
      "Warning: Not running inside a virtual environment!\n",
      "Hugging Face token loaded from cache.\n",
      " Detected project directory: .\n",
      " Data directory: data\n",
      " Models directory: models\n",
      " Translation data not found at: Data/english-arabic\n",
      " Directory structure and model paths are ready!\n",
      "HPC Config: 2 GPUs, Flash Attention: False\n",
      "COMET Config: model=Unbabel/wmt22-cometkiwi-da, batch_size=64, gpu=1\n",
      " Configuration loaded successfully!\n",
      "   - COMET enabled for scoring\n",
      "   - LoRA enabled: True\n",
      "   - KL penalty: 0.15\n",
      "   - Sample size: 100,000\n",
      "   - Batch sizes: Generation=64, RM=8, PPO=32\n",
      "Utility functions loaded!\n",
      "Configuration saved to config.json\n",
      "GPU Available: True\n",
      "Number of GPUs: 2\n",
      "  GPU 0: NVIDIA GeForce RTX 5090\n",
      "    Memory: 33.67 GB\n",
      "  GPU 1: NVIDIA GeForce RTX 5090\n",
      "    Memory: 33.67 GB\n",
      "\n",
      "Total VRAM: 67.34 GB\n",
      "Warning: Not running inside a virtual environment!\n",
      "Hugging Face token loaded from cache.\n",
      " Detected project directory: .\n",
      " Data directory: data\n",
      " Models directory: models\n",
      " Translation data not found at: Data/english-arabic\n",
      " Directory structure and model paths are ready!\n",
      "HPC Config: 2 GPUs, Flash Attention: False\n",
      "COMET Config: model=Unbabel/wmt22-cometkiwi-da, batch_size=64, gpu=1\n",
      " Configuration loaded successfully!\n",
      "   - COMET enabled for scoring\n",
      "   - LoRA enabled: True\n",
      "   - KL penalty: 0.15\n",
      "   - Sample size: 100,000\n",
      "   - Batch sizes: Generation=64, RM=8, PPO=32\n",
      "Utility functions loaded!\n",
      "Configuration saved to config.json\n"
     ]
    }
   ],
   "source": [
    "# Run setup from config notebook\n",
    "%run 0_config_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8419bc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic Data Generation Pipeline\n",
      "================================================================================\n",
      "Scoring method: Heuristic-based\n",
      "Heuristic metrics: Length ratio, Punctuation presence, Non-empty validation\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# ===========================\n",
    "# CONFIGURATION: EVALUATION METHOD\n",
    "# ===========================\n",
    "USE_COMET = False  # Set to True to use COMET model, False for fast heuristic-based scoring\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "print(\"Synthetic Data Generation Pipeline\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Scoring method: {'COMET-based' if USE_COMET else 'Heuristic-based'}\")\n",
    "print(f\"Heuristic metrics: Length ratio, Punctuation presence, Non-empty validation\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef9d6c",
   "metadata": {},
   "source": [
    "## Load SFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e480820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cache cleared\n",
      "\n",
      "Model Loading Configuration:\n",
      "Total GPUs: 2\n",
      "Total VRAM: 62.72GB\n",
      "SFT Model: 28.9B parameters\n",
      "Quantization: 8-bit\n",
      "Precision: bfloat16\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU cache and set CUDA environment variables\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"GPU cache cleared\")\n",
    "\n",
    "# CUDA environment variables for optimized memory management\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Unload COMET model if loaded to free memory for SFT model\n",
    "if USE_COMET and 'comet_model' in globals() and comet_model is not None:\n",
    "    try:\n",
    "        del comet_model\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        print(\"COMET unloaded from GPU memory\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to unload COMET: {e}\")\n",
    "\n",
    "# ===========================\n",
    "# MODEL LOADING CONFIGURATION\n",
    "# ===========================\n",
    "FORCE_CPU = False\n",
    "USE_BFLOAT16 = True\n",
    "\n",
    "print(\"\\nModel Loading Configuration:\")\n",
    "print(f\"Total GPUs: {NUM_GPUS}\")\n",
    "print(f\"Total VRAM: {NUM_GPUS * 31.36:.2f}GB\")\n",
    "print(f\"SFT Model: 28.9B parameters\")\n",
    "print(f\"Quantization: 8-bit\")\n",
    "print(f\"Precision: {'bfloat16' if USE_BFLOAT16 else 'float32'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9d02f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /ModelSpace/GemmaX2-28-9B-v0.1/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"HTTPSConnection(host=\\'huggingface.co\\', port=443): Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 5635c47c-525e-4453-af7f-559d92dee5d7)')' thrown while requesting HEAD https://huggingface.co/ModelSpace/GemmaX2-28-9B-v0.1/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading SFT model from Hugging Face...\n",
      "Loading model with 8-bit quantization across 2 GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c479d4cd8f7497398b25a662c4c0509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model split across: {'1', '0'}\n",
      "Model size: 9.24B parameters\n",
      "Model ready for inference\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading SFT model from Hugging Face...\")\n",
    "model_name = \"ModelSpace/GemmaX2-28-9B-v0.1\"\n",
    "\n",
    "if FORCE_CPU:\n",
    "    print(\"Loading model on CPU\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map={\"\": \"cpu\"},\n",
    "        torch_dtype=torch.float32,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "else:\n",
    "    print(f\"Loading model with 8-bit quantization across {NUM_GPUS} GPUs\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    \n",
    "    max_memory = {\n",
    "        0: \"31GB\",\n",
    "        1: \"31GB\",\n",
    "        \"cpu\": \"64GB\"\n",
    "    }\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        max_memory=max_memory\n",
    "    )\n",
    "\n",
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Report device placement\n",
    "try:\n",
    "    if hasattr(model, 'hf_device_map'):\n",
    "        devices_used = set(str(v) for v in model.hf_device_map.values())\n",
    "        print(f\"Model split across: {devices_used}\")\n",
    "except Exception as e:\n",
    "    print(f\"Device info: {e}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters()) / 1e9\n",
    "print(f\"Model size: {total_params:.2f}B parameters\")\n",
    "print(\"Model ready for inference\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c20a72",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9272a129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading source data for synthetic translation generation...\n",
      "Data source: FULL\n",
      "\n",
      "Loaded 3294856 English samples\n",
      "Loaded 3294856 English samples\n",
      "Loaded 484003 French samples\n",
      "\n",
      "Total available data: 3,778,859 samples\n",
      "Available by language:\n",
      "  English: 3,294,856 samples\n",
      "  French: 484,003 samples\n",
      "Loaded 484003 French samples\n",
      "\n",
      "Total available data: 3,778,859 samples\n",
      "Available by language:\n",
      "  English: 3,294,856 samples\n",
      "  French: 484,003 samples\n",
      "\n",
      "Sampled 20,000 samples for generation:\n",
      "  English to Arabic: 10,000 (50.0%)\n",
      "  French to Arabic: 10,000 (50.0%)\n",
      "\n",
      "Sampled 20,000 samples for generation:\n",
      "  English to Arabic: 10,000 (50.0%)\n",
      "  French to Arabic: 10,000 (50.0%)\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# LOAD TRAINING DATA (EN + FR)\n",
    "# ===========================\n",
    "USE_SAMPLES = False  # Set False for full dataset, True for samples\n",
    "\n",
    "print(\"\\nLoading source data for synthetic translation generation...\")\n",
    "print(f\"Data source: {'SAMPLES' if USE_SAMPLES else 'FULL'}\\n\")\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# Load English data\n",
    "english_inputs_path = PROJECT_DIR / (\"data/english_inputs_samples.json\" if USE_SAMPLES else \"data/english_inputs.json\")\n",
    "\n",
    "if english_inputs_path.exists():\n",
    "    with open(english_inputs_path, 'r', encoding='utf-8') as f:\n",
    "        english_data = json.load(f)\n",
    "    \n",
    "    if isinstance(english_data, list):\n",
    "        for item in english_data:\n",
    "            if isinstance(item, str):\n",
    "                all_data.append({'source': item, 'source_lang': 'en'})\n",
    "            elif isinstance(item, dict):\n",
    "                text = item.get('text', item.get('source', item.get('sentence', '')))\n",
    "                if text:\n",
    "                    all_data.append({'source': text, 'source_lang': 'en'})\n",
    "    print(f\"Loaded {len(english_data)} English samples\")\n",
    "else:\n",
    "    print(f\"Warning: {english_inputs_path.name} not found\")\n",
    "\n",
    "# Load French data\n",
    "french_inputs_path = PROJECT_DIR / \"data/french_inputs.json\"\n",
    "\n",
    "if french_inputs_path.exists():\n",
    "    with open(french_inputs_path, 'r', encoding='utf-8') as f:\n",
    "        french_data = json.load(f)\n",
    "    \n",
    "    if isinstance(french_data, list):\n",
    "        for item in french_data:\n",
    "            if isinstance(item, str):\n",
    "                all_data.append({'source': item, 'source_lang': 'fr'})\n",
    "            elif isinstance(item, dict):\n",
    "                text = item.get('text', item.get('source', item.get('sentence', '')))\n",
    "                if text:\n",
    "                    all_data.append({'source': text, 'source_lang': 'fr'})\n",
    "    print(f\"Loaded {len(french_data)} French samples\")\n",
    "else:\n",
    "    print(f\"Warning: french_inputs.json not found\")\n",
    "\n",
    "print(f\"\\nTotal available data: {len(all_data):,} samples\")\n",
    "\n",
    "# ===========================\n",
    "# SAMPLE DATA FOR BALANCED TRAINING\n",
    "# ===========================\n",
    "SAMPLE_SIZE_PER_LANG = 10_000  # 10K per language = 20K total\n",
    "\n",
    "en_data = [s for s in all_data if s['source_lang'] == 'en']\n",
    "fr_data = [s for s in all_data if s['source_lang'] == 'fr']\n",
    "\n",
    "print(f\"Available by language:\")\n",
    "print(f\"  English: {len(en_data):,} samples\")\n",
    "print(f\"  French: {len(fr_data):,} samples\")\n",
    "\n",
    "random.shuffle(en_data)\n",
    "random.shuffle(fr_data)\n",
    "\n",
    "en_samples = en_data[:min(SAMPLE_SIZE_PER_LANG, len(en_data))]\n",
    "fr_samples = fr_data[:min(SAMPLE_SIZE_PER_LANG, len(fr_data))]\n",
    "\n",
    "training_samples = en_samples + fr_samples\n",
    "random.shuffle(training_samples)\n",
    "\n",
    "total_samples = len(training_samples)\n",
    "en_pct = 100 * len(en_samples) / total_samples if total_samples > 0 else 0\n",
    "fr_pct = 100 * len(fr_samples) / total_samples if total_samples > 0 else 0\n",
    "\n",
    "print(f\"\\nSampled {total_samples:,} samples for generation:\")\n",
    "print(f\"  English to Arabic: {len(en_samples):,} ({en_pct:.1f}%)\")\n",
    "print(f\"  French to Arabic: {len(fr_samples):,} ({fr_pct:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aff54f2",
   "metadata": {},
   "source": [
    "## Generate Translation Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b63b4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation Configuration:\n",
      "  Batch size: 64\n",
      "  Candidates per source: 4\n",
      "  Max tokens: 128\n",
      "  Methods: 4 (temperature, top-k, nucleus, greedy)\n",
      "  Note: Batch size reduced to prevent CUDA OOM errors\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# GENERATION CONFIGURATION\n",
    "# ===========================\n",
    "MEGA_BATCH_SIZE = 64  # Batch size for parallel generation\n",
    "NUM_CANDIDATES = 4\n",
    "MAX_NEW_TOKENS = 128\n",
    "\n",
    "print(\"\\nGeneration Configuration:\")\n",
    "print(f\"  Batch size: {MEGA_BATCH_SIZE}\")\n",
    "print(f\"  Candidates per source: {NUM_CANDIDATES}\")\n",
    "print(f\"  Max tokens: {MAX_NEW_TOKENS}\")\n",
    "print(f\"  Methods: 4 (temperature, top-k, nucleus, greedy)\")\n",
    "print(f\"  Note: Batch size reduced to prevent CUDA OOM errors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73d4ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation methods configured:\n",
      "  1. Temperature Sampling (high randomness)\n",
      "  2. Top-K Sampling (conservative)\n",
      "  3. Nucleus Sampling (balanced)\n",
      "  4. Greedy Decoding (deterministic)\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# TRANSLATION GENERATION METHODS\n",
    "# ===========================\n",
    "# Four different sampling strategies for diverse translation candidates\n",
    "# Each method generates exactly ONE candidate per source\n",
    "\n",
    "def generate_with_temperature(sources, langs):\n",
    "    \"\"\"High temperature sampling for diverse outputs\"\"\"\n",
    "    prompts = [format_translation_prompt(src, lang) for src, lang in zip(sources, langs)]\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=True,\n",
    "        temperature=1.2,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    candidates = []\n",
    "    \n",
    "    for i in range(len(sources)):\n",
    "        text = generated_texts[i]\n",
    "        translation = text.split(\"Arabic translation:\")[-1].strip() if \"Arabic translation:\" in text else text.strip()\n",
    "        candidates.append({\n",
    "            'translation': translation,\n",
    "            'method': 'temperature',\n",
    "            'config': {'temperature': 1.2, 'top_p': 0.95, 'top_k': 50}\n",
    "        })\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def generate_with_topk(sources, langs):\n",
    "    \"\"\"Conservative top-k sampling\"\"\"\n",
    "    prompts = [format_translation_prompt(src, lang) for src, lang in zip(sources, langs)]\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=30,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    candidates = []\n",
    "    \n",
    "    for i in range(len(sources)):\n",
    "        text = generated_texts[i]\n",
    "        translation = text.split(\"Arabic translation:\")[-1].strip() if \"Arabic translation:\" in text else text.strip()\n",
    "        candidates.append({\n",
    "            'translation': translation,\n",
    "            'method': 'top_k',\n",
    "            'config': {'temperature': 0.7, 'top_k': 30, 'top_p': 0.9}\n",
    "        })\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def generate_with_nucleus(sources, langs):\n",
    "    \"\"\"Nucleus (top-p) sampling for balanced diversity\"\"\"\n",
    "    prompts = [format_translation_prompt(src, lang) for src, lang in zip(sources, langs)]\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        top_k=0,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    candidates = []\n",
    "    \n",
    "    for i in range(len(sources)):\n",
    "        text = generated_texts[i]\n",
    "        translation = text.split(\"Arabic translation:\")[-1].strip() if \"Arabic translation:\" in text else text.strip()\n",
    "        candidates.append({\n",
    "            'translation': translation,\n",
    "            'method': 'nucleus',\n",
    "            'config': {'temperature': 0.9, 'top_p': 0.95}\n",
    "        })\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def generate_with_greedy(sources, langs):\n",
    "    \"\"\"Greedy decoding for consistent outputs\"\"\"\n",
    "    prompts = [format_translation_prompt(src, lang) for src, lang in zip(sources, langs)]\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    candidates = []\n",
    "    \n",
    "    for i in range(len(sources)):\n",
    "        text = generated_texts[i]\n",
    "        translation = text.split(\"Arabic translation:\")[-1].strip() if \"Arabic translation:\" in text else text.strip()\n",
    "        candidates.append({\n",
    "            'translation': translation,\n",
    "            'method': 'greedy',\n",
    "            'config': {'do_sample': False}\n",
    "        })\n",
    "    return candidates\n",
    "\n",
    "\n",
    "GENERATION_METHODS = {\n",
    "    'temperature': generate_with_temperature,\n",
    "    'top_k': generate_with_topk,\n",
    "    'nucleus': generate_with_nucleus,\n",
    "    'greedy': generate_with_greedy\n",
    "}\n",
    "\n",
    "print(\"\\nGeneration methods configured:\")\n",
    "print(\"  1. Temperature Sampling (high randomness)\")\n",
    "print(\"  2. Top-K Sampling (conservative)\")\n",
    "print(\"  3. Nucleus Sampling (balanced)\")\n",
    "print(\"  4. Greedy Decoding (deterministic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c927da82",
   "metadata": {},
   "source": [
    "## Alternative: vLLM for Maximum Speed (Optional)\n",
    "\n",
    "If you have vLLM installed (`pip install vllm`), uncomment and run the cell below for 3-10x faster generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb2fc098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using transformers for generation (set USE_VLLM=True for faster inference)\n"
     ]
    }
   ],
   "source": [
    "# Optional: Use vLLM for 3-10x faster generation\n",
    "# Install: pip install vllm\n",
    "\n",
    "USE_VLLM = False\n",
    "\n",
    "if USE_VLLM:\n",
    "    try:\n",
    "        from vllm import LLM, SamplingParams\n",
    "        \n",
    "        print(\"Loading model with vLLM (tensor parallel)...\")\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        vllm_model = LLM(\n",
    "            model=\"ModelSpace/GemmaX2-28-9B-v0.1\",\n",
    "            tensor_parallel_size=NUM_GPUS,\n",
    "            dtype=\"bfloat16\",\n",
    "            max_model_len=512,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        \n",
    "        vllm_sampling = SamplingParams(\n",
    "            temperature=0.9,\n",
    "            top_p=0.95,\n",
    "            max_tokens=MAX_NEW_TOKENS,\n",
    "            n=NUM_CANDIDATES\n",
    "        )\n",
    "        \n",
    "        def generate_with_vllm(batch_sources, batch_langs, num_candidates=4):\n",
    "            \"\"\"Fast generation with vLLM\"\"\"\n",
    "            prompts = [format_translation_prompt(src, lang) \n",
    "                       for src, lang in zip(batch_sources, batch_langs)]\n",
    "            outputs = vllm_model.generate(prompts, vllm_sampling)\n",
    "            \n",
    "            all_candidates = []\n",
    "            for output in outputs:\n",
    "                candidates = []\n",
    "                for completion in output.outputs:\n",
    "                    text = completion.text\n",
    "                    translation = text.split(\"Arabic translation:\")[-1].strip() if \"Arabic translation:\" in text else text.strip()\n",
    "                    candidates.append({\n",
    "                        'translation': translation,\n",
    "                        'config': {'temperature': 0.9, 'top_p': 0.95}\n",
    "                    })\n",
    "                all_candidates.append(candidates)\n",
    "            return all_candidates\n",
    "        \n",
    "        print(\"vLLM loaded successfully (3-10x faster)\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"vLLM not installed. Using transformers instead.\")\n",
    "        print(\"Install with: pip install vllm\")\n",
    "    except Exception as e:\n",
    "        print(f\"vLLM failed to load: {e}. Using transformers instead.\")\n",
    "else:\n",
    "    print(\"Using transformers for generation (set USE_VLLM=True for faster inference)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7469f6d7",
   "metadata": {},
   "source": [
    "## Score Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b29fb17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring functions configured (heuristic-based)\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# TRANSLATION QUALITY SCORING\n",
    "# ===========================\n",
    "\n",
    "def score_candidates(translation_text: str, source_text: str) -> float:\n",
    "    \"\"\"Score a single translation using heuristics.\n",
    "    \n",
    "    Metrics: length ratio, punctuation presence, non-empty validation\n",
    "    Returns: Quality score between 0 and 1\n",
    "    \"\"\"\n",
    "    src_len = len(source_text.split())\n",
    "    src_punct = sum(1 for c in source_text if c in '.!?,;:')\n",
    "    \n",
    "    tgt = translation_text\n",
    "    tgt_len = len(tgt.split())\n",
    "    \n",
    "    # Length ratio score\n",
    "    if src_len > 0:\n",
    "        length_ratio = min(tgt_len, src_len) / max(tgt_len, src_len)\n",
    "    else:\n",
    "        length_ratio = 0.5 if tgt_len == 0 else 0.0\n",
    "    \n",
    "    # Punctuation presence score\n",
    "    tgt_punct = sum(1 for c in tgt if c in '.!?,;:')\n",
    "    punct_score = 1.0 if (src_punct > 0 and tgt_punct > 0) or (src_punct == 0 and tgt_punct == 0) else 0.7\n",
    "    \n",
    "    # Non-empty score\n",
    "    non_empty_score = 1.0 if len(tgt.strip()) > 0 and '[ERROR]' not in tgt else 0.0\n",
    "    \n",
    "    # Combined score\n",
    "    quality_score = (length_ratio * 0.5 + punct_score * 0.3 + non_empty_score * 0.2)\n",
    "    quality_score = max(0.0, min(1.0, quality_score))\n",
    "    \n",
    "    return quality_score\n",
    "\n",
    "\n",
    "def create_all_preference_pairs(source_text: str, candidates_with_methods: list) -> list:\n",
    "    \"\"\"Create preference pairs from 4 candidates using all pairwise comparisons.\n",
    "    \n",
    "    Args:\n",
    "        source_text: Source text\n",
    "        candidates_with_methods: List of dicts with 'translation' and 'method' keys\n",
    "    \n",
    "    Returns:\n",
    "        List of preference pairs\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    if len(candidates_with_methods) < 2:\n",
    "        return pairs\n",
    "    \n",
    "    # Score each candidate\n",
    "    scored_candidates = []\n",
    "    for cand in candidates_with_methods:\n",
    "        score = score_candidates(cand['translation'], source_text)\n",
    "        scored_candidates.append({\n",
    "            'translation': cand['translation'],\n",
    "            'method': cand['method'],\n",
    "            'score': score\n",
    "        })\n",
    "    \n",
    "    # Create all pairwise comparisons\n",
    "    for i in range(len(scored_candidates)):\n",
    "        for j in range(i + 1, len(scored_candidates)):\n",
    "            cand_i = scored_candidates[i]\n",
    "            cand_j = scored_candidates[j]\n",
    "            \n",
    "            # Ensure we have a meaningful preference (at least 0.01 difference)\n",
    "            score_diff = abs(cand_i['score'] - cand_j['score'])\n",
    "            \n",
    "            if score_diff >= 0.01:\n",
    "                if cand_i['score'] > cand_j['score']:\n",
    "                    chosen = cand_i\n",
    "                    rejected = cand_j\n",
    "                else:\n",
    "                    chosen = cand_j\n",
    "                    rejected = cand_i\n",
    "                \n",
    "                pairs.append({\n",
    "                    'chosen': chosen['translation'],\n",
    "                    'rejected': rejected['translation'],\n",
    "                    'chosen_score': chosen['score'],\n",
    "                    'rejected_score': rejected['score'],\n",
    "                    'score_margin': abs(chosen['score'] - rejected['score']),\n",
    "                    'chosen_method': chosen['method'],\n",
    "                    'rejected_method': rejected['method']\n",
    "                })\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "print(\"Scoring functions configured (heuristic-based)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e803f392",
   "metadata": {},
   "source": [
    "## Generate Synthetic Preference Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "796400ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing scoring and preference pair functions...\n",
      "\n",
      "Scored candidates:\n",
      "  'Hello world' (greedy): 1.000\n",
      "  'Hi there' (temperature): 1.000\n",
      "  '[ERROR]' (nucleus): 0.550\n",
      "  'Bonjour le monde' (top_k): 0.833\n",
      "\n",
      "Preference pairs created: 5\n",
      "  Chosen: 'Hello world' (1.000)\n",
      "  Rejected: '[ERROR]' (0.550)\n",
      "  Margin: 0.450\n",
      "  Chosen: 'Hello world' (1.000)\n",
      "  Rejected: 'Bonjour le monde' (0.833)\n",
      "  Margin: 0.167\n",
      "  Chosen: 'Hi there' (1.000)\n",
      "  Rejected: '[ERROR]' (0.550)\n",
      "  Margin: 0.450\n",
      "  Chosen: 'Hi there' (1.000)\n",
      "  Rejected: 'Bonjour le monde' (0.833)\n",
      "  Margin: 0.167\n",
      "  Chosen: 'Bonjour le monde' (0.833)\n",
      "  Rejected: '[ERROR]' (0.550)\n",
      "  Margin: 0.283\n",
      "\n",
      "Functions working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Test scoring functions\n",
    "print(\"Testing scoring and preference pair functions...\\n\")\n",
    "\n",
    "test_source = \"Hello world\"\n",
    "test_candidates = [\n",
    "    {'translation': 'Hello world', 'method': 'greedy'},\n",
    "    {'translation': 'Hi there', 'method': 'temperature'},\n",
    "    {'translation': '[ERROR]', 'method': 'nucleus'},\n",
    "    {'translation': 'Bonjour le monde', 'method': 'top_k'}\n",
    "]\n",
    "\n",
    "print(\"Scored candidates:\")\n",
    "for cand in test_candidates:\n",
    "    score = score_candidates(cand['translation'], test_source)\n",
    "    print(f\"  '{cand['translation'][:30]}' ({cand['method']}): {score:.3f}\")\n",
    "\n",
    "pairs = create_all_preference_pairs(test_source, test_candidates)\n",
    "print(f\"\\nPreference pairs created: {len(pairs)}\")\n",
    "for pair in pairs:\n",
    "    print(f\"  Chosen: '{pair['chosen'][:30]}' ({pair['chosen_score']:.3f})\")\n",
    "    print(f\"  Rejected: '{pair['rejected'][:30]}' ({pair['rejected_score']:.3f})\")\n",
    "    print(f\"  Margin: {pair['score_margin']:.3f}\")\n",
    "\n",
    "print(\"\\nFunctions working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dead7467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU memory...\n",
      " GPU memory cleared\n",
      "\n",
      "================================================================================\n",
      "MULTI-METHOD SYNTHETIC DATA GENERATION\n",
      "Scoring: Heuristic-based\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Total samples: 20,000\n",
      "  Batch size: 64\n",
      "  Candidates per sample: 4 (one per method)\n",
      "  Generation methods: 4\n",
      "  GPUs: 2\n",
      "\n",
      "Language distribution:\n",
      "  English to Arabic: 10,000 (50.0%)\n",
      "  French to Arabic: 10,000 (50.0%)\n",
      "================================================================================\n",
      "\n",
      " No checkpoint found. Starting from beginning...\n",
      " GPU memory cleared\n",
      "\n",
      "================================================================================\n",
      "MULTI-METHOD SYNTHETIC DATA GENERATION\n",
      "Scoring: Heuristic-based\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Total samples: 20,000\n",
      "  Batch size: 64\n",
      "  Candidates per sample: 4 (one per method)\n",
      "  Generation methods: 4\n",
      "  GPUs: 2\n",
      "\n",
      "Language distribution:\n",
      "  English to Arabic: 10,000 (50.0%)\n",
      "  French to Arabic: 10,000 (50.0%)\n",
      "================================================================================\n",
      "\n",
      " No checkpoint found. Starting from beginning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/313 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Processing batches:   6%|         | 20/313 [21:48<5:23:07, 66.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress (Batch 20/313):\n",
      "  Samples: 1,280/20,000 (6.4%)\n",
      "  Preference pairs: 5,804\n",
      "    EN->AR: 2,801, FR->AR: 3,003\n",
      "  Avg quality score: 0.8857\n",
      "  Rate: 1.0 samples/sec\n",
      "  ETA: 5.32 hours\n",
      "  Errors: 0\n",
      "   Checkpoint saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  13%|        | 40/313 [42:32<4:39:00, 61.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress (Batch 40/313):\n",
      "  Samples: 2,560/20,000 (12.8%)\n",
      "  Preference pairs: 11,620\n",
      "    EN->AR: 5,665, FR->AR: 5,955\n",
      "  Avg quality score: 0.8843\n",
      "  Rate: 1.0 samples/sec\n",
      "  ETA: 4.83 hours\n",
      "  Errors: 0\n",
      "   Checkpoint saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  19%|        | 60/313 [1:02:49<4:08:52, 59.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress (Batch 60/313):\n",
      "  Samples: 3,840/20,000 (19.2%)\n",
      "  Preference pairs: 17,464\n",
      "    EN->AR: 8,372, FR->AR: 9,092\n",
      "  Avg quality score: 0.8839\n",
      "  Rate: 1.0 samples/sec\n",
      "  ETA: 4.41 hours\n",
      "  Errors: 0\n",
      "   Checkpoint saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  26%|       | 80/313 [1:22:56<4:13:04, 65.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress (Batch 80/313):\n",
      "  Samples: 5,120/20,000 (25.6%)\n",
      "  Preference pairs: 23,232\n",
      "    EN->AR: 11,068, FR->AR: 12,164\n",
      "  Avg quality score: 0.8838\n",
      "  Rate: 1.0 samples/sec\n",
      "  ETA: 4.02 hours\n",
      "  Errors: 0\n",
      "   Checkpoint saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  32%|      | 100/313 [1:44:13<3:35:40, 60.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress (Batch 100/313):\n",
      "  Samples: 6,400/20,000 (32.0%)\n",
      "  Preference pairs: 29,067\n",
      "    EN->AR: 13,808, FR->AR: 15,259\n",
      "  Avg quality score: 0.8841\n",
      "  Rate: 1.0 samples/sec\n",
      "  ETA: 3.69 hours\n",
      "  Errors: 0\n",
      "   Checkpoint saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  38%|      | 120/313 [2:04:39<3:31:12, 65.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress (Batch 120/313):\n",
      "  Samples: 7,680/20,000 (38.4%)\n",
      "  Preference pairs: 34,946\n",
      "    EN->AR: 16,650, FR->AR: 18,296\n",
      "  Avg quality score: 0.8840\n",
      "  Rate: 1.0 samples/sec\n",
      "  ETA: 3.33 hours\n",
      "  Errors: 0\n",
      "   Checkpoint saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  45%|     | 140/313 [2:24:47<2:49:59, 58.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress (Batch 140/313):\n",
      "  Samples: 8,960/20,000 (44.8%)\n",
      "  Preference pairs: 40,808\n",
      "    EN->AR: 19,530, FR->AR: 21,278\n",
      "  Avg quality score: 0.8842\n",
      "  Rate: 1.0 samples/sec\n",
      "  ETA: 2.97 hours\n",
      "  Errors: 0\n",
      "   Checkpoint saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  51%|     | 160/313 [2:45:21<2:48:28, 66.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress (Batch 160/313):\n",
      "  Samples: 10,240/20,000 (51.2%)\n",
      "  Preference pairs: 46,639\n",
      "    EN->AR: 22,407, FR->AR: 24,232\n",
      "  Avg quality score: 0.8844\n",
      "  Rate: 1.0 samples/sec\n",
      "  ETA: 2.63 hours\n",
      "  Errors: 0\n",
      "   Checkpoint saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  58%|    | 180/313 [3:05:53<2:19:11, 62.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress (Batch 180/313):\n",
      "  Samples: 11,520/20,000 (57.6%)\n",
      "  Preference pairs: 52,542\n",
      "    EN->AR: 25,161, FR->AR: 27,381\n",
      "  Avg quality score: 0.8846\n",
      "  Rate: 1.0 samples/sec\n",
      "  ETA: 2.28 hours\n",
      "  Errors: 0\n",
      "   Checkpoint saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  64%|   | 200/313 [3:26:01<1:51:35, 59.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress (Batch 200/313):\n",
      "  Samples: 12,800/20,000 (64.0%)\n",
      "  Preference pairs: 58,377\n",
      "    EN->AR: 28,138, FR->AR: 30,239\n",
      "  Avg quality score: 0.8849\n",
      "  Rate: 1.0 samples/sec\n",
      "  ETA: 1.93 hours\n",
      "  Errors: 0\n",
      "   Checkpoint saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  70%|   | 220/313 [3:46:09<1:37:03, 62.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress (Batch 220/313):\n",
      "  Samples: 14,080/20,000 (70.4%)\n",
      "  Preference pairs: 64,217\n",
      "    EN->AR: 31,133, FR->AR: 33,084\n",
      "  Avg quality score: 0.8847\n",
      "  Rate: 1.0 samples/sec\n",
      "  ETA: 1.58 hours\n",
      "  Errors: 0\n",
      "   Checkpoint saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  77%|  | 240/313 [4:06:55<1:16:06, 62.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress (Batch 240/313):\n",
      "  Samples: 15,360/20,000 (76.8%)\n",
      "  Preference pairs: 70,068\n",
      "    EN->AR: 33,960, FR->AR: 36,108\n",
      "  Avg quality score: 0.8847\n",
      "  Rate: 1.0 samples/sec\n",
      "  ETA: 1.24 hours\n",
      "  Errors: 0\n",
      "   Checkpoint saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  83%| | 260/313 [4:28:24<56:47, 64.30s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress (Batch 260/313):\n",
      "  Samples: 16,640/20,000 (83.2%)\n",
      "  Preference pairs: 75,866\n",
      "    EN->AR: 36,883, FR->AR: 38,983\n",
      "  Avg quality score: 0.8849\n",
      "  Rate: 1.0 samples/sec\n",
      "  ETA: 0.90 hours\n",
      "  Errors: 0\n",
      "   Checkpoint saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  89%| | 280/313 [4:49:52<37:50, 68.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress (Batch 280/313):\n",
      "  Samples: 17,920/20,000 (89.6%)\n",
      "  Preference pairs: 81,743\n",
      "    EN->AR: 39,807, FR->AR: 41,936\n",
      "  Avg quality score: 0.8846\n",
      "  Rate: 1.0 samples/sec\n",
      "  ETA: 0.56 hours\n",
      "  Errors: 0\n",
      "   Checkpoint saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  96%|| 300/313 [5:11:30<12:48, 59.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress (Batch 300/313):\n",
      "  Samples: 19,200/20,000 (96.0%)\n",
      "  Preference pairs: 87,534\n",
      "    EN->AR: 42,463, FR->AR: 45,071\n",
      "  Avg quality score: 0.8845\n",
      "  Rate: 1.0 samples/sec\n",
      "  ETA: 0.22 hours\n",
      "  Errors: 0\n",
      "   Checkpoint saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|| 313/313 [5:24:11<00:00, 62.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATION COMPLETE\n",
      "================================================================================\n",
      "  Samples processed: 20,000\n",
      "  EN-AR candidates: 10,000\n",
      "  FR-AR candidates: 10,000\n",
      "  Total preference pairs: 91,232\n",
      "    English to Arabic: 44,324\n",
      "    French to Arabic: 46,908\n",
      "  Total time: 5.40 hours\n",
      "  Average rate: 1.0 samples/sec\n",
      "  Errors: 0\n",
      "\n",
      "Overall avg quality score: 0.8847\n",
      "\n",
      "Method statistics:\n",
      "  temperature: avg=0.8820\n",
      "  top_k: avg=0.8857\n",
      "  nucleus: avg=0.8856\n",
      "  greedy: avg=0.8855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# MAIN GENERATION AND SCORING LOOP WITH CHECKPOINTING\n",
    "# ===========================\n",
    "\n",
    "# Clear GPU memory before starting\n",
    "print(\"Clearing GPU memory...\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    gc.collect()\n",
    "    print(\" GPU memory cleared\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MULTI-METHOD SYNTHETIC DATA GENERATION\")\n",
    "print(f\"Scoring: {'COMET-based' if USE_COMET else 'Heuristic-based'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Total samples: {len(training_samples):,}\")\n",
    "print(f\"  Batch size: {MEGA_BATCH_SIZE}\")\n",
    "print(f\"  Candidates per sample: 4 (one per method)\")\n",
    "print(f\"  Generation methods: 4\")\n",
    "print(f\"  GPUs: {NUM_GPUS}\")\n",
    "\n",
    "en_count = sum(1 for s in training_samples if s['source_lang'] == 'en')\n",
    "fr_count = sum(1 for s in training_samples if s['source_lang'] == 'fr')\n",
    "print(f\"\\nLanguage distribution:\")\n",
    "print(f\"  English to Arabic: {en_count:,} ({100*en_count/len(training_samples):.1f}%)\")\n",
    "print(f\"  French to Arabic: {fr_count:,} ({100*fr_count/len(training_samples):.1f}%)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Output file paths\n",
    "en_ar_candidates_file = OUTPUTS_DIR / \"english_arabic_candidates.jsonl\"\n",
    "fr_ar_candidates_file = OUTPUTS_DIR / \"french_arabic_candidates.jsonl\"\n",
    "en_ar_preferences_file = OUTPUTS_DIR / \"en-ar-preferences.jsonl\"\n",
    "fr_ar_preferences_file = OUTPUTS_DIR / \"fr-ar-preferences.jsonl\"\n",
    "\n",
    "# Checkpoint file paths\n",
    "checkpoint_dir = DATA_DIR / \"checkpoints\"\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "checkpoint_file = checkpoint_dir / \"generation_checkpoint.json\"\n",
    "en_candidates_checkpoint = checkpoint_dir / \"en_candidates_checkpoint.jsonl\"\n",
    "fr_candidates_checkpoint = checkpoint_dir / \"fr_candidates_checkpoint.jsonl\"\n",
    "en_preferences_checkpoint = checkpoint_dir / \"en_preferences_checkpoint.jsonl\"\n",
    "fr_preferences_checkpoint = checkpoint_dir / \"fr_preferences_checkpoint.jsonl\"\n",
    "stats_checkpoint = checkpoint_dir / \"stats_checkpoint.json\"\n",
    "\n",
    "# Load checkpoint if exists\n",
    "resume_from_batch = 0\n",
    "en_candidates_data = []\n",
    "fr_candidates_data = []\n",
    "en_preferences_data = []\n",
    "fr_preferences_data = []\n",
    "method_stats = {method_name: {'count': 0, 'avg_score': 0, 'scores': []} for method_name in GENERATION_METHODS.keys()}\n",
    "quality_scores_collected = []\n",
    "en_pairs_count = 0\n",
    "fr_pairs_count = 0\n",
    "errors_count = 0\n",
    "\n",
    "if checkpoint_file.exists():\n",
    "    print(\"\\n Loading checkpoint...\")\n",
    "    try:\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            checkpoint = json.load(f)\n",
    "        resume_from_batch = checkpoint['last_completed_batch'] + 1\n",
    "        en_pairs_count = checkpoint['en_pairs_count']\n",
    "        fr_pairs_count = checkpoint['fr_pairs_count']\n",
    "        errors_count = checkpoint['errors_count']\n",
    "        \n",
    "        # Load checkpoint data files\n",
    "        if en_candidates_checkpoint.exists():\n",
    "            with open(en_candidates_checkpoint, 'r', encoding='utf-8') as f:\n",
    "                en_candidates_data = [json.loads(line) for line in f if line.strip()]\n",
    "        \n",
    "        if fr_candidates_checkpoint.exists():\n",
    "            with open(fr_candidates_checkpoint, 'r', encoding='utf-8') as f:\n",
    "                fr_candidates_data = [json.loads(line) for line in f if line.strip()]\n",
    "        \n",
    "        if en_preferences_checkpoint.exists():\n",
    "            with open(en_preferences_checkpoint, 'r', encoding='utf-8') as f:\n",
    "                en_preferences_data = [json.loads(line) for line in f if line.strip()]\n",
    "        \n",
    "        if fr_preferences_checkpoint.exists():\n",
    "            with open(fr_preferences_checkpoint, 'r', encoding='utf-8') as f:\n",
    "                fr_preferences_data = [json.loads(line) for line in f if line.strip()]\n",
    "        \n",
    "        if stats_checkpoint.exists():\n",
    "            with open(stats_checkpoint, 'r') as f:\n",
    "                checkpoint_stats = json.load(f)\n",
    "            method_stats = checkpoint_stats.get('method_stats', method_stats)\n",
    "            quality_scores_collected = checkpoint_stats.get('quality_scores_collected', [])\n",
    "        \n",
    "        print(f\" Checkpoint loaded!\")\n",
    "        print(f\"  Resuming from batch {resume_from_batch}\")\n",
    "        print(f\"  EN candidates: {len(en_candidates_data):,}\")\n",
    "        print(f\"  FR candidates: {len(fr_candidates_data):,}\")\n",
    "        print(f\"  EN preference pairs: {len(en_preferences_data):,}\")\n",
    "        print(f\"  FR preference pairs: {len(fr_preferences_data):,}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading checkpoint: {e}\")\n",
    "        print(\"  Starting from beginning...\")\n",
    "        resume_from_batch = 0\n",
    "else:\n",
    "    print(\"\\n No checkpoint found. Starting from beginning...\")\n",
    "\n",
    "num_batches = (len(training_samples) + MEGA_BATCH_SIZE - 1) // MEGA_BATCH_SIZE\n",
    "start_time = time.time()\n",
    "samples_processed = resume_from_batch * MEGA_BATCH_SIZE\n",
    "checkpoint_interval = 20  # Save checkpoint more frequently (every 20 batches)\n",
    "\n",
    "for batch_idx in tqdm(range(resume_from_batch, num_batches), desc=\"Processing batches\", initial=resume_from_batch, total=num_batches):\n",
    "    start_idx = batch_idx * MEGA_BATCH_SIZE\n",
    "    end_idx = min(start_idx + MEGA_BATCH_SIZE, len(training_samples))\n",
    "    batch_samples = training_samples[start_idx:end_idx]\n",
    "    \n",
    "    batch_sources = [s['source'] for s in batch_samples]\n",
    "    batch_langs = [s['source_lang'] for s in batch_samples]\n",
    "    \n",
    "    try:\n",
    "        # Generate with all 4 methods\n",
    "        all_method_results = {}\n",
    "        for method_name, method_func in GENERATION_METHODS.items():\n",
    "            try:\n",
    "                # Clear GPU memory before each method\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                \n",
    "                method_candidates = method_func(batch_sources, batch_langs)\n",
    "                all_method_results[method_name] = method_candidates\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(f\"\\n  OOM Error in method {method_name}: Clearing memory and retrying...\")\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                        torch.cuda.reset_peak_memory_stats()\n",
    "                    gc.collect()\n",
    "                    time.sleep(2)  # Wait a moment before retrying\n",
    "                    try:\n",
    "                        method_candidates = method_func(batch_sources, batch_langs)\n",
    "                        all_method_results[method_name] = method_candidates\n",
    "                        print(f\" Retry successful for {method_name}\")\n",
    "                    except Exception as retry_err:\n",
    "                        print(f\" Retry failed: {retry_err}\")\n",
    "                        all_method_results[method_name] = [\n",
    "                            {'translation': '[ERROR]', 'method': method_name, 'config': {}} \n",
    "                            for _ in batch_samples\n",
    "                        ]\n",
    "                        errors_count += 1\n",
    "                else:\n",
    "                    print(f\"Error in method {method_name}: {e}\")\n",
    "                    all_method_results[method_name] = [\n",
    "                        {'translation': '[ERROR]', 'method': method_name, 'config': {}} \n",
    "                        for _ in batch_samples\n",
    "                    ]\n",
    "                    errors_count += 1\n",
    "        \n",
    "        # Process each sample\n",
    "        for sample_idx, sample in enumerate(batch_samples):\n",
    "            source_text = sample['source']\n",
    "            source_lang = sample['source_lang']\n",
    "            \n",
    "            # Collect 4 candidates (one per method)\n",
    "            four_candidates = []\n",
    "            for method_name in GENERATION_METHODS.keys():\n",
    "                if sample_idx < len(all_method_results[method_name]):\n",
    "                    cand = all_method_results[method_name][sample_idx]\n",
    "                    four_candidates.append({\n",
    "                        'translation': cand['translation'],\n",
    "                        'method': cand['method']\n",
    "                    })\n",
    "                    \n",
    "                    # Track statistics\n",
    "                    score = score_candidates(cand['translation'], source_text)\n",
    "                    quality_scores_collected.append(score)\n",
    "                    method_stats[method_name]['count'] += 1\n",
    "                    method_stats[method_name]['scores'].append(score)\n",
    "            \n",
    "            # Create candidate record\n",
    "            candidate_record = {\n",
    "                'source': source_text,\n",
    "                'source_lang': source_lang,\n",
    "                'candidates': [\n",
    "                    {\n",
    "                        'translation': c['translation'],\n",
    "                        'method': c['method']\n",
    "                    }\n",
    "                    for c in four_candidates\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # Save to appropriate candidates file\n",
    "            if source_lang == 'en':\n",
    "                en_candidates_data.append(candidate_record)\n",
    "            elif source_lang == 'fr':\n",
    "                fr_candidates_data.append(candidate_record)\n",
    "            \n",
    "            # Create preference pairs from 4 candidates\n",
    "            preference_pairs = create_all_preference_pairs(source_text, four_candidates)\n",
    "            \n",
    "            for pair in preference_pairs:\n",
    "                preference_record = {\n",
    "                    'source': source_text,\n",
    "                    'source_lang': source_lang,\n",
    "                    'chosen': pair['chosen'],\n",
    "                    'rejected': pair['rejected'],\n",
    "                    'chosen_score': pair['chosen_score'],\n",
    "                    'rejected_score': pair['rejected_score'],\n",
    "                    'margin': pair['score_margin'],\n",
    "                    'chosen_method': pair['chosen_method'],\n",
    "                    'rejected_method': pair['rejected_method']\n",
    "                }\n",
    "                \n",
    "                if source_lang == 'en':\n",
    "                    en_preferences_data.append(preference_record)\n",
    "                    en_pairs_count += 1\n",
    "                elif source_lang == 'fr':\n",
    "                    fr_preferences_data.append(preference_record)\n",
    "                    fr_pairs_count += 1\n",
    "        \n",
    "        samples_processed += len(batch_samples)\n",
    "        \n",
    "        # Aggressive memory management\n",
    "        if batch_idx % 10 == 0 and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    except Exception as e:\n",
    "        errors_count += 1\n",
    "        if errors_count <= 5:\n",
    "            print(f\"Error in batch {batch_idx}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Save checkpoint periodically\n",
    "    if (batch_idx + 1) % checkpoint_interval == 0 or batch_idx == num_batches - 1:\n",
    "        checkpoint_data = {\n",
    "            'last_completed_batch': batch_idx,\n",
    "            'samples_processed': samples_processed,\n",
    "            'en_pairs_count': en_pairs_count,\n",
    "            'fr_pairs_count': fr_pairs_count,\n",
    "            'errors_count': errors_count,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        \n",
    "        with open(checkpoint_file, 'w') as f:\n",
    "            json.dump(checkpoint_data, f, indent=2)\n",
    "        \n",
    "        # Save checkpoint data files\n",
    "        with open(en_candidates_checkpoint, 'w', encoding='utf-8') as f:\n",
    "            for item in en_candidates_data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        with open(fr_candidates_checkpoint, 'w', encoding='utf-8') as f:\n",
    "            for item in fr_candidates_data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        with open(en_preferences_checkpoint, 'w', encoding='utf-8') as f:\n",
    "            for item in en_preferences_data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        with open(fr_preferences_checkpoint, 'w', encoding='utf-8') as f:\n",
    "            for item in fr_preferences_data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        checkpoint_stats = {\n",
    "            'method_stats': method_stats,\n",
    "            'quality_scores_collected': quality_scores_collected\n",
    "        }\n",
    "        with open(stats_checkpoint, 'w') as f:\n",
    "            json.dump(checkpoint_stats, f, indent=2)\n",
    "    \n",
    "    # Progress update\n",
    "    if (batch_idx + 1) % 20 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = samples_processed / elapsed if elapsed > 0 else 0\n",
    "        remaining = (len(training_samples) - samples_processed) / rate if rate > 0 else 0\n",
    "        \n",
    "        if quality_scores_collected:\n",
    "            avg_score = sum(quality_scores_collected) / len(quality_scores_collected)\n",
    "        else:\n",
    "            avg_score = 0\n",
    "        \n",
    "        print(f\"\\nProgress (Batch {batch_idx + 1}/{num_batches}):\")\n",
    "        print(f\"  Samples: {samples_processed:,}/{len(training_samples):,} ({100*samples_processed/len(training_samples):.1f}%)\")\n",
    "        print(f\"  Preference pairs: {en_pairs_count + fr_pairs_count:,}\")\n",
    "        print(f\"    EN->AR: {en_pairs_count:,}, FR->AR: {fr_pairs_count:,}\")\n",
    "        print(f\"  Avg quality score: {avg_score:.4f}\")\n",
    "        print(f\"  Rate: {rate:.1f} samples/sec\")\n",
    "        print(f\"  ETA: {remaining/3600:.2f} hours\")\n",
    "        print(f\"  Errors: {errors_count}\")\n",
    "        print(f\"   Checkpoint saved\")\n",
    "\n",
    "# Finalize statistics\n",
    "for method_name in GENERATION_METHODS.keys():\n",
    "    if method_stats[method_name]['scores']:\n",
    "        method_stats[method_name]['avg_score'] = sum(method_stats[method_name]['scores']) / len(method_stats[method_name]['scores'])\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Samples processed: {samples_processed:,}\")\n",
    "print(f\"  EN-AR candidates: {len(en_candidates_data):,}\")\n",
    "print(f\"  FR-AR candidates: {len(fr_candidates_data):,}\")\n",
    "print(f\"  Total preference pairs: {en_pairs_count + fr_pairs_count:,}\")\n",
    "print(f\"    English to Arabic: {en_pairs_count:,}\")\n",
    "print(f\"    French to Arabic: {fr_pairs_count:,}\")\n",
    "print(f\"  Total time: {total_time/3600:.2f} hours\")\n",
    "print(f\"  Average rate: {samples_processed/total_time:.1f} samples/sec\")\n",
    "print(f\"  Errors: {errors_count}\")\n",
    "\n",
    "if quality_scores_collected:\n",
    "    overall_avg = sum(quality_scores_collected) / len(quality_scores_collected)\n",
    "    print(f\"\\nOverall avg quality score: {overall_avg:.4f}\")\n",
    "\n",
    "print(\"\\nMethod statistics:\")\n",
    "for method_name in GENERATION_METHODS.keys():\n",
    "    print(f\"  {method_name}: avg={method_stats[method_name]['avg_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f0bf7a",
   "metadata": {},
   "source": [
    "## Save Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4552f41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving datasets...\n",
      "\n",
      "Saving 10,000 EN-AR candidates to english_arabic_candidates.jsonl...\n",
      "   Saved\n",
      "Saving 10,000 FR-AR candidates to french_arabic_candidates.jsonl...\n",
      "   Saved\n",
      "Saving 44,324 EN-AR preference pairs to en-ar-preferences.jsonl...\n",
      "   Saved\n",
      "Saving 46,908 FR-AR preference pairs to fr-ar-preferences.jsonl...\n",
      "   Saved\n",
      "Saving 46,908 FR-AR preference pairs to fr-ar-preferences.jsonl...\n",
      "   Saved\n",
      "\n",
      "================================================================================\n",
      "DATASET STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Candidates:\n",
      "  English to Arabic: 10,000\n",
      "  French to Arabic: 10,000\n",
      "  Total: 20,000\n",
      "\n",
      "Preference Pairs:\n",
      "  Total pairs: 91,232\n",
      "  Average margin: 0.0633\n",
      "  Average chosen score: 0.9166\n",
      "\n",
      "Language Breakdown:\n",
      "  English to Arabic:\n",
      "    Candidates: 10,000\n",
      "    Preference pairs: 44,324\n",
      "    Avg margin: 0.0549\n",
      "    Avg score: 0.9224\n",
      "  French to Arabic:\n",
      "    Candidates: 10,000\n",
      "    Preference pairs: 46,908\n",
      "    Avg margin: 0.0712\n",
      "    Avg score: 0.9111\n",
      "\n",
      "Method Breakdown (Preference Pairs):\n",
      "  temperature:\n",
      "    Chosen: 24,553\n",
      "    Rejected: 24,405\n",
      "    Avg score: 0.8820\n",
      "  top_k:\n",
      "    Chosen: 21,838\n",
      "    Rejected: 22,133\n",
      "    Avg score: 0.8857\n",
      "  nucleus:\n",
      "    Chosen: 23,723\n",
      "    Rejected: 22,458\n",
      "    Avg score: 0.8856\n",
      "  greedy:\n",
      "    Chosen: 21,118\n",
      "    Rejected: 22,236\n",
      "    Avg score: 0.8855\n",
      "\n",
      "Generated files:\n",
      "  - english_arabic_candidates.jsonl\n",
      "  - french_arabic_candidates.jsonl\n",
      "  - en-ar-preferences.jsonl\n",
      "  - fr-ar-preferences.jsonl\n",
      "  - generation_stats.json\n",
      "\n",
      "Statistics saved to outputs/generation_stats.json\n",
      "================================================================================\n",
      "   Saved\n",
      "\n",
      "================================================================================\n",
      "DATASET STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Candidates:\n",
      "  English to Arabic: 10,000\n",
      "  French to Arabic: 10,000\n",
      "  Total: 20,000\n",
      "\n",
      "Preference Pairs:\n",
      "  Total pairs: 91,232\n",
      "  Average margin: 0.0633\n",
      "  Average chosen score: 0.9166\n",
      "\n",
      "Language Breakdown:\n",
      "  English to Arabic:\n",
      "    Candidates: 10,000\n",
      "    Preference pairs: 44,324\n",
      "    Avg margin: 0.0549\n",
      "    Avg score: 0.9224\n",
      "  French to Arabic:\n",
      "    Candidates: 10,000\n",
      "    Preference pairs: 46,908\n",
      "    Avg margin: 0.0712\n",
      "    Avg score: 0.9111\n",
      "\n",
      "Method Breakdown (Preference Pairs):\n",
      "  temperature:\n",
      "    Chosen: 24,553\n",
      "    Rejected: 24,405\n",
      "    Avg score: 0.8820\n",
      "  top_k:\n",
      "    Chosen: 21,838\n",
      "    Rejected: 22,133\n",
      "    Avg score: 0.8857\n",
      "  nucleus:\n",
      "    Chosen: 23,723\n",
      "    Rejected: 22,458\n",
      "    Avg score: 0.8856\n",
      "  greedy:\n",
      "    Chosen: 21,118\n",
      "    Rejected: 22,236\n",
      "    Avg score: 0.8855\n",
      "\n",
      "Generated files:\n",
      "  - english_arabic_candidates.jsonl\n",
      "  - french_arabic_candidates.jsonl\n",
      "  - en-ar-preferences.jsonl\n",
      "  - fr-ar-preferences.jsonl\n",
      "  - generation_stats.json\n",
      "\n",
      "Statistics saved to outputs/generation_stats.json\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save datasets to separate files\n",
    "print(f\"Saving datasets...\\n\")\n",
    "\n",
    "# Save EN-AR candidates\n",
    "print(f\"Saving {len(en_candidates_data):,} EN-AR candidates to {en_ar_candidates_file.name}...\")\n",
    "with open(en_ar_candidates_file, 'w', encoding='utf-8') as f:\n",
    "    for item in en_candidates_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "print(f\"   Saved\")\n",
    "\n",
    "# Save FR-AR candidates\n",
    "print(f\"Saving {len(fr_candidates_data):,} FR-AR candidates to {fr_ar_candidates_file.name}...\")\n",
    "with open(fr_ar_candidates_file, 'w', encoding='utf-8') as f:\n",
    "    for item in fr_candidates_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "print(f\"   Saved\")\n",
    "\n",
    "# Save EN-AR preferences\n",
    "print(f\"Saving {len(en_preferences_data):,} EN-AR preference pairs to {en_ar_preferences_file.name}...\")\n",
    "with open(en_ar_preferences_file, 'w', encoding='utf-8') as f:\n",
    "    for item in en_preferences_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "print(f\"   Saved\")\n",
    "\n",
    "# Save FR-AR preferences\n",
    "print(f\"Saving {len(fr_preferences_data):,} FR-AR preference pairs to {fr_ar_preferences_file.name}...\")\n",
    "with open(fr_ar_preferences_file, 'w', encoding='utf-8') as f:\n",
    "    for item in fr_preferences_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "print(f\"   Saved\")\n",
    "\n",
    "# Calculate statistics\n",
    "en_pairs = en_preferences_data\n",
    "fr_pairs = fr_preferences_data\n",
    "\n",
    "# Analyze which methods produced best pairs\n",
    "method_pair_stats = {}\n",
    "all_pairs = en_pairs + fr_pairs\n",
    "for method_name in GENERATION_METHODS.keys():\n",
    "    chosen_count = sum(1 for item in all_pairs if item['chosen_method'] == method_name)\n",
    "    rejected_count = sum(1 for item in all_pairs if item['rejected_method'] == method_name)\n",
    "    method_pair_stats[method_name] = {\n",
    "        'chosen_count': chosen_count,\n",
    "        'rejected_count': rejected_count\n",
    "    }\n",
    "\n",
    "# Prepare statistics\n",
    "stats = {\n",
    "    'en_candidates': len(en_candidates_data),\n",
    "    'fr_candidates': len(fr_candidates_data),\n",
    "    'total_preference_pairs': len(all_pairs),\n",
    "    'en_pairs': len(en_pairs),\n",
    "    'fr_pairs': len(fr_pairs),\n",
    "    'avg_margin': sum(item['margin'] for item in all_pairs) / len(all_pairs) if all_pairs else 0,\n",
    "    'avg_chosen_score': sum(item['chosen_score'] for item in all_pairs) / len(all_pairs) if all_pairs else 0,\n",
    "    'avg_rejected_score': sum(item['rejected_score'] for item in all_pairs) / len(all_pairs) if all_pairs else 0,\n",
    "    'language_breakdown': {\n",
    "        'english': {\n",
    "            'candidates': len(en_candidates_data),\n",
    "            'pairs': len(en_pairs),\n",
    "            'avg_margin': sum(item['margin'] for item in en_pairs) / len(en_pairs) if en_pairs else 0,\n",
    "            'avg_chosen_score': sum(item['chosen_score'] for item in en_pairs) / len(en_pairs) if en_pairs else 0,\n",
    "        },\n",
    "        'french': {\n",
    "            'candidates': len(fr_candidates_data),\n",
    "            'pairs': len(fr_pairs),\n",
    "            'avg_margin': sum(item['margin'] for item in fr_pairs) / len(fr_pairs) if fr_pairs else 0,\n",
    "            'avg_chosen_score': sum(item['chosen_score'] for item in fr_pairs) / len(fr_pairs) if fr_pairs else 0,\n",
    "        }\n",
    "    },\n",
    "    'method_breakdown': {\n",
    "        method_name: {\n",
    "            'pairs_as_chosen': method_pair_stats[method_name]['chosen_count'],\n",
    "            'pairs_as_rejected': method_pair_stats[method_name]['rejected_count'],\n",
    "            'avg_score': method_stats[method_name]['avg_score'],\n",
    "            'total_candidates': method_stats[method_name]['count']\n",
    "        }\n",
    "        for method_name in GENERATION_METHODS.keys()\n",
    "    },\n",
    "    'scoring_method': 'comet' if USE_COMET else 'heuristic'\n",
    "}\n",
    "\n",
    "stats_path = OUTPUTS_DIR / \"generation_stats.json\"\n",
    "with open(stats_path, 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nCandidates:\")\n",
    "print(f\"  English to Arabic: {stats['language_breakdown']['english']['candidates']:,}\")\n",
    "print(f\"  French to Arabic: {stats['language_breakdown']['french']['candidates']:,}\")\n",
    "print(f\"  Total: {stats['en_candidates'] + stats['fr_candidates']:,}\")\n",
    "\n",
    "print(f\"\\nPreference Pairs:\")\n",
    "print(f\"  Total pairs: {stats['total_preference_pairs']:,}\")\n",
    "print(f\"  Average margin: {stats['avg_margin']:.4f}\")\n",
    "print(f\"  Average chosen score: {stats['avg_chosen_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nLanguage Breakdown:\")\n",
    "print(f\"  English to Arabic:\")\n",
    "print(f\"    Candidates: {stats['language_breakdown']['english']['candidates']:,}\")\n",
    "print(f\"    Preference pairs: {stats['language_breakdown']['english']['pairs']:,}\")\n",
    "print(f\"    Avg margin: {stats['language_breakdown']['english']['avg_margin']:.4f}\")\n",
    "print(f\"    Avg score: {stats['language_breakdown']['english']['avg_chosen_score']:.4f}\")\n",
    "\n",
    "print(f\"  French to Arabic:\")\n",
    "print(f\"    Candidates: {stats['language_breakdown']['french']['candidates']:,}\")\n",
    "print(f\"    Preference pairs: {stats['language_breakdown']['french']['pairs']:,}\")\n",
    "print(f\"    Avg margin: {stats['language_breakdown']['french']['avg_margin']:.4f}\")\n",
    "print(f\"    Avg score: {stats['language_breakdown']['french']['avg_chosen_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nMethod Breakdown (Preference Pairs):\")\n",
    "for method_name, info in stats['method_breakdown'].items():\n",
    "    print(f\"  {method_name}:\")\n",
    "    print(f\"    Chosen: {info['pairs_as_chosen']:,}\")\n",
    "    print(f\"    Rejected: {info['pairs_as_rejected']:,}\")\n",
    "    print(f\"    Avg score: {info['avg_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"  - {en_ar_candidates_file.name}\")\n",
    "print(f\"  - {fr_ar_candidates_file.name}\")\n",
    "print(f\"  - {en_ar_preferences_file.name}\")\n",
    "print(f\"  - {fr_ar_preferences_file.name}\")\n",
    "print(f\"  - {stats_path.name}\")\n",
    "\n",
    "print(f\"\\nStatistics saved to {stats_path}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344339ab",
   "metadata": {},
   "source": [
    "## Sample Preference Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80684f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Generated Data\n",
      "\n",
      "================================================================================\n",
      "SAMPLE TRANSLATION CANDIDATES (4 methods per source)\n",
      "================================================================================\n",
      "\n",
      "Example 1: ENAR\n",
      "Source: Implementation. Under a mandate generally deriving from Article 98 of the Charter of the United Nati\n",
      "Candidates:\n",
      "  1. [temperature]:  -        98        \n",
      "  2. [top_k]:  -        98        \n",
      "  3. [nucleus]:  -        98        \n",
      "  4. [greedy]:  -       98        \n",
      "\n",
      "Example 2: ENAR\n",
      "Source: \"In this respect, a message from the President of Maldives, Maumoon Abdul Gayoom, was read to the me\n",
      "Candidates:\n",
      "  1. [temperature]: \"                \n",
      "  2. [top_k]: \"                 \n",
      "  3. [nucleus]: \"                \n",
      "  4. [greedy]: \"                 \n",
      "\n",
      "Example 3: FRAR\n",
      "Source: Ils taient dj noys par le fumier. (Rires) Donc en 1860, ils voyaient cette technologie sale qui \n",
      "Candidates:\n",
      "  1. [temperature]:    . ()    1860         \n",
      "  2. [top_k]:    . ()    1860        \n",
      "  3. [nucleus]:      ()   1860       \n",
      "  4. [greedy]:     . ()    1860       \n",
      "\n",
      "================================================================================\n",
      "SAMPLE PREFERENCE PAIRS\n",
      "================================================================================\n",
      "\n",
      "Example 1: ENAR\n",
      "Source: At the beginning of 360 BC, Nectanebo's predecessor, Teos, started preparations for war against intr\n",
      "\n",
      " Chosen (temperature, score: 0.888):\n",
      "     360          .     \n",
      "\n",
      " Rejected (nucleus, score: 0.878):\n",
      "     360          .    \n",
      "\n",
      "Margin: 0.010\n",
      "\n",
      "Example 2: ENAR\n",
      "Source: Death penalty in America is defined by error.\n",
      "\n",
      " Chosen (nucleus, score: 0.944):\n",
      "          .\n",
      "\n",
      " Rejected (top_k, score: 0.875):\n",
      "       .\n",
      "\n",
      "Margin: 0.069\n",
      "\n",
      "Example 3: FRAR\n",
      "Source: Et l, sur Internet, ils allaient inviter de parfaits inconnus  dormir chez eux.\n",
      "\n",
      " Chosen (top_k, score: 1.000):\n",
      "               .\n",
      "\n",
      " Rejected (nucleus, score: 0.857):\n",
      "           .\n",
      "\n",
      "Margin: 0.143\n",
      "\n",
      "================================================================================\n",
      "METHOD PERFORMANCE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "temperature:\n",
      "  Chosen: 24,553 (50.2%)\n",
      "  Rejected: 24,405 (49.8%)\n",
      "  Total: 48,958\n",
      "\n",
      "top_k:\n",
      "  Chosen: 21,838 (49.7%)\n",
      "  Rejected: 22,133 (50.3%)\n",
      "  Total: 43,971\n",
      "\n",
      "nucleus:\n",
      "  Chosen: 23,723 (51.4%)\n",
      "  Rejected: 22,458 (48.6%)\n",
      "  Total: 46,181\n",
      "\n",
      "greedy:\n",
      "  Chosen: 21,118 (48.7%)\n",
      "  Rejected: 22,236 (51.3%)\n",
      "  Total: 43,354\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display sample generated data\n",
    "print(\"Sample Generated Data\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show sample candidates\n",
    "print(\"SAMPLE TRANSLATION CANDIDATES (4 methods per source)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "en_examples = random.sample(en_candidates_data, min(2, len(en_candidates_data)))\n",
    "fr_examples = random.sample(fr_candidates_data, min(1, len(fr_candidates_data)))\n",
    "\n",
    "for i, item in enumerate(en_examples + fr_examples, 1):\n",
    "    lang_label = 'ENAR' if item['source_lang'] == 'en' else 'FRAR'\n",
    "    print(f\"\\nExample {i}: {lang_label}\")\n",
    "    print(f\"Source: {item['source'][:100]}\")\n",
    "    print(f\"Candidates:\")\n",
    "    for j, cand in enumerate(item['candidates'], 1):\n",
    "        print(f\"  {j}. [{cand['method']}]: {cand['translation'][:100]}\")\n",
    "\n",
    "# Show sample preferences\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE PREFERENCE PAIRS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "en_pref_examples = random.sample(en_preferences_data, min(2, len(en_preferences_data)))\n",
    "fr_pref_examples = random.sample(fr_preferences_data, min(1, len(fr_preferences_data)))\n",
    "\n",
    "for i, item in enumerate(en_pref_examples + fr_pref_examples, 1):\n",
    "    lang_label = 'ENAR' if item['source_lang'] == 'en' else 'FRAR'\n",
    "    print(f\"\\nExample {i}: {lang_label}\")\n",
    "    print(f\"Source: {item['source'][:100]}\")\n",
    "    print(f\"\\n Chosen ({item['chosen_method']}, score: {item['chosen_score']:.3f}):\")\n",
    "    print(f\"  {item['chosen'][:100]}\")\n",
    "    print(f\"\\n Rejected ({item['rejected_method']}, score: {item['rejected_score']:.3f}):\")\n",
    "    print(f\"  {item['rejected'][:100]}\")\n",
    "    print(f\"\\nMargin: {item['margin']:.3f}\")\n",
    "\n",
    "# Method performance summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"METHOD PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_pairs = en_preferences_data + fr_preferences_data\n",
    "for method_name in GENERATION_METHODS.keys():\n",
    "    chosen = sum(1 for item in all_pairs if item['chosen_method'] == method_name)\n",
    "    rejected = sum(1 for item in all_pairs if item['rejected_method'] == method_name)\n",
    "    total = chosen + rejected\n",
    "    if total > 0:\n",
    "        chosen_pct = 100 * chosen / total\n",
    "        print(f\"\\n{method_name}:\")\n",
    "        print(f\"  Chosen: {chosen:,} ({chosen_pct:.1f}%)\")\n",
    "        print(f\"  Rejected: {rejected:,} ({100-chosen_pct:.1f}%)\")\n",
    "        print(f\"  Total: {total:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fcaf8e",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "Proceed to **notebook 2** to train the reward model using this synthetic preference data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
