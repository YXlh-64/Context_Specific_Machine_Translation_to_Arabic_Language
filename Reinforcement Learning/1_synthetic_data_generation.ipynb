{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24c70776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/imane/gemma_env/lib/python3.12/site-packages (4.57.3)\n",
      "Requirement already satisfied: accelerate in /home/imane/gemma_env/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: torch in /home/imane/gemma_env/lib/python3.12/site-packages (2.10.0.dev20251203+cu126)\n",
      "Requirement already satisfied: datasets in /home/imane/gemma_env/lib/python3.12/site-packages (4.4.1)\n",
      "Requirement already satisfied: trl in /home/imane/gemma_env/lib/python3.12/site-packages (0.25.1)\n",
      "Requirement already satisfied: peft in /home/imane/gemma_env/lib/python3.12/site-packages (0.18.0)\n",
      "Requirement already satisfied: bitsandbytes in /home/imane/gemma_env/lib/python3.12/site-packages (0.48.2)\n",
      "Requirement already satisfied: filelock in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: filelock in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/imane/gemma_env/lib/python3.12/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: pytorch-triton==3.5.1+gitbfeb0668 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (3.5.1+gitbfeb0668)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from cuda-bindings==12.9.4->torch) (1.2.2)\n",
      "Requirement already satisfied: psutil in /home/imane/gemma_env/lib/python3.12/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: pytorch-triton==3.5.1+gitbfeb0668 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch) (3.5.1+gitbfeb0668)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from cuda-bindings==12.9.4->torch) (1.2.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/imane/gemma_env/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /home/imane/gemma_env/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/imane/gemma_env/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/imane/gemma_env/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /home/imane/gemma_env/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/imane/gemma_env/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/imane/gemma_env/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /home/imane/gemma_env/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /home/imane/gemma_env/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/imane/gemma_env/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/imane/gemma_env/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/imane/gemma_env/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/imane/gemma_env/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /home/imane/gemma_env/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /home/imane/gemma_env/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/imane/gemma_env/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/imane/gemma_env/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/imane/gemma_env/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/imane/gemma_env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/imane/gemma_env/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from requests->transformers) (2.6.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/imane/gemma_env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/imane/gemma_env/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from requests->transformers) (2.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/imane/gemma_env/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/imane/gemma_env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/imane/gemma_env/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/imane/gemma_env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: comet-ml in /home/imane/gemma_env/lib/python3.12/site-packages (3.54.2)\n",
      "Requirement already satisfied: unbabel-comet in /home/imane/gemma_env/lib/python3.12/site-packages (2.2.7)\n",
      "Requirement already satisfied: bert-score in /home/imane/gemma_env/lib/python3.12/site-packages (0.3.13)\n",
      "Requirement already satisfied: sacrebleu in /home/imane/gemma_env/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: dulwich!=0.20.33,>=0.20.6 in /home/imane/gemma_env/lib/python3.12/site-packages (from comet-ml) (0.24.10)\n",
      "Requirement already satisfied: everett<3.2.0,>=1.0.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from everett[ini]<3.2.0,>=1.0.1->comet-ml) (3.1.0)\n",
      "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from comet-ml) (4.25.1)\n",
      "Requirement already satisfied: psutil>=5.6.3 in /home/imane/gemma_env/lib/python3.12/site-packages (from comet-ml) (7.1.3)\n",
      "Requirement already satisfied: python-box<7.0.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from comet-ml) (6.1.0)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from comet-ml) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in /home/imane/gemma_env/lib/python3.12/site-packages (from comet-ml) (2.32.5)\n",
      "Requirement already satisfied: rich>=13.3.2 in /home/imane/gemma_env/lib/python3.12/site-packages (from comet-ml) (14.2.0)\n",
      "Requirement already satisfied: semantic-version>=2.8.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from comet-ml) (2.10.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.1.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from comet-ml) (2.47.0)\n",
      "Requirement already satisfied: setuptools in /home/imane/gemma_env/lib/python3.12/site-packages (from comet-ml) (70.2.0)\n",
      "Requirement already satisfied: simplejson in /home/imane/gemma_env/lib/python3.12/site-packages (from comet-ml) (3.20.2)\n",
      "Requirement already satisfied: urllib3>=1.26.8 in /home/imane/gemma_env/lib/python3.12/site-packages (from comet-ml) (2.6.0)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in /home/imane/gemma_env/lib/python3.12/site-packages (from comet-ml) (2.0.1)\n",
      "Requirement already satisfied: wurlitzer>=1.0.2 in /home/imane/gemma_env/lib/python3.12/site-packages (from comet-ml) (3.1.1)\n",
      "Requirement already satisfied: entmax<2.0,>=1.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from unbabel-comet) (1.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/imane/gemma_env/lib/python3.12/site-packages (from unbabel-comet) (0.36.0)\n",
      "Requirement already satisfied: jsonargparse==3.13.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from unbabel-comet) (3.13.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.20.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from unbabel-comet) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.4.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from unbabel-comet) (2.3.3)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.24.4 in /home/imane/gemma_env/lib/python3.12/site-packages (from unbabel-comet) (4.25.8)\n",
      "Requirement already satisfied: pytorch-lightning<3.0.0,>=2.0.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from unbabel-comet) (2.6.0)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.5.4 in /home/imane/gemma_env/lib/python3.12/site-packages (from unbabel-comet) (1.16.3)\n",
      "Requirement already satisfied: sentencepiece<0.3.0,>=0.2.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from unbabel-comet) (0.2.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from unbabel-comet) (2.10.0.dev20251203+cu126)\n",
      "Requirement already satisfied: torchmetrics<0.11.0,>=0.10.2 in /home/imane/gemma_env/lib/python3.12/site-packages (from unbabel-comet) (0.10.3)\n",
      "Requirement already satisfied: transformers<5.0,>=4.17 in /home/imane/gemma_env/lib/python3.12/site-packages (from unbabel-comet) (4.57.3)\n",
      "Requirement already satisfied: PyYAML>=3.13 in /home/imane/gemma_env/lib/python3.12/site-packages (from jsonargparse==3.13.1->unbabel-comet) (6.0.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from bert-score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /home/imane/gemma_env/lib/python3.12/site-packages (from bert-score) (3.10.7)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/imane/gemma_env/lib/python3.12/site-packages (from bert-score) (25.0)\n",
      "Requirement already satisfied: portalocker in /home/imane/gemma_env/lib/python3.12/site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: regex in /home/imane/gemma_env/lib/python3.12/site-packages (from sacrebleu) (2025.11.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/imane/gemma_env/lib/python3.12/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in /home/imane/gemma_env/lib/python3.12/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /home/imane/gemma_env/lib/python3.12/site-packages (from sacrebleu) (6.0.2)\n",
      "Requirement already satisfied: configobj in /home/imane/gemma_env/lib/python3.12/site-packages (from everett[ini]<3.2.0,>=1.0.1->comet-ml) (5.0.9)\n",
      "Requirement already satisfied: PyYAML>=3.13 in /home/imane/gemma_env/lib/python3.12/site-packages (from jsonargparse==3.13.1->unbabel-comet) (6.0.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from bert-score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /home/imane/gemma_env/lib/python3.12/site-packages (from bert-score) (3.10.7)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/imane/gemma_env/lib/python3.12/site-packages (from bert-score) (25.0)\n",
      "Requirement already satisfied: portalocker in /home/imane/gemma_env/lib/python3.12/site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: regex in /home/imane/gemma_env/lib/python3.12/site-packages (from sacrebleu) (2025.11.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/imane/gemma_env/lib/python3.12/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in /home/imane/gemma_env/lib/python3.12/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /home/imane/gemma_env/lib/python3.12/site-packages (from sacrebleu) (6.0.2)\n",
      "Requirement already satisfied: configobj in /home/imane/gemma_env/lib/python3.12/site-packages (from everett[ini]<3.2.0,>=1.0.1->comet-ml) (5.0.9)\n",
      "Requirement already satisfied: filelock in /home/imane/gemma_env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/imane/gemma_env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/imane/gemma_env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (1.2.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/imane/gemma_env/lib/python3.12/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/imane/gemma_env/lib/python3.12/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.30.0)\n",
      "Requirement already satisfied: filelock in /home/imane/gemma_env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/imane/gemma_env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/imane/gemma_env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (1.2.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/imane/gemma_env/lib/python3.12/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/imane/gemma_env/lib/python3.12/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.30.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/imane/gemma_env/lib/python3.12/site-packages (from pandas>=1.4.1->unbabel-comet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from pandas>=1.4.1->unbabel-comet) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/imane/gemma_env/lib/python3.12/site-packages (from pandas>=1.4.1->unbabel-comet) (2025.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/imane/gemma_env/lib/python3.12/site-packages (from pandas>=1.4.1->unbabel-comet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from pandas>=1.4.1->unbabel-comet) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/imane/gemma_env/lib/python3.12/site-packages (from pandas>=1.4.1->unbabel-comet) (2025.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.15.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/imane/gemma_env/lib/python3.12/site-packages (from requests>=2.18.4->comet-ml) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from requests>=2.18.4->comet-ml) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/imane/gemma_env/lib/python3.12/site-packages (from requests>=2.18.4->comet-ml) (2025.11.12)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from rich>=13.3.2->comet-ml) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from rich>=13.3.2->comet-ml) (2.19.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/imane/gemma_env/lib/python3.12/site-packages (from requests>=2.18.4->comet-ml) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from requests>=2.18.4->comet-ml) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/imane/gemma_env/lib/python3.12/site-packages (from requests>=2.18.4->comet-ml) (2025.11.12)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from rich>=13.3.2->comet-ml) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from rich>=13.3.2->comet-ml) (2.19.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (3.1.6)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (1.11.1.6)\n",
      "Requirement already satisfied: pytorch-triton==3.5.1+gitbfeb0668 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (3.5.1+gitbfeb0668)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from cuda-bindings==12.9.4->torch>=1.6.0->unbabel-comet) (1.2.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (3.1.6)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (1.11.1.6)\n",
      "Requirement already satisfied: pytorch-triton==3.5.1+gitbfeb0668 in /home/imane/gemma_env/lib/python3.12/site-packages (from torch>=1.6.0->unbabel-comet) (3.5.1+gitbfeb0668)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from cuda-bindings==12.9.4->torch>=1.6.0->unbabel-comet) (1.2.2)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.7.0)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/imane/gemma_env/lib/python3.12/site-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.7.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from matplotlib->bert-score) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/imane/gemma_env/lib/python3.12/site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from matplotlib->bert-score) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from matplotlib->bert-score) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /home/imane/gemma_env/lib/python3.12/site-packages (from matplotlib->bert-score) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/imane/gemma_env/lib/python3.12/site-packages (from matplotlib->bert-score) (3.2.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from matplotlib->bert-score) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/imane/gemma_env/lib/python3.12/site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from matplotlib->bert-score) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from matplotlib->bert-score) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /home/imane/gemma_env/lib/python3.12/site-packages (from matplotlib->bert-score) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/imane/gemma_env/lib/python3.12/site-packages (from matplotlib->bert-score) (3.2.5)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/imane/gemma_env/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (3.13.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet-ml) (0.1.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/imane/gemma_env/lib/python3.12/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (3.13.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet-ml) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.4.1->unbabel-comet) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.6.0->unbabel-comet) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.4.1->unbabel-comet) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.6.0->unbabel-comet) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from jinja2->torch>=1.6.0->unbabel-comet) (2.1.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.22.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from jinja2->torch>=1.6.0->unbabel-comet) (2.1.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.22.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /home/imane/gemma_env/lib/python3.12/site-packages (0.23.1)\n",
      "Requirement already satisfied: pandas in /home/imane/gemma_env/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in /home/imane/gemma_env/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in /home/imane/gemma_env/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: click>=8.0.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from wandb) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: packaging in /home/imane/gemma_env/lib/python3.12/site-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in /home/imane/gemma_env/lib/python3.12/site-packages (from wandb) (4.5.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from wandb) (4.25.8)\n",
      "Requirement already satisfied: pydantic<3 in /home/imane/gemma_env/lib/python3.12/site-packages (from wandb) (2.12.5)\n",
      "Requirement already satisfied: pyyaml in /home/imane/gemma_env/lib/python3.12/site-packages (from wandb) (6.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from wandb) (2.32.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from wandb) (2.47.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /home/imane/gemma_env/lib/python3.12/site-packages (from wandb) (4.15.0)\n",
      "Requirement already satisfied: numpy in /home/imane/gemma_env/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in /home/imane/gemma_env/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: click>=8.0.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from wandb) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: packaging in /home/imane/gemma_env/lib/python3.12/site-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in /home/imane/gemma_env/lib/python3.12/site-packages (from wandb) (4.5.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from wandb) (4.25.8)\n",
      "Requirement already satisfied: pydantic<3 in /home/imane/gemma_env/lib/python3.12/site-packages (from wandb) (2.12.5)\n",
      "Requirement already satisfied: pyyaml in /home/imane/gemma_env/lib/python3.12/site-packages (from wandb) (6.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from wandb) (2.32.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from wandb) (2.47.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /home/imane/gemma_env/lib/python3.12/site-packages (from wandb) (4.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/imane/gemma_env/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/imane/gemma_env/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from pydantic<3->wandb) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/imane/gemma_env/lib/python3.12/site-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/imane/gemma_env/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/imane/gemma_env/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/imane/gemma_env/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/imane/gemma_env/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/imane/gemma_env/lib/python3.12/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from pydantic<3->wandb) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/imane/gemma_env/lib/python3.12/site-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/imane/gemma_env/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/imane/gemma_env/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/imane/gemma_env/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/imane/gemma_env/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "GPU Available: True\n",
      "GPU Name: NVIDIA GeForce RTX 5090\n",
      "GPU Memory: 33.67 GB\n",
      "GPU Available: True\n",
      "GPU Name: NVIDIA GeForce RTX 5090\n",
      "GPU Memory: 33.67 GB\n",
      "Using virtual environment at: /home/imane/gemma_env\n",
      "Hugging Face token loaded from cache.\n",
      "Directory structure and model paths are ready!\n",
      "Configuration loaded successfully!\n",
      "Utility functions loaded!\n",
      "Configuration saved to /home/imane/Downloads/Reinforcement Learning/config.json\n",
      "Using virtual environment at: /home/imane/gemma_env\n",
      "Hugging Face token loaded from cache.\n",
      "Directory structure and model paths are ready!\n",
      "Configuration loaded successfully!\n",
      "Utility functions loaded!\n",
      "Configuration saved to /home/imane/Downloads/Reinforcement Learning/config.json\n"
     ]
    }
   ],
   "source": [
    "# Run setup from config notebook\n",
    "%run 0_config_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8419bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef9d6c",
   "metadata": {},
   "source": [
    "## Load SFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e480820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPU cache cleared\n",
      "Device mode: CPU (forced)\n",
      " Using CPU mode to avoid 'no kernel image' error\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU cache if needed\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Set CUDA environment variable for better error reporting\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\" GPU cache cleared\")\n",
    "\n",
    "# Force CPU mode due to CUDA kernel incompatibility\n",
    "FORCE_CPU = True  # Set to True to use CPU (RECOMMENDED due to kernel errors)\n",
    "USE_8BIT = False  # Only relevant if FORCE_CPU=False\n",
    "print(f\"Device mode: {'CPU (forced)' if FORCE_CPU else ('GPU with 8-bit quantization' if USE_8BIT else 'GPU (full precision)')}\")\n",
    "print(\" Using CPU mode to avoid 'no kernel image' error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9d02f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SFT model from Hugging Face...\n",
      " Loading model on CPU (FORCE_CPU=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imane/gemma_env/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1041: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/imane/gemma_env/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/imane/gemma_env/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|| 5/5 [00:20<00:00,  4.03s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model loaded on CPU\n",
      "Device placement: {'': 'cpu'}\n",
      "Model size: 9.24B parameters\n",
      " Model ready for inference\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading SFT model from Hugging Face...\")\n",
    "\n",
    "# Load directly from Hugging Face Hub\n",
    "model_name = \"ModelSpace/GemmaX2-28-9B-v0.1\"\n",
    "\n",
    "# Load with forced CPU mode if kernel incompatibility detected\n",
    "if FORCE_CPU:\n",
    "    print(\" Loading model on CPU (FORCE_CPU=True)\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map={\"\": \"cpu\"},\n",
    "        torch_dtype=torch.float32,\n",
    "        trust_remote_code=True,\n",
    "        use_auth_token=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    print(\" Model loaded on CPU\")\n",
    "elif USE_8BIT:\n",
    "    print(\" Loading model on GPU with 8-bit quantization\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        load_in_8bit=True,\n",
    "        trust_remote_code=True,\n",
    "        use_auth_token=True\n",
    "    )\n",
    "    print(\" Model loaded on GPU with 8-bit quantization\")\n",
    "else:\n",
    "    # Attempt to load on GPU (device_map=\"auto\") and fall back to CPU if we hit CUDA/kernel errors\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True, trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            use_auth_token=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(\" Model loaded onto GPU/accelerator (device_map='auto').\")\n",
    "    except Exception as e:\n",
    "        err = str(e)\n",
    "        print(\" Warning: failed to load model on GPU/accelerator:\", err)\n",
    "        # Heuristics: handle CUDA driver / kernel mismatch or OOM by falling back to CPU\n",
    "        if (\"no kernel image\" in err) or (\"CUDA error\" in err) or (\"out of memory\" in err) or (\"failed to load\" in err) or (\"CUDA\" in err and \"kernel\" in err):\n",
    "            print(\" Falling back to CPU to avoid CUDA/kernel incompatibility.\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True, trust_remote_code=True)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map={\"\": \"cpu\"},\n",
    "                torch_dtype=torch.float32,\n",
    "                trust_remote_code=True,\n",
    "                use_auth_token=True,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "        else:\n",
    "            # re-raise unexpected errors\n",
    "            raise\n",
    "\n",
    "# Ensure padding token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Report device placement\n",
    "try:\n",
    "    device_summary = model.hf_device_map if hasattr(model, 'hf_device_map') else { '': str(next(model.parameters()).device) }\n",
    "    print(\"Device placement:\", device_summary)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(f\"Model size: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters\")\n",
    "print(\" Model ready for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c20a72",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9272a129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test prompts for synthetic data generation...\n",
      "Note: We don't need reference translations - just source texts\n",
      "\n",
      "Loaded 91 test prompts from file\n",
      "\n",
      "Using 91 samples for synthetic data generation\n"
     ]
    }
   ],
   "source": [
    "# Load test prompts (NO parallel corpus needed - Phase 1 already complete!)\n",
    "print(\"Loading test prompts for synthetic data generation...\")\n",
    "print(\"Note: We don't need reference translations - just source texts\\n\")\n",
    "\n",
    "try:\n",
    "    all_data = load_test_prompts(TEST_PROMPTS)\n",
    "    print(f\"Loaded {len(all_data)} test prompts from file\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Creating sample test prompts...\")\n",
    "    # Create sample prompts if file doesn't exist\n",
    "    all_data = [\n",
    "        {\"source\": \"Hello, how are you?\", \"source_lang\": \"en\"},\n",
    "        {\"source\": \"Good morning, have a nice day.\", \"source_lang\": \"en\"},\n",
    "        {\"source\": \"Thank you for your help.\", \"source_lang\": \"en\"},\n",
    "        {\"source\": \"Where is the nearest hospital?\", \"source_lang\": \"en\"},\n",
    "        {\"source\": \"I would like to order some food.\", \"source_lang\": \"en\"},\n",
    "        {\"source\": \"Bonjour, comment allez-vous?\", \"source_lang\": \"fr\"},\n",
    "        {\"source\": \"Merci beaucoup pour votre aide.\", \"source_lang\": \"fr\"},\n",
    "        {\"source\": \"O se trouve la gare?\", \"source_lang\": \"fr\"},\n",
    "    ] * 100  # Replicate for testing\n",
    "    print(f\"Created {len(all_data)} sample prompts\")\n",
    "\n",
    "# Use a subset for synthetic data generation (adjust as needed)\n",
    "SYNTHETIC_DATA_SIZE = min(5000, len(all_data))\n",
    "random.shuffle(all_data)\n",
    "training_samples = all_data[:SYNTHETIC_DATA_SIZE]\n",
    "\n",
    "print(f\"\\nUsing {len(training_samples)} samples for synthetic data generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aff54f2",
   "metadata": {},
   "source": [
    "## Generate Translation Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b63b4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with: Whether organize files will group directories according to their filetype....\n",
      "\n",
      "Generated 8 candidates:\n",
      "1.         ....\n",
      "2.        ....\n",
      "3.         ....\n",
      "4.      ....\n",
      "5.           ....\n",
      "6.         ....\n",
      "7.        ....\n",
      "8.          ....\n",
      "\n",
      "Generated 8 candidates:\n",
      "1.         ....\n",
      "2.        ....\n",
      "3.         ....\n",
      "4.      ....\n",
      "5.           ....\n",
      "6.         ....\n",
      "7.        ....\n",
      "8.          ....\n"
     ]
    }
   ],
   "source": [
    "def generate_candidates(source_text, source_lang, num_candidates=8):\n",
    "    \"\"\"Generate multiple translation candidates with varying sampling parameters\"\"\"\n",
    "    \n",
    "    prompt = format_translation_prompt(source_text, source_lang)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    # Generate candidates with different sampling strategies\n",
    "    sampling_configs = []\n",
    "    for temp in TEMPERATURES:\n",
    "        for top_k in TOP_K_VALUES:\n",
    "            for top_p in TOP_P_VALUES:\n",
    "                sampling_configs.append({\n",
    "                    'temperature': temp,\n",
    "                    'top_k': top_k,\n",
    "                    'top_p': top_p\n",
    "                })\n",
    "    \n",
    "    # Sample configurations to get desired number of candidates\n",
    "    selected_configs = random.sample(sampling_configs, min(num_candidates, len(sampling_configs)))\n",
    "    \n",
    "    for config in selected_configs:\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                temperature=config['temperature'],\n",
    "                top_k=config['top_k'],\n",
    "                top_p=config['top_p'],\n",
    "                do_sample=True,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode and extract translation\n",
    "        full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Extract only the generated translation (after the prompt)\n",
    "        translation = full_text.split(\"Arabic translation:\")[-1].strip()\n",
    "        \n",
    "        candidates.append({\n",
    "            'translation': translation,\n",
    "            'config': config\n",
    "        })\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "# Test generation\n",
    "test_sample = training_samples[0]\n",
    "print(f\"Testing with: {test_sample['source'][:100]}...\")\n",
    "test_candidates = generate_candidates(test_sample['source'], test_sample['source_lang'], num_candidates=8)\n",
    "print(f\"\\nGenerated {len(test_candidates)} candidates:\")\n",
    "for i, cand in enumerate(test_candidates, 1):\n",
    "    print(f\"{i}. {cand['translation'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c927da82",
   "metadata": {},
   "source": [
    "## Load Automatic Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb2fc098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading evaluation metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|| 5/5 [00:00<00:00, 34267.19it/s]\n",
      "\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "Encoder model frozen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " COMET loaded\n",
      " CHRF loaded\n",
      " BERTScore (will load on first use)\n",
      "\n",
      "All metrics ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imane/gemma_env/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "from bert_score import score as bert_score\n",
    "from sacrebleu.metrics import CHRF\n",
    "\n",
    "print(\"Loading evaluation metrics...\")\n",
    "\n",
    "# Load COMET model\n",
    "comet_model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "comet_model = load_from_checkpoint(comet_model_path)\n",
    "print(\" COMET loaded\")\n",
    "\n",
    "# CHRF metric\n",
    "chrf_metric = CHRF()\n",
    "print(\" CHRF loaded\")\n",
    "\n",
    "print(\" BERTScore (will load on first use)\")\n",
    "print(\"\\nAll metrics ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7469f6d7",
   "metadata": {},
   "source": [
    "## Score Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b29fb17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 5090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 5090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/imane/gemma_env/lib/python3.12/site-packages/pytorch_lightning/utilities/_pytree.py:21: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
      "/home/imane/gemma_env/lib/python3.12/site-packages/pytorch_lightning/utilities/_pytree.py:21: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Scores: {'comet': '0.0000', 'length_score': '0.9000', 'arabic_score': '1.0000', 'combined': '0.2800'}\n"
     ]
    }
   ],
   "source": [
    "def score_translation(source, translation):\n",
    "    \"\"\"Score a translation using reference-free metrics.\n",
    "\n",
    "    Returns a dict with keys: 'comet', 'length_score', 'arabic_score', 'combined'.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "\n",
    "    # COMET-QE (Quality Estimation - no reference needed)\n",
    "    try:\n",
    "        comet_input = [{\n",
    "            'src': source,\n",
    "            'mt': translation\n",
    "        }]\n",
    "        comet_output = comet_model.predict(comet_input, batch_size=1, gpus=1)\n",
    "        scores['comet'] = float(comet_output.scores[0])\n",
    "    except Exception as e:\n",
    "        # If COMET fails, warn and use a fallback score of 0.0\n",
    "        print(\"Warning: COMET scoring failed:\", e)\n",
    "        scores['comet'] = 0.0\n",
    "\n",
    "    # Simple heuristic scores (no reference needed)\n",
    "    src_len = len(source.split())\n",
    "    tgt_len = len(translation.split())\n",
    "    length_ratio = min(tgt_len, src_len * 2) / max(src_len, 1)  # Penalize very long/short\n",
    "    scores['length_score'] = float(min(length_ratio, 1.0))\n",
    "\n",
    "    # 2. Contains Arabic characters (basic validation)\n",
    "    arabic_chars = sum(1 for c in translation if '\\u0600' <= c <= '\\u06FF')\n",
    "    scores['arabic_score'] = float(min(arabic_chars / max(tgt_len, 1), 1.0))\n",
    "\n",
    "    # Weighted combined score\n",
    "    combined_score = (\n",
    "        scores['comet'] * 0.7 +  # Main quality metric\n",
    "        scores['length_score'] * 0.2 +\n",
    "        scores['arabic_score'] * 0.1\n",
    "    )\n",
    "    scores['combined'] = float(combined_score)\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Quick self-test (prints scores for the first test candidate if available)\n",
    "try:\n",
    "    test_scores = score_translation(test_sample['source'], test_candidates[0]['translation'])\n",
    "    print(\"Scores:\", {k: f\"{v:.4f}\" for k, v in test_scores.items()})\n",
    "except Exception as e:\n",
    "    print(\"Test scoring failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e803f392",
   "metadata": {},
   "source": [
    "## Generate Synthetic Preference Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead7467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic preference dataset...\n",
      "Processing 91 samples...\n",
      "\n",
      "Saving generated candidates to /home/imane/Downloads/Reinforcement Learning/data/generated_candidates.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating preferences:   0%|          | 0/91 [00:00<?, ?it/s] Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating preferences:   1%|          | 1/91 [02:16<3:24:06, 136.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating preferences:   2%|         | 2/91 [04:06<2:59:51, 121.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating preferences:   3%|         | 3/91 [05:45<2:42:31, 110.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating preferences:   4%|         | 4/91 [11:57<5:10:03, 213.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating preferences:   5%|         | 5/91 [15:52<5:17:49, 221.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating preferences:   7%|         | 6/91 [18:12<4:34:23, 193.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating preferences:   8%|         | 7/91 [26:54<7:01:23, 301.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating preferences:   9%|         | 8/91 [30:29<6:18:56, 273.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating preferences:  10%|         | 9/91 [31:44<4:49:06, 211.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating preferences:  11%|         | 10/91 [34:07<4:17:06, 190.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating preferences:  12%|        | 11/91 [35:53<3:39:35, 164.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating preferences:  13%|        | 12/91 [37:47<3:16:15, 149.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating preferences:  14%|        | 13/91 [40:45<3:25:28, 158.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating preferences:  15%|        | 14/91 [44:17<3:43:34, 174.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating preferences:  16%|        | 15/91 [47:21<3:44:15, 177.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating preferences:  18%|        | 16/91 [52:11<4:23:55, 211.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating preferences:  19%|        | 17/91 [54:22<3:50:53, 187.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating preferences:  20%|        | 18/91 [57:34<3:49:31, 188.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating preferences:  21%|        | 19/91 [59:05<3:10:50, 159.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: COMET scoring failed: CUDA error: no kernel image is available for execution on the device\n",
      "Search for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_preference_pairs(candidates_with_scores):\n",
    "    \"\"\"Create pairwise preference data from scored candidates\"\"\"\n",
    "\n",
    "    preference_pairs = []\n",
    "\n",
    "    # Sort candidates by combined score\n",
    "    sorted_candidates = sorted(candidates_with_scores, key=lambda x: x['scores']['combined'], reverse=True)\n",
    "\n",
    "    # Create pairs: better vs worse\n",
    "    # Compare each candidate with worse ones\n",
    "    for i in range(len(sorted_candidates)):\n",
    "        for j in range(i + 1, len(sorted_candidates)):\n",
    "            # i has higher score than j\n",
    "            chosen = sorted_candidates[i]\n",
    "            rejected = sorted_candidates[j]\n",
    "\n",
    "            # Only create pair if there's a meaningful difference\n",
    "            score_diff = chosen['scores']['combined'] - rejected['scores']['combined']\n",
    "            if score_diff > 0.05:  # Minimum threshold for meaningful difference\n",
    "                preference_pairs.append({\n",
    "                    'chosen': chosen['translation'],\n",
    "                    'rejected': rejected['translation'],\n",
    "                    'chosen_score': chosen['scores']['combined'],\n",
    "                    'rejected_score': rejected['scores']['combined'],\n",
    "                    'score_margin': score_diff\n",
    "                })\n",
    "\n",
    "    return preference_pairs\n",
    "\n",
    "print(\"Generating synthetic preference dataset...\")\n",
    "print(f\"Processing {len(training_samples)} samples...\\n\")\n",
    "\n",
    "synthetic_dataset = []\n",
    "\n",
    "# Path to save all generated candidates per source for inspection\n",
    "candidates_path = DATA_DIR / \"generated_candidates.jsonl\"\n",
    "print(f\"Saving generated candidates to {candidates_path}\")\n",
    "\n",
    "# Open file once and append per sample\n",
    "with open(candidates_path, 'w', encoding='utf-8') as cand_f:\n",
    "    for idx, sample in enumerate(tqdm(training_samples, desc=\"Generating preferences\")):\n",
    "        try:\n",
    "            # Generate candidates\n",
    "            candidates = generate_candidates(\n",
    "                sample['source'],\n",
    "                sample['source_lang'],\n",
    "                num_candidates=NUM_CANDIDATES\n",
    "            )\n",
    "\n",
    "            # Save raw candidates for this sample\n",
    "            cand_record = {\n",
    "                'source': sample['source'],\n",
    "                'source_lang': sample['source_lang'],\n",
    "                'candidates': candidates\n",
    "            }\n",
    "            cand_f.write(json.dumps(cand_record, ensure_ascii=False) + '\\n')\n",
    "            cand_f.flush()\n",
    "\n",
    "            # Score each candidate (no reference needed)\n",
    "            scored_candidates = []\n",
    "            for cand in candidates:\n",
    "                scores = score_translation(\n",
    "                    sample['source'],\n",
    "                    cand['translation']\n",
    "                )\n",
    "                scored_candidates.append({\n",
    "                    'translation': cand['translation'],\n",
    "                    'scores': scores,\n",
    "                    'config': cand['config']\n",
    "                })\n",
    "\n",
    "            # Create preference pairs\n",
    "            preference_pairs = create_preference_pairs(scored_candidates)\n",
    "\n",
    "            # Store with metadata\n",
    "            for pair in preference_pairs:\n",
    "                synthetic_dataset.append({\n",
    "                    'source': sample['source'],\n",
    "                    'source_lang': sample['source_lang'],\n",
    "                    'chosen': pair['chosen'],\n",
    "                    'rejected': pair['rejected'],\n",
    "                    'chosen_score': pair['chosen_score'],\n",
    "                    'rejected_score': pair['rejected_score'],\n",
    "                    'margin': pair['score_margin']\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Progress update every 100 samples\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"\\nProcessed {idx + 1}/{len(training_samples)} samples\")\n",
    "            print(f\"Generated {len(synthetic_dataset)} preference pairs so far\\n\")\n",
    "\n",
    "print(f\"\\n Generation complete!\")\n",
    "print(f\"Total preference pairs: {len(synthetic_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f0bf7a",
   "metadata": {},
   "source": [
    "## Save Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4552f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSONL format\n",
    "print(f\"Saving dataset to {SYNTHETIC_PREFERENCES}...\")\n",
    "\n",
    "with open(SYNTHETIC_PREFERENCES, 'w', encoding='utf-8') as f:\n",
    "    for item in synthetic_dataset:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\" Saved {len(synthetic_dataset)} preference pairs\")\n",
    "\n",
    "# Save summary statistics\n",
    "stats = {\n",
    "    'total_pairs': len(synthetic_dataset),\n",
    "    'avg_margin': sum(item['margin'] for item in synthetic_dataset) / len(synthetic_dataset),\n",
    "    'avg_chosen_score': sum(item['chosen_score'] for item in synthetic_dataset) / len(synthetic_dataset),\n",
    "    'avg_rejected_score': sum(item['rejected_score'] for item in synthetic_dataset) / len(synthetic_dataset),\n",
    "    'num_sources': len(training_samples)\n",
    "}\n",
    "\n",
    "stats_path = DATA_DIR / \"synthetic_data_stats.json\"\n",
    "with open(stats_path, 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value if isinstance(value, int) else f'{value:.4f}'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344339ab",
   "metadata": {},
   "source": [
    "## Sample Preference Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80684f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some examples\n",
    "print(\"Sample preference pairs:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, item in enumerate(random.sample(synthetic_dataset, min(3, len(synthetic_dataset))), 1):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Source ({item['source_lang']}): {item['source'][:150]}\")\n",
    "    print(f\"\\nChosen (score: {item['chosen_score']:.3f}): {item['chosen'][:150]}\")\n",
    "    print(f\"\\nRejected (score: {item['rejected_score']:.3f}): {item['rejected'][:150]}\")\n",
    "    print(f\"\\nMargin: {item['margin']:.3f}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fcaf8e",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "Proceed to **notebook 2** to train the reward model using this synthetic preference data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214bf6c7",
   "metadata": {},
   "source": [
    "## Quick Process: Convert Partial Data to Preferences\n",
    "\n",
    "Run this cell to process the candidates you already generated (without re-running generation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma_env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
