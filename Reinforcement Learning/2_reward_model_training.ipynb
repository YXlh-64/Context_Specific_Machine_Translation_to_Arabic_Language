{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7128ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run setup from config notebook\n",
    "%run 0_config_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ea6ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import wandb\n",
    "from dataclasses import dataclass\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9713d8eb",
   "metadata": {},
   "source": [
    "## Load Synthetic Preference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2165021",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading synthetic preference data from {SYNTHETIC_PREFERENCES}...\")\n",
    "\n",
    "preference_data = []\n",
    "with open(SYNTHETIC_PREFERENCES, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        preference_data.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(preference_data)} preference pairs\")\n",
    "\n",
    "# Split into train/validation\n",
    "train_size = int(0.9 * len(preference_data))\n",
    "train_data = preference_data[:train_size]\n",
    "val_data = preference_data[train_size:]\n",
    "\n",
    "print(f\"Train: {len(train_data)} pairs\")\n",
    "print(f\"Validation: {len(val_data)} pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7732aa83",
   "metadata": {},
   "source": [
    "## Preference Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075da4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferenceDataset(Dataset):\n",
    "    \"\"\"Dataset for pairwise preference data\"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Format: \"Source: ... \\nTranslation: ...\"\n",
    "        chosen_text = f\"Source: {item['source']}\\nTranslation: {item['chosen']}\"\n",
    "        rejected_text = f\"Source: {item['source']}\\nTranslation: {item['rejected']}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        chosen_tokens = self.tokenizer(\n",
    "            chosen_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        rejected_tokens = self.tokenizer(\n",
    "            rejected_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'chosen_input_ids': chosen_tokens['input_ids'].squeeze(0),\n",
    "            'chosen_attention_mask': chosen_tokens['attention_mask'].squeeze(0),\n",
    "            'rejected_input_ids': rejected_tokens['input_ids'].squeeze(0),\n",
    "            'rejected_attention_mask': rejected_tokens['attention_mask'].squeeze(0),\n",
    "            'margin': item['margin']  # For analysis\n",
    "        }\n",
    "\n",
    "print(\"PreferenceDataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae73ece",
   "metadata": {},
   "source": [
    "## Reward Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa918a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    \"\"\"Reward model with base LM + reward head\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model, hidden_dim=256, head_type='mlp'):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.head_type = head_type\n",
    "        \n",
    "        # Get hidden size from base model\n",
    "        self.hidden_size = base_model.config.hidden_size\n",
    "        \n",
    "        # Reward head\n",
    "        if head_type == 'linear':\n",
    "            self.reward_head = nn.Linear(self.hidden_size, 1)\n",
    "        elif head_type == 'mlp':\n",
    "            self.reward_head = nn.Sequential(\n",
    "                nn.Linear(self.hidden_size, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(hidden_dim, 1)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown head_type: {head_type}\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get base model outputs\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        # Get last hidden state\n",
    "        hidden_states = outputs.hidden_states[-1]  # [batch, seq_len, hidden_size]\n",
    "        \n",
    "        # Pool: use last token representation (similar to value head in PPO)\n",
    "        # Get the last non-padding token for each sequence\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        pooled = hidden_states[torch.arange(batch_size), sequence_lengths]\n",
    "        \n",
    "        # Apply reward head\n",
    "        reward = self.reward_head(pooled)  # [batch, 1]\n",
    "        \n",
    "        return reward.squeeze(-1)  # [batch]\n",
    "\n",
    "print(\"RewardModel class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da92850",
   "metadata": {},
   "source": [
    "## Load Base Model and Create Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a4891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading base model: {REWARD_BASE_MODEL}...\")\n",
    "\n",
    "# Load tokenizer\n",
    "rm_tokenizer = AutoTokenizer.from_pretrained(REWARD_BASE_MODEL)\n",
    "if rm_tokenizer.pad_token is None:\n",
    "    rm_tokenizer.pad_token = rm_tokenizer.eos_token\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    REWARD_BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Freeze base model parameters (optional - can fine-tune last layers)\n",
    "# For faster training, freeze most layers\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze last few layers for fine-tuning\n",
    "num_unfrozen_layers = 4\n",
    "for layer in base_model.model.layers[-num_unfrozen_layers:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "print(f\"✓ Base model loaded (unfrozen last {num_unfrozen_layers} layers)\")\n",
    "\n",
    "# Create reward model\n",
    "reward_model = RewardModel(\n",
    "    base_model=base_model,\n",
    "    hidden_dim=RM_HIDDEN_DIM,\n",
    "    head_type=RM_HEAD_TYPE\n",
    ")\n",
    "\n",
    "print(f\"✓ Reward model created with {RM_HEAD_TYPE} head\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in reward_model.parameters()) / 1e6:.2f}M\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in reward_model.parameters() if p.requires_grad) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8dee49",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e66565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = PreferenceDataset(train_data, rm_tokenizer, max_length=RM_MAX_LENGTH)\n",
    "val_dataset = PreferenceDataset(val_data, rm_tokenizer, max_length=RM_MAX_LENGTH)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=RM_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0  # Set to 0 for Windows compatibility\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=RM_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665ba7c7",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9874cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bradley-Terry loss for pairwise preferences\n",
    "def bradley_terry_loss(chosen_rewards, rejected_rewards):\n",
    "    \"\"\"Bradley-Terry model loss: -log(sigmoid(r_chosen - r_rejected))\"\"\"\n",
    "    return -torch.log(torch.sigmoid(chosen_rewards - rejected_rewards)).mean()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in reward_model.parameters() if p.requires_grad],\n",
    "    lr=RM_LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "num_training_steps = len(train_loader) * RM_EPOCHS // RM_GRADIENT_ACCUMULATION_STEPS\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_training_steps // 10,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Initialize wandb (optional)\n",
    "if USE_WANDB:\n",
    "    wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        name=\"reward-model-coldstart\",\n",
    "        config={\n",
    "            'learning_rate': RM_LEARNING_RATE,\n",
    "            'batch_size': RM_BATCH_SIZE,\n",
    "            'epochs': RM_EPOCHS,\n",
    "            'base_model': REWARD_BASE_MODEL,\n",
    "            'head_type': RM_HEAD_TYPE\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"Training setup complete!\")\n",
    "print(f\"Total training steps: {num_training_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71beaf2e",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166ebc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, device, gradient_accumulation_steps=1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    for step, batch in enumerate(pbar):\n",
    "        # Move to device\n",
    "        chosen_input_ids = batch['chosen_input_ids'].to(device)\n",
    "        chosen_attention_mask = batch['chosen_attention_mask'].to(device)\n",
    "        rejected_input_ids = batch['rejected_input_ids'].to(device)\n",
    "        rejected_attention_mask = batch['rejected_attention_mask'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        chosen_rewards = model(chosen_input_ids, chosen_attention_mask)\n",
    "        rejected_rewards = model(rejected_input_ids, rejected_attention_mask)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = bradley_terry_loss(chosen_rewards, rejected_rewards)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Accuracy: chosen should have higher reward\n",
    "        accuracy = (chosen_rewards > rejected_rewards).float().mean()\n",
    "        \n",
    "        total_loss += loss.item() * gradient_accumulation_steps\n",
    "        total_accuracy += accuracy.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update weights\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{total_loss / num_batches:.4f}\",\n",
    "            'acc': f\"{total_accuracy / num_batches:.4f}\",\n",
    "            'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "        })\n",
    "    \n",
    "    return total_loss / num_batches, total_accuracy / num_batches\n",
    "\n",
    "\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            chosen_input_ids = batch['chosen_input_ids'].to(device)\n",
    "            chosen_attention_mask = batch['chosen_attention_mask'].to(device)\n",
    "            rejected_input_ids = batch['rejected_input_ids'].to(device)\n",
    "            rejected_attention_mask = batch['rejected_attention_mask'].to(device)\n",
    "            \n",
    "            chosen_rewards = model(chosen_input_ids, chosen_attention_mask)\n",
    "            rejected_rewards = model(rejected_input_ids, rejected_attention_mask)\n",
    "            \n",
    "            loss = bradley_terry_loss(chosen_rewards, rejected_rewards)\n",
    "            accuracy = (chosen_rewards > rejected_rewards).float().mean()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += accuracy.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches, total_accuracy / num_batches\n",
    "\n",
    "print(\"Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e63d0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "reward_model = reward_model.to(device)\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for epoch in range(RM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{RM_EPOCHS}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        reward_model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        lr_scheduler,\n",
    "        device,\n",
    "        gradient_accumulation_steps=RM_GRADIENT_ACCUMULATION_STEPS\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(reward_model, val_loader, device)\n",
    "    \n",
    "    print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Log to wandb\n",
    "    if USE_WANDB:\n",
    "        wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_accuracy': train_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_acc\n",
    "        })\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_accuracy:\n",
    "        best_val_accuracy = val_acc\n",
    "        print(f\"\\n✓ New best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "        print(f\"Saving model to {REWARD_MODEL_COLD_START}...\")\n",
    "        \n",
    "        # Save model\n",
    "        REWARD_MODEL_COLD_START.mkdir(exist_ok=True, parents=True)\n",
    "        torch.save({\n",
    "            'model_state_dict': reward_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'val_accuracy': val_acc,\n",
    "            'config': {\n",
    "                'base_model': REWARD_BASE_MODEL,\n",
    "                'head_type': RM_HEAD_TYPE,\n",
    "                'hidden_dim': RM_HIDDEN_DIM\n",
    "            }\n",
    "        }, REWARD_MODEL_COLD_START / \"reward_model.pt\")\n",
    "        \n",
    "        # Save tokenizer\n",
    "        rm_tokenizer.save_pretrained(REWARD_MODEL_COLD_START)\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"Training complete!\")\n",
    "print(f\"Best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4eb8b8",
   "metadata": {},
   "source": [
    "## Test Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d27d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained reward model\n",
    "reward_model.eval()\n",
    "\n",
    "print(\"Testing reward model on sample translations...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get some test examples\n",
    "test_samples = random.sample(val_data, min(5, len(val_data)))\n",
    "\n",
    "for i, sample in enumerate(test_samples, 1):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Source: {sample['source'][:100]}...\")\n",
    "    \n",
    "    # Prepare inputs\n",
    "    chosen_text = f\"Source: {sample['source']}\\nTranslation: {sample['chosen']}\"\n",
    "    rejected_text = f\"Source: {sample['source']}\\nTranslation: {sample['rejected']}\"\n",
    "    \n",
    "    chosen_tokens = rm_tokenizer(\n",
    "        chosen_text,\n",
    "        max_length=RM_MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    \n",
    "    rejected_tokens = rm_tokenizer(\n",
    "        rejected_text,\n",
    "        max_length=RM_MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    \n",
    "    # Get rewards\n",
    "    with torch.no_grad():\n",
    "        chosen_reward = reward_model(\n",
    "            chosen_tokens['input_ids'],\n",
    "            chosen_tokens['attention_mask']\n",
    "        ).item()\n",
    "        \n",
    "        rejected_reward = reward_model(\n",
    "            rejected_tokens['input_ids'],\n",
    "            rejected_tokens['attention_mask']\n",
    "        ).item()\n",
    "    \n",
    "    print(f\"\\nChosen translation: {sample['chosen'][:100]}...\")\n",
    "    print(f\"Chosen reward: {chosen_reward:.4f} (original score: {sample['chosen_score']:.4f})\")\n",
    "    \n",
    "    print(f\"\\nRejected translation: {sample['rejected'][:100]}...\")\n",
    "    print(f\"Rejected reward: {rejected_reward:.4f} (original score: {sample['rejected_score']:.4f})\")\n",
    "    \n",
    "    print(f\"\\nReward margin: {chosen_reward - rejected_reward:.4f}\")\n",
    "    print(f\"Correct preference: {'✓' if chosen_reward > rejected_reward else '✗'}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a20df7",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "Proceed to **notebook 3** to run PPO optimization using this trained reward model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
