{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run setup from config notebook\n",
    "%run 0_config_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b14ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import json\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ef7032",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacef9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading models...\\n\")\n",
    "\n",
    "# 1. Load SFT model (policy model for PPO)\n",
    "print(f\"Loading SFT model from {SFT_MODEL_PATH}...\")\n",
    "policy_tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL_PATH)\n",
    "if policy_tokenizer.pad_token is None:\n",
    "    policy_tokenizer.pad_token = policy_tokenizer.eos_token\n",
    "\n",
    "# Load with value head for PPO\n",
    "policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    SFT_MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    peft_config=None  # Add LoRA config if needed for memory efficiency\n",
    ")\n",
    "print(\"✓ SFT model loaded\")\n",
    "\n",
    "# 2. Load reference model (for KL penalty)\n",
    "print(f\"\\nLoading reference model (frozen SFT) from {SFT_MODEL_PATH}...\")\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    SFT_MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "# Freeze reference model\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"✓ Reference model loaded (frozen)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ef710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load reward model\n",
    "print(f\"\\nLoading reward model from {REWARD_MODEL_COLD_START}...\")\n",
    "\n",
    "# Load tokenizer\n",
    "rm_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_COLD_START)\n",
    "\n",
    "# Load base model\n",
    "rm_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    REWARD_BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Recreate reward model structure (from notebook 2)\n",
    "from torch import nn\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, base_model, hidden_dim=256, head_type='mlp'):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.head_type = head_type\n",
    "        self.hidden_size = base_model.config.hidden_size\n",
    "        \n",
    "        if head_type == 'linear':\n",
    "            self.reward_head = nn.Linear(self.hidden_size, 1)\n",
    "        elif head_type == 'mlp':\n",
    "            self.reward_head = nn.Sequential(\n",
    "                nn.Linear(self.hidden_size, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(hidden_dim, 1)\n",
    "            )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        pooled = hidden_states[torch.arange(batch_size), sequence_lengths]\n",
    "        reward = self.reward_head(pooled)\n",
    "        return reward.squeeze(-1)\n",
    "\n",
    "# Create and load weights\n",
    "reward_model = RewardModel(\n",
    "    base_model=rm_base_model,\n",
    "    hidden_dim=RM_HIDDEN_DIM,\n",
    "    head_type=RM_HEAD_TYPE\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(\n",
    "    REWARD_MODEL_COLD_START / \"reward_model.pt\",\n",
    "    map_location='cpu'\n",
    ")\n",
    "reward_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "reward_model.eval()\n",
    "\n",
    "# Freeze reward model\n",
    "for param in reward_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"✓ Reward model loaded (frozen)\")\n",
    "print(f\"\\nAll models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acac98e",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6605aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training prompts (no parallel corpus needed)\n",
    "print(\"Loading training prompts...\")\n",
    "\n",
    "try:\n",
    "    all_data = load_test_prompts(TEST_PROMPTS)\n",
    "    print(f\"Loaded {len(all_data)} test prompts\")\n",
    "except:\n",
    "    # Create sample prompts if file doesn't exist\n",
    "    all_data = [\n",
    "        {\"source\": \"Hello, how are you?\", \"source_lang\": \"en\"},\n",
    "        {\"source\": \"Good morning.\", \"source_lang\": \"en\"},\n",
    "        {\"source\": \"Thank you very much.\", \"source_lang\": \"en\"},\n",
    "        {\"source\": \"Bonjour, comment allez-vous?\", \"source_lang\": \"fr\"},\n",
    "        {\"source\": \"Merci beaucoup.\", \"source_lang\": \"fr\"},\n",
    "    ] * 1000  # Replicate for training\n",
    "    print(f\"Created {len(all_data)} sample prompts\")\n",
    "\n",
    "# Create dataset of prompts for PPO\n",
    "prompts = []\n",
    "for item in all_data[:5000]:  # Adjust size based on resources\n",
    "    prompt = format_translation_prompt(item['source'], item['source_lang'])\n",
    "    prompts.append({\n",
    "        'query': prompt,\n",
    "        'source': item['source'],\n",
    "        'source_lang': item['source_lang']\n",
    "    })\n",
    "\n",
    "# Create HuggingFace dataset\n",
    "dataset = Dataset.from_list(prompts)\n",
    "\n",
    "print(f\"Loaded {len(dataset)} training prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a02bcd1",
   "metadata": {},
   "source": [
    "## PPO Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO configuration\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=SFT_MODEL_PATH,\n",
    "    learning_rate=PPO_LEARNING_RATE,\n",
    "    batch_size=PPO_BATCH_SIZE,\n",
    "    mini_batch_size=PPO_MINI_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=PPO_GRADIENT_ACCUMULATION_STEPS,\n",
    "    ppo_epochs=PPO_EPOCHS,\n",
    "    \n",
    "    # KL penalty to stay close to reference model\n",
    "    init_kl_coef=KL_PENALTY_COEF,\n",
    "    target_kl=0.1,\n",
    "    \n",
    "    # PPO clipping\n",
    "    cliprange=CLIP_RANGE,\n",
    "    cliprange_value=VALUE_CLIP_RANGE,\n",
    "    \n",
    "    # GAE parameters\n",
    "    vf_coef=0.1,\n",
    "    \n",
    "    # Other settings\n",
    "    seed=SEED,\n",
    "    log_with=\"wandb\" if USE_WANDB else None,\n",
    "    tracker_project_name=WANDB_PROJECT,\n",
    "    tracker_kwargs={\"name\": \"ppo-coldstart\"},\n",
    ")\n",
    "\n",
    "print(\"PPO Configuration:\")\n",
    "print(f\"  Learning rate: {ppo_config.learning_rate}\")\n",
    "print(f\"  Batch size: {ppo_config.batch_size}\")\n",
    "print(f\"  KL penalty: {ppo_config.init_kl_coef}\")\n",
    "print(f\"  Clip range: {ppo_config.cliprange}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37897813",
   "metadata": {},
   "source": [
    "## Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc7ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(source_texts, translations):\n",
    "    \"\"\"\n",
    "    Compute rewards for generated translations using the reward model.\n",
    "    \n",
    "    Args:\n",
    "        source_texts: List of source texts\n",
    "        translations: List of generated translations\n",
    "    \n",
    "    Returns:\n",
    "        List of reward scores (tensors)\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for source, translation in zip(source_texts, translations):\n",
    "        # Format for reward model\n",
    "        text = f\"Source: {source}\\nTranslation: {translation}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = rm_tokenizer(\n",
    "            text,\n",
    "            max_length=RM_MAX_LENGTH,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(reward_model.base_model.device)\n",
    "        \n",
    "        # Get reward\n",
    "        with torch.no_grad():\n",
    "            reward = reward_model(\n",
    "                inputs['input_ids'],\n",
    "                inputs['attention_mask']\n",
    "            )\n",
    "        \n",
    "        rewards.append(reward.cpu())\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "print(\"Reward function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e013fb0f",
   "metadata": {},
   "source": [
    "## Initialize PPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5814993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PPO trainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=policy_model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=policy_tokenizer,\n",
    "    dataset=dataset,\n",
    "    data_collator=None,\n",
    ")\n",
    "\n",
    "print(\"PPO Trainer initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87891d6d",
   "metadata": {},
   "source": [
    "## PPO Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39849c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation settings\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": PPO_MAX_NEW_TOKENS,\n",
    "    \"temperature\": PPO_TEMPERATURE,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": policy_tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": policy_tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "print(\"Starting PPO training...\\n\")\n",
    "print(f\"Total steps: {PPO_STEPS}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Training loop\n",
    "for step, batch in enumerate(tqdm(ppo_trainer.dataloader, total=PPO_STEPS)):\n",
    "    if step >= PPO_STEPS:\n",
    "        break\n",
    "    \n",
    "    # Get queries (prompts)\n",
    "    query_tensors = batch['input_ids']\n",
    "    \n",
    "    # Generate responses (translations)\n",
    "    response_tensors = ppo_trainer.generate(\n",
    "        query_tensors,\n",
    "        return_prompt=False,\n",
    "        **generation_kwargs\n",
    "    )\n",
    "    \n",
    "    # Decode responses\n",
    "    batch_texts = policy_tokenizer.batch_decode(query_tensors, skip_special_tokens=True)\n",
    "    response_texts = policy_tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
    "    \n",
    "    # Extract source texts (for reward computation)\n",
    "    source_texts = []\n",
    "    for text in batch_texts:\n",
    "        # Extract source from prompt\n",
    "        if \"English text to Arabic:\" in text:\n",
    "            source = text.split(\"English text to Arabic:\")[1].split(\"\\n\\nArabic translation:\")[0].strip()\n",
    "        elif \"French text to Arabic:\" in text:\n",
    "            source = text.split(\"French text to Arabic:\")[1].split(\"\\n\\nArabic translation:\")[0].strip()\n",
    "        else:\n",
    "            source = text\n",
    "        source_texts.append(source)\n",
    "    \n",
    "    # Compute rewards\n",
    "    rewards = compute_reward(source_texts, response_texts)\n",
    "    \n",
    "    # Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    \n",
    "    # Log statistics\n",
    "    if step % 10 == 0:\n",
    "        ppo_trainer.log_stats(\n",
    "            stats,\n",
    "            batch,\n",
    "            rewards,\n",
    "            columns_to_log=[\"query\", \"response\"]\n",
    "        )\n",
    "    \n",
    "    # Print progress\n",
    "    if step % 50 == 0:\n",
    "        mean_reward = torch.tensor(rewards).mean().item()\n",
    "        print(f\"\\nStep {step}:\")\n",
    "        print(f\"  Mean reward: {mean_reward:.4f}\")\n",
    "        print(f\"  Mean KL: {stats['objective/kl']:.4f}\")\n",
    "        print(f\"  Policy loss: {stats['ppo/loss/policy']:.4f}\")\n",
    "        print(f\"  Value loss: {stats['ppo/loss/value']:.4f}\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(f\"\\n  Sample translation:\")\n",
    "        print(f\"  Source: {source_texts[0][:100]}...\")\n",
    "        print(f\"  Generated: {response_texts[0][:100]}...\")\n",
    "        print(f\"  Reward: {rewards[0].item():.4f}\")\n",
    "    \n",
    "    # Save checkpoint periodically\n",
    "    if step > 0 and step % 200 == 0:\n",
    "        checkpoint_path = PPO_MODEL_COLD_START / f\"checkpoint-{step}\"\n",
    "        checkpoint_path.mkdir(exist_ok=True, parents=True)\n",
    "        ppo_trainer.model.save_pretrained(checkpoint_path)\n",
    "        policy_tokenizer.save_pretrained(checkpoint_path)\n",
    "        print(f\"\\n✓ Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PPO training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a344da",
   "metadata": {},
   "source": [
    "## Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d87b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final optimized model\n",
    "print(f\"Saving final model to {PPO_MODEL_COLD_START}...\")\n",
    "\n",
    "PPO_MODEL_COLD_START.mkdir(exist_ok=True, parents=True)\n",
    "ppo_trainer.model.save_pretrained(PPO_MODEL_COLD_START)\n",
    "policy_tokenizer.save_pretrained(PPO_MODEL_COLD_START)\n",
    "\n",
    "# Save training info\n",
    "training_info = {\n",
    "    'base_model': SFT_MODEL_PATH,\n",
    "    'reward_model': str(REWARD_MODEL_COLD_START),\n",
    "    'ppo_steps': PPO_STEPS,\n",
    "    'ppo_config': {\n",
    "        'learning_rate': PPO_LEARNING_RATE,\n",
    "        'kl_penalty': KL_PENALTY_COEF,\n",
    "        'clip_range': CLIP_RANGE\n",
    "    },\n",
    "    'stage': 'cold_start'\n",
    "}\n",
    "\n",
    "with open(PPO_MODEL_COLD_START / \"training_info.json\", 'w') as f:\n",
    "    json.dump(training_info, f, indent=2)\n",
    "\n",
    "print(\"✓ Model saved successfully!\")\n",
    "print(f\"\\nPath: {PPO_MODEL_COLD_START}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c924142",
   "metadata": {},
   "source": [
    "## Test Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb175f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the optimized model\n",
    "print(\"Testing optimized model...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_samples = [\n",
    "    {\"text\": \"Hello, how are you today?\", \"lang\": \"en\"},\n",
    "    {\"text\": \"The weather is beautiful this morning.\", \"lang\": \"en\"},\n",
    "    {\"text\": \"Bonjour, comment allez-vous?\", \"lang\": \"fr\"},\n",
    "]\n",
    "\n",
    "ppo_trainer.model.eval()\n",
    "\n",
    "for i, sample in enumerate(test_samples, 1):\n",
    "    prompt = format_translation_prompt(sample['text'], sample['lang'])\n",
    "    \n",
    "    inputs = policy_tokenizer(prompt, return_tensors=\"pt\").to(ppo_trainer.model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = ppo_trainer.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            temperature=0.8,\n",
    "            do_sample=True,\n",
    "            pad_token_id=policy_tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    full_text = policy_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    translation = full_text.split(\"Arabic translation:\")[-1].strip()\n",
    "    \n",
    "    # Compute reward\n",
    "    reward = compute_reward([sample['text']], [translation])[0].item()\n",
    "    \n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Source ({sample['lang']}): {sample['text']}\")\n",
    "    print(f\"Translation: {translation}\")\n",
    "    print(f\"Reward: {reward:.4f}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad05695",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "Proceed to **notebook 4** for inference and user feedback collection."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
