{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9126b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run setup from config notebook\n",
    "%run 0_config_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc5af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import wandb\n",
    "import itertools\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83024f24",
   "metadata": {},
   "source": [
    "## Load Human Feedback Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e069c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading human feedback from {HUMAN_PREFERENCES}...\")\n",
    "\n",
    "human_feedback = []\n",
    "try:\n",
    "    with open(HUMAN_PREFERENCES, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            human_feedback.append(json.loads(line))\n",
    "    print(f\"✓ Loaded {len(human_feedback)} feedback entries\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No human feedback file found!\")\n",
    "    print(\"Please collect feedback using notebook 4 first.\")\n",
    "    raise\n",
    "\n",
    "if len(human_feedback) < 100:\n",
    "    print(f\"\\n⚠️ Warning: Only {len(human_feedback)} feedback entries found.\")\n",
    "    print(\"Recommended: 500+ entries for effective fine-tuning.\")\n",
    "    print(\"Continue anyway? The model may not improve significantly with limited data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f56d242",
   "metadata": {},
   "source": [
    "## Convert to Preference Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3bafa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_preference_pairs(feedback_entries):\n",
    "    \"\"\"Convert human feedback to preference pairs\"\"\"\n",
    "    \n",
    "    preference_pairs = []\n",
    "    \n",
    "    for entry in feedback_entries:\n",
    "        source = entry['source']\n",
    "        candidates = entry['candidates']\n",
    "        ranking = entry.get('user_ranking', [])\n",
    "        custom = entry.get('custom_translation')\n",
    "        \n",
    "        # Case 1: User provided ranking\n",
    "        if ranking and len(ranking) >= 2:\n",
    "            # Convert ranking to preference pairs\n",
    "            # ranking[0] is best, ranking[1] is second best, etc.\n",
    "            for i in range(len(ranking)):\n",
    "                for j in range(i + 1, len(ranking)):\n",
    "                    better_idx = ranking[i] - 1  # Convert to 0-based\n",
    "                    worse_idx = ranking[j] - 1\n",
    "                    \n",
    "                    if 0 <= better_idx < len(candidates) and 0 <= worse_idx < len(candidates):\n",
    "                        preference_pairs.append({\n",
    "                            'source': source,\n",
    "                            'chosen': candidates[better_idx]['translation'],\n",
    "                            'rejected': candidates[worse_idx]['translation'],\n",
    "                            'type': 'ranking'\n",
    "                        })\n",
    "        \n",
    "        # Case 2: User provided custom translation\n",
    "        if custom:\n",
    "            # Custom translation is better than all candidates\n",
    "            for candidate in candidates:\n",
    "                preference_pairs.append({\n",
    "                    'source': source,\n",
    "                    'chosen': custom,\n",
    "                    'rejected': candidate['translation'],\n",
    "                    'type': 'custom'\n",
    "                })\n",
    "    \n",
    "    return preference_pairs\n",
    "\n",
    "print(\"Converting feedback to preference pairs...\")\n",
    "human_preference_pairs = extract_preference_pairs(human_feedback)\n",
    "\n",
    "print(f\"\\n✓ Extracted {len(human_preference_pairs)} preference pairs\")\n",
    "print(f\"  From {len(human_feedback)} feedback entries\")\n",
    "\n",
    "# Split train/val\n",
    "train_size = int(0.9 * len(human_preference_pairs))\n",
    "train_pairs = human_preference_pairs[:train_size]\n",
    "val_pairs = human_preference_pairs[train_size:]\n",
    "\n",
    "print(f\"\\nTrain: {len(train_pairs)} pairs\")\n",
    "print(f\"Validation: {len(val_pairs)} pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdccb6f",
   "metadata": {},
   "source": [
    "## Load Cold-Start Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6592fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading reward model from {REWARD_MODEL_COLD_START}...\")\n",
    "\n",
    "# Load tokenizer\n",
    "rm_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_COLD_START)\n",
    "if rm_tokenizer.pad_token is None:\n",
    "    rm_tokenizer.pad_token = rm_tokenizer.eos_token\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    REWARD_BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Recreate reward model\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, base_model, hidden_dim=256, head_type='mlp'):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.head_type = head_type\n",
    "        self.hidden_size = base_model.config.hidden_size\n",
    "        \n",
    "        if head_type == 'linear':\n",
    "            self.reward_head = nn.Linear(self.hidden_size, 1)\n",
    "        elif head_type == 'mlp':\n",
    "            self.reward_head = nn.Sequential(\n",
    "                nn.Linear(self.hidden_size, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(hidden_dim, 1)\n",
    "            )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        pooled = hidden_states[torch.arange(batch_size), sequence_lengths]\n",
    "        reward = self.reward_head(pooled)\n",
    "        return reward.squeeze(-1)\n",
    "\n",
    "reward_model = RewardModel(\n",
    "    base_model=base_model,\n",
    "    hidden_dim=RM_HIDDEN_DIM,\n",
    "    head_type=RM_HEAD_TYPE\n",
    ")\n",
    "\n",
    "# Load cold-start weights\n",
    "checkpoint = torch.load(\n",
    "    REWARD_MODEL_COLD_START / \"reward_model.pt\",\n",
    "    map_location='cpu'\n",
    ")\n",
    "reward_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(\"✓ Cold-start reward model loaded\")\n",
    "\n",
    "# Unfreeze for fine-tuning\n",
    "for param in reward_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze reward head and last few layers\n",
    "for param in reward_model.reward_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for layer in reward_model.base_model.model.layers[-4:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in reward_model.parameters() if p.requires_grad) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bc410e",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b377605",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        chosen_text = f\"Source: {item['source']}\\nTranslation: {item['chosen']}\"\n",
    "        rejected_text = f\"Source: {item['source']}\\nTranslation: {item['rejected']}\"\n",
    "        \n",
    "        chosen_tokens = self.tokenizer(\n",
    "            chosen_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        rejected_tokens = self.tokenizer(\n",
    "            rejected_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'chosen_input_ids': chosen_tokens['input_ids'].squeeze(0),\n",
    "            'chosen_attention_mask': chosen_tokens['attention_mask'].squeeze(0),\n",
    "            'rejected_input_ids': rejected_tokens['input_ids'].squeeze(0),\n",
    "            'rejected_attention_mask': rejected_tokens['attention_mask'].squeeze(0),\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = PreferenceDataset(train_pairs, rm_tokenizer, max_length=RM_MAX_LENGTH)\n",
    "val_dataset = PreferenceDataset(val_pairs, rm_tokenizer, max_length=RM_MAX_LENGTH)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=RM_BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=RM_BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f9aab3",
   "metadata": {},
   "source": [
    "## Fine-tune Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3446f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "def bradley_terry_loss(chosen_rewards, rejected_rewards):\n",
    "    return -torch.log(torch.sigmoid(chosen_rewards - rejected_rewards)).mean()\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in reward_model.parameters() if p.requires_grad],\n",
    "    lr=RM_LEARNING_RATE * 0.5  # Lower LR for fine-tuning\n",
    ")\n",
    "\n",
    "num_training_steps = len(train_loader) * RM_EPOCHS // RM_GRADIENT_ACCUMULATION_STEPS\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_training_steps // 10,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        name=\"reward-model-human-aligned\",\n",
    "        config={'stage': 'human_alignment', 'base': 'cold_start_rm'}\n",
    "    )\n",
    "\n",
    "print(\"Starting reward model fine-tuning with human preferences...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8381e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions (same as notebook 2)\n",
    "def train_epoch(model, loader, optimizer, scheduler, device, gradient_accumulation_steps=1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    for step, batch in enumerate(pbar):\n",
    "        chosen_input_ids = batch['chosen_input_ids'].to(device)\n",
    "        chosen_attention_mask = batch['chosen_attention_mask'].to(device)\n",
    "        rejected_input_ids = batch['rejected_input_ids'].to(device)\n",
    "        rejected_attention_mask = batch['rejected_attention_mask'].to(device)\n",
    "        \n",
    "        chosen_rewards = model(chosen_input_ids, chosen_attention_mask)\n",
    "        rejected_rewards = model(rejected_input_ids, rejected_attention_mask)\n",
    "        \n",
    "        loss = bradley_terry_loss(chosen_rewards, rejected_rewards)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        accuracy = (chosen_rewards > rejected_rewards).float().mean()\n",
    "        \n",
    "        total_loss += loss.item() * gradient_accumulation_steps\n",
    "        total_accuracy += accuracy.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{total_loss / num_batches:.4f}\",\n",
    "            'acc': f\"{total_accuracy / num_batches:.4f}\"\n",
    "        })\n",
    "    \n",
    "    return total_loss / num_batches, total_accuracy / num_batches\n",
    "\n",
    "\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            chosen_input_ids = batch['chosen_input_ids'].to(device)\n",
    "            chosen_attention_mask = batch['chosen_attention_mask'].to(device)\n",
    "            rejected_input_ids = batch['rejected_input_ids'].to(device)\n",
    "            rejected_attention_mask = batch['rejected_attention_mask'].to(device)\n",
    "            \n",
    "            chosen_rewards = model(chosen_input_ids, chosen_attention_mask)\n",
    "            rejected_rewards = model(rejected_input_ids, rejected_attention_mask)\n",
    "            \n",
    "            loss = bradley_terry_loss(chosen_rewards, rejected_rewards)\n",
    "            accuracy = (chosen_rewards > rejected_rewards).float().mean()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += accuracy.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches, total_accuracy / num_batches\n",
    "\n",
    "# Train\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "reward_model = reward_model.to(device)\n",
    "\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for epoch in range(RM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{RM_EPOCHS}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(\n",
    "        reward_model, train_loader, optimizer, lr_scheduler, device,\n",
    "        gradient_accumulation_steps=RM_GRADIENT_ACCUMULATION_STEPS\n",
    "    )\n",
    "    \n",
    "    val_loss, val_acc = validate(reward_model, val_loader, device)\n",
    "    \n",
    "    print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    if USE_WANDB:\n",
    "        wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_accuracy': train_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_acc\n",
    "        })\n",
    "    \n",
    "    if val_acc > best_val_accuracy:\n",
    "        best_val_accuracy = val_acc\n",
    "        print(f\"\\n✓ New best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "        \n",
    "        REWARD_MODEL_HUMAN_ALIGNED.mkdir(exist_ok=True, parents=True)\n",
    "        torch.save({\n",
    "            'model_state_dict': reward_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'val_accuracy': val_acc,\n",
    "            'config': {\n",
    "                'base_model': REWARD_BASE_MODEL,\n",
    "                'head_type': RM_HEAD_TYPE,\n",
    "                'hidden_dim': RM_HIDDEN_DIM,\n",
    "                'stage': 'human_aligned'\n",
    "            }\n",
    "        }, REWARD_MODEL_HUMAN_ALIGNED / \"reward_model.pt\")\n",
    "        \n",
    "        rm_tokenizer.save_pretrained(REWARD_MODEL_HUMAN_ALIGNED)\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"Reward model fine-tuning complete!\")\n",
    "print(f\"Best validation accuracy: {best_val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dba991",
   "metadata": {},
   "source": [
    "## Re-run PPO with Human-Aligned Reward Model\n",
    "\n",
    "Now use the human-aligned reward model to further optimize the translation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb58dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is similar to notebook 3, but using the human-aligned reward model\n",
    "print(\"\\nPreparing for final PPO optimization...\")\n",
    "print(\"This will use the human-aligned reward model.\\n\")\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load policy model (start from cold-start PPO model)\n",
    "print(f\"Loading policy model from {PPO_MODEL_COLD_START}...\")\n",
    "policy_tokenizer = AutoTokenizer.from_pretrained(PPO_MODEL_COLD_START)\n",
    "if policy_tokenizer.pad_token is None:\n",
    "    policy_tokenizer.pad_token = policy_tokenizer.eos_token\n",
    "\n",
    "policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    PPO_MODEL_COLD_START,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"✓ Policy model loaded\")\n",
    "\n",
    "# Load reference model\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    PPO_MODEL_COLD_START,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"✓ Reference model loaded\")\n",
    "\n",
    "# Reward model is already loaded and fine-tuned\n",
    "reward_model.eval()\n",
    "for param in reward_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"✓ Using human-aligned reward model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8435eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training prompts (no parallel corpus needed)\n",
    "try:\n",
    "    all_data = load_test_prompts(TEST_PROMPTS)\n",
    "except:\n",
    "    all_data = [\n",
    "        {\"source\": \"Hello, how are you?\", \"source_lang\": \"en\"},\n",
    "        {\"source\": \"Good morning.\", \"source_lang\": \"en\"},\n",
    "        {\"source\": \"Bonjour, comment allez-vous?\", \"source_lang\": \"fr\"},\n",
    "    ] * 1000\n",
    "\n",
    "prompts = []\n",
    "for item in all_data[:5000]:\n",
    "    prompt = format_translation_prompt(item['source'], item['source_lang'])\n",
    "    prompts.append({'query': prompt, 'source': item['source']})\n",
    "\n",
    "dataset = Dataset.from_list(prompts)\n",
    "print(f\"Loaded {len(dataset)} training prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3368b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO configuration\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=str(PPO_MODEL_COLD_START),\n",
    "    learning_rate=PPO_LEARNING_RATE * 0.5,  # Lower LR for fine-tuning\n",
    "    batch_size=PPO_BATCH_SIZE,\n",
    "    mini_batch_size=PPO_MINI_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=PPO_GRADIENT_ACCUMULATION_STEPS,\n",
    "    ppo_epochs=PPO_EPOCHS,\n",
    "    init_kl_coef=KL_PENALTY_COEF,\n",
    "    target_kl=0.1,\n",
    "    cliprange=CLIP_RANGE,\n",
    "    cliprange_value=VALUE_CLIP_RANGE,\n",
    "    vf_coef=0.1,\n",
    "    seed=SEED,\n",
    "    log_with=\"wandb\" if USE_WANDB else None,\n",
    "    tracker_project_name=WANDB_PROJECT,\n",
    "    tracker_kwargs={\"name\": \"ppo-human-aligned\"},\n",
    ")\n",
    "\n",
    "# Initialize PPO trainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=policy_model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=policy_tokenizer,\n",
    "    dataset=dataset,\n",
    ")\n",
    "\n",
    "print(\"PPO Trainer initialized for final optimization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08593b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function using human-aligned reward model\n",
    "def compute_reward(source_texts, translations):\n",
    "    rewards = []\n",
    "    for source, translation in zip(source_texts, translations):\n",
    "        text = f\"Source: {source}\\nTranslation: {translation}\"\n",
    "        inputs = rm_tokenizer(\n",
    "            text,\n",
    "            max_length=RM_MAX_LENGTH,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(reward_model.base_model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            reward = reward_model(inputs['input_ids'], inputs['attention_mask'])\n",
    "        rewards.append(reward.cpu())\n",
    "    return rewards\n",
    "\n",
    "# Generation settings\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": PPO_MAX_NEW_TOKENS,\n",
    "    \"temperature\": PPO_TEMPERATURE,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": policy_tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": policy_tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "print(\"Starting final PPO training with human-aligned rewards...\\n\")\n",
    "print(f\"Total steps: {PPO_STEPS}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b432619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for step, batch in enumerate(tqdm(ppo_trainer.dataloader, total=PPO_STEPS)):\n",
    "    if step >= PPO_STEPS:\n",
    "        break\n",
    "    \n",
    "    query_tensors = batch['input_ids']\n",
    "    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)\n",
    "    \n",
    "    batch_texts = policy_tokenizer.batch_decode(query_tensors, skip_special_tokens=True)\n",
    "    response_texts = policy_tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
    "    \n",
    "    # Extract source texts\n",
    "    source_texts = []\n",
    "    for text in batch_texts:\n",
    "        if \"English text to Arabic:\" in text:\n",
    "            source = text.split(\"English text to Arabic:\")[1].split(\"\\n\\nArabic translation:\")[0].strip()\n",
    "        elif \"French text to Arabic:\" in text:\n",
    "            source = text.split(\"French text to Arabic:\")[1].split(\"\\n\\nArabic translation:\")[0].strip()\n",
    "        else:\n",
    "            source = text\n",
    "        source_texts.append(source)\n",
    "    \n",
    "    rewards = compute_reward(source_texts, response_texts)\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "    \n",
    "    if step % 50 == 0:\n",
    "        mean_reward = torch.tensor(rewards).mean().item()\n",
    "        print(f\"\\nStep {step}: Mean reward: {mean_reward:.4f} | KL: {stats['objective/kl']:.4f}\")\n",
    "    \n",
    "    if step > 0 and step % 200 == 0:\n",
    "        checkpoint_path = PPO_MODEL_FINAL / f\"checkpoint-{step}\"\n",
    "        checkpoint_path.mkdir(exist_ok=True, parents=True)\n",
    "        ppo_trainer.model.save_pretrained(checkpoint_path)\n",
    "        policy_tokenizer.save_pretrained(checkpoint_path)\n",
    "        print(f\"\\n✓ Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Final PPO training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e964464",
   "metadata": {},
   "source": [
    "## Save Final Production Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7207c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving final production model to {PPO_MODEL_FINAL}...\")\n",
    "\n",
    "PPO_MODEL_FINAL.mkdir(exist_ok=True, parents=True)\n",
    "ppo_trainer.model.save_pretrained(PPO_MODEL_FINAL)\n",
    "policy_tokenizer.save_pretrained(PPO_MODEL_FINAL)\n",
    "\n",
    "training_info = {\n",
    "    'base_model': SFT_MODEL_PATH,\n",
    "    'reward_model': str(REWARD_MODEL_HUMAN_ALIGNED),\n",
    "    'stage': 'human_aligned',\n",
    "    'human_feedback_entries': len(human_feedback),\n",
    "    'preference_pairs': len(human_preference_pairs),\n",
    "    'ppo_steps': PPO_STEPS,\n",
    "    'final_training': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(PPO_MODEL_FINAL / \"training_info.json\", 'w') as f:\n",
    "    json.dump(training_info, f, indent=2)\n",
    "\n",
    "print(\"✓ Final model saved successfully!\")\n",
    "print(f\"\\nPath: {PPO_MODEL_FINAL}\")\n",
    "print(\"\\nThis model is now ready for production use!\")\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d5d397",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Congratulations!** You have completed the full RLHF pipeline:\n",
    "\n",
    "1. ✅ **Phase 1**: Supervised Fine-Tuning (Gemma-2X289B - pre-completed)\n",
    "2. ✅ **Phase 2.1**: Generated synthetic preference data from automatic metrics\n",
    "3. ✅ **Phase 2.2**: Trained cold-start reward model\n",
    "4. ✅ **Phase 2.3**: Ran PPO optimization with synthetic rewards\n",
    "5. ✅ **Phase 3**: Collected human feedback and fine-tuned reward model\n",
    "6. ✅ **Phase 3**: Re-ran PPO with human-aligned rewards\n",
    "\n",
    "### Final Models:\n",
    "- **Translation Model**: `{PPO_MODEL_FINAL}`\n",
    "- **Reward Model**: `{REWARD_MODEL_HUMAN_ALIGNED}`\n",
    "\n",
    "### Continuous Improvement:\n",
    "To continuously improve the system:\n",
    "1. Keep collecting user feedback using notebook 4\n",
    "2. Periodically re-run this notebook to update models\n",
    "3. Monitor translation quality metrics\n",
    "\n",
    "### Next Steps:\n",
    "- Deploy the final model in production\n",
    "- Set up automated feedback collection\n",
    "- Schedule regular retraining cycles"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
