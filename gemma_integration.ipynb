{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10767409",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e5ca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e3efcf",
   "metadata": {},
   "source": [
    "## 2. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93826bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"ModelSpace/GemmaX2-28-9B-v0.1\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading model on CPU...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"cpu\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b66aa8",
   "metadata": {},
   "source": [
    "## 3. Helper Function for Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_tokens=200, temperature=0.8, top_p=0.95):\n",
    "    \"\"\"\n",
    "    Generate text using the Gemma model.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input text prompt\n",
    "        max_tokens: Maximum number of tokens to generate\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "        top_p: Nucleus sampling parameter\n",
    "    \n",
    "    Returns:\n",
    "        Generated text string\n",
    "    \"\"\"\n",
    "    # Format prompt for Gemma chat model\n",
    "    formatted_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=1.15,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the model's response\n",
    "    if \"model\" in generated_text:\n",
    "        response = generated_text.split(\"model\")[-1].strip()\n",
    "    else:\n",
    "        response = generated_text\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca9e57f",
   "metadata": {},
   "source": [
    "## 4. Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a972ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: General question\n",
    "prompt = \"Explain reinforcement learning in simple terms.\"\n",
    "response = generate_text(prompt, max_tokens=150)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nResponse: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55649b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Creative task\n",
    "prompt = \"Write a short story about a robot learning to make decisions.\"\n",
    "response = generate_text(prompt, max_tokens=300, temperature=0.9)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nResponse: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae4ce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Code generation\n",
    "prompt = \"Write a Python function to calculate the Bellman equation for Q-learning.\"\n",
    "response = generate_text(prompt, max_tokens=200, temperature=0.7)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nResponse: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cad045a",
   "metadata": {},
   "source": [
    "## 5. Batch Generation for Multiple Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What is the difference between Q-learning and SARSA?\",\n",
    "    \"Explain policy gradient methods.\",\n",
    "    \"What are the main challenges in reinforcement learning?\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt {i}: {prompt}\")\n",
    "    print('='*60)\n",
    "    response = generate_text(prompt, max_tokens=150)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083475a7",
   "metadata": {},
   "source": [
    "## 6. Integration with RL Workflow\n",
    "\n",
    "You can use this model for:\n",
    "- Generating synthetic training data\n",
    "- Creating reward model descriptions\n",
    "- Generating explanations for RL agent decisions\n",
    "- Creating natural language interfaces for your RL system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ad686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generate synthetic RL scenarios\n",
    "def generate_rl_scenario(task_type=\"navigation\"):\n",
    "    prompt = f\"\"\"Describe a {task_type} scenario for training a reinforcement learning agent. \n",
    "    Include the environment, states, actions, and rewards.\"\"\"\n",
    "    \n",
    "    return generate_text(prompt, max_tokens=250, temperature=0.8)\n",
    "\n",
    "# Generate a navigation scenario\n",
    "scenario = generate_rl_scenario(\"navigation\")\n",
    "print(\"Generated RL Scenario:\")\n",
    "print(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5534dfa4",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- **Performance**: Currently running on CPU. Generation may be slow for long sequences.\n",
    "- **GPU Support**: Once PyTorch adds support for RTX 5090 (sm_120), you can change `device_map=\"cpu\"` to `device_map=\"auto\"` to use GPU acceleration.\n",
    "- **Model Behavior**: This is a multilingual model and may occasionally generate responses in other languages.\n",
    "- **Memory**: The model requires significant RAM when running on CPU (~30GB for the full model)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
