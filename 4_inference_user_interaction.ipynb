{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c711376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run setup from config notebook\n",
    "%run 0_config_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1deedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf3357e",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb3e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading models...\\n\")\n",
    "\n",
    "# Load PPO-optimized translation model\n",
    "print(f\"Loading translation model from {PPO_MODEL_COLD_START}...\")\n",
    "translation_tokenizer = AutoTokenizer.from_pretrained(PPO_MODEL_COLD_START)\n",
    "if translation_tokenizer.pad_token is None:\n",
    "    translation_tokenizer.pad_token = translation_tokenizer.eos_token\n",
    "\n",
    "translation_model = AutoModelForCausalLM.from_pretrained(\n",
    "    PPO_MODEL_COLD_START,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "translation_model.eval()\n",
    "print(\"‚úì Translation model loaded\")\n",
    "\n",
    "# Load reward model\n",
    "print(f\"\\nLoading reward model from {REWARD_MODEL_COLD_START}...\")\n",
    "rm_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_COLD_START)\n",
    "\n",
    "rm_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    REWARD_BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Recreate reward model\n",
    "import torch.nn as nn\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, base_model, hidden_dim=256, head_type='mlp'):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.head_type = head_type\n",
    "        self.hidden_size = base_model.config.hidden_size\n",
    "        \n",
    "        if head_type == 'linear':\n",
    "            self.reward_head = nn.Linear(self.hidden_size, 1)\n",
    "        elif head_type == 'mlp':\n",
    "            self.reward_head = nn.Sequential(\n",
    "                nn.Linear(self.hidden_size, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(hidden_dim, 1)\n",
    "            )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        pooled = hidden_states[torch.arange(batch_size), sequence_lengths]\n",
    "        reward = self.reward_head(pooled)\n",
    "        return reward.squeeze(-1)\n",
    "\n",
    "reward_model = RewardModel(\n",
    "    base_model=rm_base_model,\n",
    "    hidden_dim=RM_HIDDEN_DIM,\n",
    "    head_type=RM_HEAD_TYPE\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(\n",
    "    REWARD_MODEL_COLD_START / \"reward_model.pt\",\n",
    "    map_location='cpu'\n",
    ")\n",
    "reward_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "reward_model.eval()\n",
    "\n",
    "print(\"‚úì Reward model loaded\")\n",
    "print(\"\\nAll models ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45efb91a",
   "metadata": {},
   "source": [
    "## Translation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa48e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates(source_text, source_lang, num_candidates=8):\n",
    "    \"\"\"Generate multiple translation candidates\"\"\"\n",
    "    \n",
    "    prompt = format_translation_prompt(source_text, source_lang)\n",
    "    inputs = translation_tokenizer(prompt, return_tensors=\"pt\").to(translation_model.device)\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    # Generate with different sampling parameters\n",
    "    for i in range(num_candidates):\n",
    "        # Vary temperature and sampling\n",
    "        temp = 0.7 + (i * 0.1)  # 0.7 to 1.4\n",
    "        top_p = 0.85 + (i * 0.02)  # 0.85 to 0.99\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = translation_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=INFERENCE_MAX_LENGTH,\n",
    "                temperature=min(temp, 1.5),\n",
    "                top_p=min(top_p, 0.99),\n",
    "                top_k=50,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=translation_tokenizer.pad_token_id,\n",
    "                eos_token_id=translation_tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        full_text = translation_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        translation = full_text.split(\"Arabic translation:\")[-1].strip()\n",
    "        \n",
    "        candidates.append(translation)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique_candidates = []\n",
    "    for cand in candidates:\n",
    "        if cand not in seen:\n",
    "            seen.add(cand)\n",
    "            unique_candidates.append(cand)\n",
    "    \n",
    "    return unique_candidates\n",
    "\n",
    "\n",
    "def score_candidates(source_text, candidates):\n",
    "    \"\"\"Score candidates using the reward model\"\"\"\n",
    "    \n",
    "    scored_candidates = []\n",
    "    \n",
    "    for translation in candidates:\n",
    "        text = f\"Source: {source_text}\\nTranslation: {translation}\"\n",
    "        \n",
    "        inputs = rm_tokenizer(\n",
    "            text,\n",
    "            max_length=RM_MAX_LENGTH,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(reward_model.base_model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            reward = reward_model(\n",
    "                inputs['input_ids'],\n",
    "                inputs['attention_mask']\n",
    "            ).item()\n",
    "        \n",
    "        scored_candidates.append({\n",
    "            'translation': translation,\n",
    "            'score': reward\n",
    "        })\n",
    "    \n",
    "    # Sort by score (descending)\n",
    "    scored_candidates.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    return scored_candidates\n",
    "\n",
    "\n",
    "def translate_with_ranking(source_text, source_lang='en'):\n",
    "    \"\"\"Complete translation pipeline with ranking\"\"\"\n",
    "    \n",
    "    print(f\"Generating {INFERENCE_NUM_CANDIDATES} candidate translations...\")\n",
    "    candidates = generate_candidates(source_text, source_lang, INFERENCE_NUM_CANDIDATES)\n",
    "    print(f\"‚úì Generated {len(candidates)} unique candidates\")\n",
    "    \n",
    "    print(\"\\nScoring candidates with reward model...\")\n",
    "    scored = score_candidates(source_text, candidates)\n",
    "    print(\"‚úì Scoring complete\")\n",
    "    \n",
    "    # Get top K\n",
    "    top_candidates = scored[:INFERENCE_TOP_K]\n",
    "    \n",
    "    return top_candidates, scored\n",
    "\n",
    "print(\"Translation pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdddaf12",
   "metadata": {},
   "source": [
    "## Feedback Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef70fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feedback(source_text, source_lang, candidates, user_ranking, custom_translation=None):\n",
    "    \"\"\"Save user feedback to file\"\"\"\n",
    "    \n",
    "    feedback_entry = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'source': source_text,\n",
    "        'source_lang': source_lang,\n",
    "        'candidates': candidates,\n",
    "        'user_ranking': user_ranking,\n",
    "        'custom_translation': custom_translation\n",
    "    }\n",
    "    \n",
    "    # Append to feedback file\n",
    "    with open(HUMAN_PREFERENCES, 'a', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(feedback_entry, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"‚úì Feedback saved to {HUMAN_PREFERENCES}\")\n",
    "\n",
    "print(\"Feedback storage ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f4c204",
   "metadata": {},
   "source": [
    "## Interactive Translation Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a633a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interface\n",
    "print(\"Creating interactive interface...\\n\")\n",
    "\n",
    "# Input widgets\n",
    "source_input = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Enter text to translate...',\n",
    "    description='Source:',\n",
    "    layout=widgets.Layout(width='80%', height='100px')\n",
    ")\n",
    "\n",
    "lang_select = widgets.Dropdown(\n",
    "    options=[('English', 'en'), ('French', 'fr')],\n",
    "    value='en',\n",
    "    description='Language:',\n",
    ")\n",
    "\n",
    "translate_button = widgets.Button(\n",
    "    description='Translate',\n",
    "    button_style='primary',\n",
    "    icon='language'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Storage for current session\n",
    "current_session = {\n",
    "    'source': None,\n",
    "    'lang': None,\n",
    "    'candidates': None,\n",
    "    'all_candidates': None\n",
    "}\n",
    "\n",
    "def on_translate_click(b):\n",
    "    with output_area:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        source_text = source_input.value.strip()\n",
    "        if not source_text:\n",
    "            print(\"Please enter text to translate.\")\n",
    "            return\n",
    "        \n",
    "        source_lang = lang_select.value\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Source ({lang_select.options[lang_select.index][0]}): {source_text}\")\n",
    "        print(\"=\" * 80)\n",
    "        print()\n",
    "        \n",
    "        # Generate and rank translations\n",
    "        top_candidates, all_candidates = translate_with_ranking(source_text, source_lang)\n",
    "        \n",
    "        # Store in session\n",
    "        current_session['source'] = source_text\n",
    "        current_session['lang'] = source_lang\n",
    "        current_session['candidates'] = top_candidates\n",
    "        current_session['all_candidates'] = all_candidates\n",
    "        \n",
    "        # Display top candidates\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"Top {len(top_candidates)} Translations (ranked by quality):\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, cand in enumerate(top_candidates, 1):\n",
    "            print(f\"\\n{i}. {cand['translation']}\")\n",
    "            print(f\"   Score: {cand['score']:.4f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"\\nTo provide feedback, use the feedback functions below.\")\n",
    "\n",
    "translate_button.on_click(on_translate_click)\n",
    "\n",
    "# Display interface\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h2>üåê Arabic Translation System</h2>\"),\n",
    "    source_input,\n",
    "    lang_select,\n",
    "    translate_button,\n",
    "    output_area\n",
    "]))\n",
    "\n",
    "print(\"Interface ready! Use the form above to translate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02fae5",
   "metadata": {},
   "source": [
    "## Provide Feedback\n",
    "\n",
    "After seeing translations, provide feedback using the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a5722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Rank the displayed translations\n",
    "# Provide ranking as list of indices (1, 2, 3)\n",
    "# Example: [2, 1, 3] means \"translation 2 is best, then 1, then 3\"\n",
    "\n",
    "def submit_ranking(ranking):\n",
    "    \"\"\"\n",
    "    Submit ranking for current translations.\n",
    "    \n",
    "    Args:\n",
    "        ranking: List of indices (1-based) in order of preference\n",
    "                 Example: [2, 1, 3] means option 2 is best\n",
    "    \"\"\"\n",
    "    if current_session['candidates'] is None:\n",
    "        print(\"No translations to rank. Please translate text first.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Submitting ranking: {ranking}\")\n",
    "    \n",
    "    # Convert to 0-based and validate\n",
    "    try:\n",
    "        ranking_0based = [r - 1 for r in ranking]\n",
    "        if not all(0 <= r < len(current_session['candidates']) for r in ranking_0based):\n",
    "            print(\"Invalid ranking indices!\")\n",
    "            return\n",
    "    except:\n",
    "        print(\"Invalid ranking format!\")\n",
    "        return\n",
    "    \n",
    "    save_feedback(\n",
    "        current_session['source'],\n",
    "        current_session['lang'],\n",
    "        current_session['candidates'],\n",
    "        ranking,\n",
    "        custom_translation=None\n",
    "    )\n",
    "    \n",
    "    print(\"Thank you for your feedback!\")\n",
    "\n",
    "# Example usage:\n",
    "# submit_ranking([2, 1, 3])  # Translation 2 is best, then 1, then 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d801a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Provide your own translation\n",
    "\n",
    "def submit_custom_translation(custom_text, best_candidate_idx=None):\n",
    "    \"\"\"\n",
    "    Submit a custom translation.\n",
    "    \n",
    "    Args:\n",
    "        custom_text: Your custom Arabic translation\n",
    "        best_candidate_idx: (Optional) Index of best system translation (1-based)\n",
    "    \"\"\"\n",
    "    if current_session['candidates'] is None:\n",
    "        print(\"No session active. Please translate text first.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Submitting custom translation: {custom_text[:100]}...\")\n",
    "    \n",
    "    ranking = [best_candidate_idx] if best_candidate_idx else []\n",
    "    \n",
    "    save_feedback(\n",
    "        current_session['source'],\n",
    "        current_session['lang'],\n",
    "        current_session['candidates'],\n",
    "        ranking,\n",
    "        custom_translation=custom_text\n",
    "    )\n",
    "    \n",
    "    print(\"Thank you for your feedback!\")\n",
    "\n",
    "# Example usage:\n",
    "# submit_custom_translation(\"ŸÖÿ±ÿ≠ÿ®ÿßÿå ŸÉŸäŸÅ ÿ≠ÿßŸÑŸÉÿü\", best_candidate_idx=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a4ce3b",
   "metadata": {},
   "source": [
    "## Batch Translation (Optional)\n",
    "\n",
    "Translate multiple texts at once for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e28f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_translate(texts, source_lang='en'):\n",
    "    \"\"\"Translate multiple texts\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, text in enumerate(texts, 1):\n",
    "        print(f\"\\nTranslating {i}/{len(texts)}: {text[:50]}...\")\n",
    "        \n",
    "        top_candidates, _ = translate_with_ranking(text, source_lang)\n",
    "        \n",
    "        results.append({\n",
    "            'source': text,\n",
    "            'best_translation': top_candidates[0]['translation'],\n",
    "            'score': top_candidates[0]['score'],\n",
    "            'all_top_candidates': top_candidates\n",
    "        })\n",
    "        \n",
    "        print(f\"Best: {top_candidates[0]['translation'][:80]}...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# batch_texts = [\n",
    "#     \"Hello, how are you?\",\n",
    "#     \"The weather is nice today.\",\n",
    "#     \"I love learning new languages.\"\n",
    "# ]\n",
    "# batch_results = batch_translate(batch_texts, source_lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4526dc64",
   "metadata": {},
   "source": [
    "## View Collected Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6160e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how much feedback has been collected\n",
    "try:\n",
    "    with open(HUMAN_PREFERENCES, 'r', encoding='utf-8') as f:\n",
    "        feedback_count = sum(1 for line in f)\n",
    "    print(f\"Total feedback entries collected: {feedback_count}\")\n",
    "    \n",
    "    if feedback_count > 0:\n",
    "        print(\"\\nSample feedback entries:\")\n",
    "        with open(HUMAN_PREFERENCES, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= 3:  # Show first 3\n",
    "                    break\n",
    "                entry = json.loads(line)\n",
    "                print(f\"\\n{i+1}. Source: {entry['source'][:60]}...\")\n",
    "                print(f\"   Ranking: {entry.get('user_ranking', 'N/A')}\")\n",
    "                print(f\"   Custom: {entry.get('custom_translation', 'None')[:60] if entry.get('custom_translation') else 'None'}...\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No feedback collected yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90ca5e6",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "Once you have collected sufficient human feedback (recommended: 500+ preference pairs), proceed to **notebook 5** to fine-tune the reward model with human preferences and re-run PPO for final alignment."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
