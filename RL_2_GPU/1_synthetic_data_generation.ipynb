{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c70776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/aya/anaconda3/lib/python3.13/site-packages (4.57.3)\n",
      "Requirement already satisfied: accelerate in /home/aya/anaconda3/lib/python3.13/site-packages (1.12.0)\n",
      "Requirement already satisfied: torch in /home/aya/anaconda3/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: datasets in /home/aya/anaconda3/lib/python3.13/site-packages (4.4.1)\n",
      "Requirement already satisfied: trl in /home/aya/anaconda3/lib/python3.13/site-packages (0.25.1)\n",
      "Requirement already satisfied: peft in /home/aya/anaconda3/lib/python3.13/site-packages (0.18.0)\n",
      "Requirement already satisfied: bitsandbytes in /home/aya/anaconda3/lib/python3.13/site-packages (0.48.2)\n",
      "Requirement already satisfied: filelock in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: psutil in /home/aya/anaconda3/lib/python3.13/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: setuptools in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (78.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/aya/anaconda3/lib/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /home/aya/anaconda3/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/aya/anaconda3/lib/python3.13/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/aya/anaconda3/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.11.10)\n",
      "Requirement already satisfied: anyio in /home/aya/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.7.0)\n",
      "Requirement already satisfied: certifi in /home/aya/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /home/aya/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/aya/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /home/aya/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.18.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests->transformers) (2.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: comet-ml in /home/aya/anaconda3/lib/python3.13/site-packages (3.54.2)\n",
      "Requirement already satisfied: unbabel-comet in /home/aya/anaconda3/lib/python3.13/site-packages (2.2.7)\n",
      "Requirement already satisfied: bert-score in /home/aya/anaconda3/lib/python3.13/site-packages (0.3.13)\n",
      "Requirement already satisfied: sacrebleu in /home/aya/anaconda3/lib/python3.13/site-packages (2.5.1)\n",
      "Requirement already satisfied: dulwich!=0.20.33,>=0.20.6 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (0.24.10)\n",
      "Requirement already satisfied: everett<3.2.0,>=1.0.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from everett[ini]<3.2.0,>=1.0.1->comet-ml) (3.1.0)\n",
      "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (4.23.0)\n",
      "Requirement already satisfied: psutil>=5.6.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (5.9.0)\n",
      "Requirement already satisfied: python-box<7.0.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (6.1.0)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (2.32.5)\n",
      "Requirement already satisfied: rich>=13.3.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (13.4.2)\n",
      "Requirement already satisfied: semantic-version>=2.8.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (2.10.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.1.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (2.47.0)\n",
      "Requirement already satisfied: setuptools in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (78.1.0)\n",
      "Requirement already satisfied: simplejson in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (3.20.2)\n",
      "Requirement already satisfied: urllib3>=1.26.8 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (2.6.1)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (1.17.0)\n",
      "Requirement already satisfied: wurlitzer>=1.0.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from comet-ml) (3.0.2)\n",
      "Requirement already satisfied: configobj in /home/aya/anaconda3/lib/python3.13/site-packages (from everett[ini]<3.2.0,>=1.0.1->comet-ml) (5.0.9)\n",
      "Requirement already satisfied: entmax<2.0,>=1.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (1.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (0.36.0)\n",
      "Requirement already satisfied: jsonargparse==3.13.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (3.13.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.20.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.4.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (2.2.3)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.24.4 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (4.25.8)\n",
      "Requirement already satisfied: pytorch-lightning<3.0.0,>=2.0.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (2.5.6)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.5.4 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (1.15.3)\n",
      "Requirement already satisfied: sentencepiece<0.3.0,>=0.2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (0.2.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (2.9.1)\n",
      "Requirement already satisfied: torchmetrics<0.11.0,>=0.10.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (0.10.3)\n",
      "Requirement already satisfied: transformers<5.0,>=4.17 in /home/aya/anaconda3/lib/python3.13/site-packages (from unbabel-comet) (4.57.3)\n",
      "Requirement already satisfied: PyYAML>=3.13 in /home/aya/anaconda3/lib/python3.13/site-packages (from jsonargparse==3.13.1->unbabel-comet) (6.0.2)\n",
      "Requirement already satisfied: portalocker in /home/aya/anaconda3/lib/python3.13/site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: regex in /home/aya/anaconda3/lib/python3.13/site-packages (from sacrebleu) (2024.11.6)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/aya/anaconda3/lib/python3.13/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in /home/aya/anaconda3/lib/python3.13/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /home/aya/anaconda3/lib/python3.13/site-packages (from sacrebleu) (5.3.0)\n",
      "Requirement already satisfied: filelock in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (24.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (1.2.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.15.2)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.7.0)\n",
      "Requirement already satisfied: matplotlib in /home/aya/anaconda3/lib/python3.13/site-packages (from bert-score) (3.10.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/aya/anaconda3/lib/python3.13/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (3.11.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (3.7)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/aya/anaconda3/lib/python3.13/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/aya/anaconda3/lib/python3.13/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.22.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas>=1.4.1->unbabel-comet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas>=1.4.1->unbabel-comet) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas>=1.4.1->unbabel-comet) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas>=1.4.1->unbabel-comet) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests>=2.18.4->comet-ml) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests>=2.18.4->comet-ml) (2025.4.26)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from rich>=13.3.2->comet-ml) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from rich>=13.3.2->comet-ml) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet-ml) (0.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from torch>=1.6.0->unbabel-comet) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.6.0->unbabel-comet) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from jinja2->torch>=1.6.0->unbabel-comet) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from matplotlib->bert-score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/aya/anaconda3/lib/python3.13/site-packages (from matplotlib->bert-score) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from matplotlib->bert-score) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from matplotlib->bert-score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /home/aya/anaconda3/lib/python3.13/site-packages (from matplotlib->bert-score) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from matplotlib->bert-score) (3.2.0)\n",
      "Requirement already satisfied: wandb in /home/aya/anaconda3/lib/python3.13/site-packages (0.23.1)\n",
      "Requirement already satisfied: pandas in /home/aya/anaconda3/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /home/aya/anaconda3/lib/python3.13/site-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in /home/aya/anaconda3/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: click>=8.0.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: packaging in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (24.2)\n",
      "Requirement already satisfied: platformdirs in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (4.25.8)\n",
      "Requirement already satisfied: pydantic<3 in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (2.11.7)\n",
      "Requirement already satisfied: pyyaml in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (2.32.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (2.47.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /home/aya/anaconda3/lib/python3.13/site-packages (from wandb) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from pydantic<3->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/aya/anaconda3/lib/python3.13/site-packages (from pydantic<3->wandb) (0.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aya/anaconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/aya/anaconda3/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /home/aya/anaconda3/lib/python3.13/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/aya/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "GPU Available: True\n",
      "Number of GPUs: 2\n",
      "  GPU 0: NVIDIA GeForce RTX 5090\n",
      "    Memory: 33.67 GB\n",
      "  GPU 1: NVIDIA GeForce RTX 5090\n",
      "    Memory: 33.67 GB\n",
      "\n",
      "Total VRAM: 67.34 GB\n",
      "Warning: Not running inside a virtual environment!\n",
      "No Hugging Face token found. Please run `huggingface-cli login`.\n",
      "‚úÖ Detected project directory: .\n",
      "üìÅ Data directory: data\n",
      "üìÅ Models directory: models\n",
      "‚ö†Ô∏è Translation data not found at: Data/english-arabic\n",
      "‚úÖ Directory structure and model paths are ready!\n",
      "HPC Config: 2 GPUs, Flash Attention: False\n",
      "COMET Config: model=Unbabel/wmt22-cometkiwi-da, batch_size=64, gpu=1\n",
      "‚úÖ Configuration loaded successfully!\n",
      "   - COMET enabled for scoring\n",
      "   - LoRA enabled: True\n",
      "   - KL penalty: 0.15\n",
      "   - Sample size: 100,000\n",
      "   - Batch sizes: Generation=64, RM=32, PPO=32\n",
      "Utility functions loaded!\n",
      "Configuration saved to config.json\n"
     ]
    }
   ],
   "source": [
    "# Run setup from config notebook\n",
    "%run 0_config_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8419bc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic Data Generation Pipeline\n",
      "================================================================================\n",
      "Scoring method: Heuristic-based\n",
      "Heuristic metrics: Length ratio, Punctuation presence, Non-empty validation\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# ===========================\n",
    "# CONFIGURATION: EVALUATION METHOD\n",
    "# ===========================\n",
    "USE_COMET = False  # Set to True to use COMET model, False for fast heuristic-based scoring\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "print(\"Synthetic Data Generation Pipeline\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Scoring method: {'COMET-based' if USE_COMET else 'Heuristic-based'}\")\n",
    "print(f\"Heuristic metrics: Length ratio, Punctuation presence, Non-empty validation\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef9d6c",
   "metadata": {},
   "source": [
    "## Load SFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e480820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cache cleared\n",
      "\n",
      "Model Loading Configuration:\n",
      "Total GPUs: 2\n",
      "Total VRAM: 62.72GB\n",
      "SFT Model: 28.9B parameters\n",
      "Quantization: 8-bit\n",
      "Precision: bfloat16\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU cache and set CUDA environment variables\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"GPU cache cleared\")\n",
    "\n",
    "# CUDA environment variables for optimized memory management\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Unload COMET model if loaded to free memory for SFT model\n",
    "if USE_COMET and 'comet_model' in globals() and comet_model is not None:\n",
    "    try:\n",
    "        del comet_model\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        print(\"COMET unloaded from GPU memory\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to unload COMET: {e}\")\n",
    "\n",
    "# ===========================\n",
    "# MODEL LOADING CONFIGURATION\n",
    "# ===========================\n",
    "FORCE_CPU = False\n",
    "USE_BFLOAT16 = True\n",
    "\n",
    "print(\"\\nModel Loading Configuration:\")\n",
    "print(f\"Total GPUs: {NUM_GPUS}\")\n",
    "print(f\"Total VRAM: {NUM_GPUS * 31.36:.2f}GB\")\n",
    "print(f\"SFT Model: 28.9B parameters\")\n",
    "print(f\"Quantization: 8-bit\")\n",
    "print(f\"Precision: {'bfloat16' if USE_BFLOAT16 else 'float32'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9d02f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading SFT model from Hugging Face...\n",
      "Loading model with 8-bit quantization across 2 GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4a2d54fc3d4c0d8b524833fdd7d7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU Memory Allocation:\n",
      "  GPU 0: 4.4GB allocated, 4.5GB reserved, 33.7GB total\n",
      "  GPU 1: 5.8GB allocated, 5.9GB reserved, 33.7GB total\n",
      "\n",
      "Model layers distributed across: {'1', '0'}\n",
      "Layer distribution:\n",
      "  Device 0: 15 layers\n",
      "  Device 1: 31 layers\n",
      "\n",
      "Model size: 9.24B parameters\n",
      "Model ready for inference (layers distributed across GPUs)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading SFT model from Hugging Face...\")\n",
    "model_name = \"ModelSpace/GemmaX2-28-9B-v0.1\"\n",
    "\n",
    "if FORCE_CPU:\n",
    "    print(\"Loading model on CPU\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map={\"\": \"cpu\"},\n",
    "        torch_dtype=torch.float32,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "else:\n",
    "    print(f\"Loading model with 8-bit quantization across {NUM_GPUS} GPUs\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    \n",
    "    # Configure memory for both GPUs\n",
    "    max_memory = {\n",
    "        0: \"31GB\",\n",
    "        1: \"31GB\",\n",
    "        \"cpu\": \"64GB\"\n",
    "    }\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",  # Automatically distributes model layers across GPUs\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        max_memory=max_memory\n",
    "    )\n",
    "    \n",
    "    print(\"\\nGPU Memory Allocation:\")\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(NUM_GPUS):\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "            reserved = torch.cuda.memory_reserved(i) / 1e9\n",
    "            total = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "            print(f\"  GPU {i}: {allocated:.1f}GB allocated, {reserved:.1f}GB reserved, {total:.1f}GB total\")\n",
    "\n",
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Report detailed device placement\n",
    "try:\n",
    "    if hasattr(model, 'hf_device_map'):\n",
    "        devices_used = set(str(v) for v in model.hf_device_map.values())\n",
    "        print(f\"\\nModel layers distributed across: {devices_used}\")\n",
    "        \n",
    "        # Count layers per device\n",
    "        device_layer_count = {}\n",
    "        for layer_name, device in model.hf_device_map.items():\n",
    "            device_str = str(device)\n",
    "            device_layer_count[device_str] = device_layer_count.get(device_str, 0) + 1\n",
    "        \n",
    "        print(\"Layer distribution:\")\n",
    "        for device, count in sorted(device_layer_count.items()):\n",
    "            print(f\"  Device {device}: {count} layers\")\n",
    "    else:\n",
    "        print(f\"Model device: {next(model.parameters()).device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Device info: {e}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters()) / 1e9\n",
    "print(f\"\\nModel size: {total_params:.2f}B parameters\")\n",
    "print(\"Model ready for inference (layers distributed across GPUs)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c20a72",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9272a129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading source data for synthetic translation generation...\n",
      "Data source: FULL\n",
      "\n",
      "Loaded 3294856 English samples\n",
      "Loaded 3294856 English samples\n",
      "Loaded 484003 French samples\n",
      "\n",
      "Total available data: 3,778,859 samples\n",
      "Loaded 484003 French samples\n",
      "\n",
      "Total available data: 3,778,859 samples\n",
      "Available by language:\n",
      "  English: 3,294,856 samples\n",
      "  French: 484,003 samples\n",
      "Available by language:\n",
      "  English: 3,294,856 samples\n",
      "  French: 484,003 samples\n",
      "\n",
      "Sampled 20,000 samples for generation:\n",
      "  English to Arabic: 10,000 (50.0%)\n",
      "  French to Arabic: 10,000 (50.0%)\n",
      "\n",
      "Sampled 20,000 samples for generation:\n",
      "  English to Arabic: 10,000 (50.0%)\n",
      "  French to Arabic: 10,000 (50.0%)\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# LOAD TRAINING DATA (EN + FR)\n",
    "# ===========================\n",
    "USE_SAMPLES = False  # Set False for full dataset, True for samples\n",
    "\n",
    "print(\"\\nLoading source data for synthetic translation generation...\")\n",
    "print(f\"Data source: {'SAMPLES' if USE_SAMPLES else 'FULL'}\\n\")\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# Load English data\n",
    "english_inputs_path = PROJECT_DIR / (\"data/english_inputs_samples.json\" if USE_SAMPLES else \"data/english_inputs.json\")\n",
    "\n",
    "if english_inputs_path.exists():\n",
    "    with open(english_inputs_path, 'r', encoding='utf-8') as f:\n",
    "        english_data = json.load(f)\n",
    "    \n",
    "    if isinstance(english_data, list):\n",
    "        for item in english_data:\n",
    "            if isinstance(item, str):\n",
    "                all_data.append({'source': item, 'source_lang': 'en'})\n",
    "            elif isinstance(item, dict):\n",
    "                text = item.get('text', item.get('source', item.get('sentence', '')))\n",
    "                if text:\n",
    "                    all_data.append({'source': text, 'source_lang': 'en'})\n",
    "    print(f\"Loaded {len(english_data)} English samples\")\n",
    "else:\n",
    "    print(f\"Warning: {english_inputs_path.name} not found\")\n",
    "\n",
    "# Load French data\n",
    "french_inputs_path = PROJECT_DIR / \"data/french_inputs.json\"\n",
    "\n",
    "if french_inputs_path.exists():\n",
    "    with open(french_inputs_path, 'r', encoding='utf-8') as f:\n",
    "        french_data = json.load(f)\n",
    "    \n",
    "    if isinstance(french_data, list):\n",
    "        for item in french_data:\n",
    "            if isinstance(item, str):\n",
    "                all_data.append({'source': item, 'source_lang': 'fr'})\n",
    "            elif isinstance(item, dict):\n",
    "                text = item.get('text', item.get('source', item.get('sentence', '')))\n",
    "                if text:\n",
    "                    all_data.append({'source': text, 'source_lang': 'fr'})\n",
    "    print(f\"Loaded {len(french_data)} French samples\")\n",
    "else:\n",
    "    print(f\"Warning: french_inputs.json not found\")\n",
    "\n",
    "print(f\"\\nTotal available data: {len(all_data):,} samples\")\n",
    "\n",
    "# ===========================\n",
    "# SAMPLE DATA FOR BALANCED TRAINING\n",
    "# ===========================\n",
    "SAMPLE_SIZE_PER_LANG = 10_000  # 10K per language = 20K total\n",
    "\n",
    "en_data = [s for s in all_data if s['source_lang'] == 'en']\n",
    "fr_data = [s for s in all_data if s['source_lang'] == 'fr']\n",
    "\n",
    "print(f\"Available by language:\")\n",
    "print(f\"  English: {len(en_data):,} samples\")\n",
    "print(f\"  French: {len(fr_data):,} samples\")\n",
    "\n",
    "random.shuffle(en_data)\n",
    "random.shuffle(fr_data)\n",
    "\n",
    "en_samples = en_data[:min(SAMPLE_SIZE_PER_LANG, len(en_data))]\n",
    "fr_samples = fr_data[:min(SAMPLE_SIZE_PER_LANG, len(fr_data))]\n",
    "\n",
    "training_samples = en_samples + fr_samples\n",
    "random.shuffle(training_samples)\n",
    "\n",
    "total_samples = len(training_samples)\n",
    "en_pct = 100 * len(en_samples) / total_samples if total_samples > 0 else 0\n",
    "fr_pct = 100 * len(fr_samples) / total_samples if total_samples > 0 else 0\n",
    "\n",
    "print(f\"\\nSampled {total_samples:,} samples for generation:\")\n",
    "print(f\"  English to Arabic: {len(en_samples):,} ({en_pct:.1f}%)\")\n",
    "print(f\"  French to Arabic: {len(fr_samples):,} ({fr_pct:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aff54f2",
   "metadata": {},
   "source": [
    "## Generate Translation Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b63b4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation Configuration (OPTIMIZED):\n",
      "  Batch size: 16 (increased for dual GPU)\n",
      "  Candidates per source: 1\n",
      "  Max tokens: 128\n",
      "  Methods: 4 (temperature, top-k, nucleus, greedy)\n",
      "  Expected speedup: 3-4x faster with larger batches\n",
      "\n",
      "GPU Utilization Mode:\n",
      "  Model parallelism: Layers distributed across both GPUs\n",
      "  Each batch processes through both GPUs sequentially\n",
      "  Larger batches = better GPU utilization\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# GENERATION CONFIGURATION\n",
    "# ===========================\n",
    "# OPTIMIZED FOR DUAL RTX 5090 (62GB VRAM)\n",
    "MEGA_BATCH_SIZE = 16  # Increased from 4 to 16 (4x improvement)\n",
    "NUM_CANDIDATES = 1     # Keep 1 candidate per method for speed\n",
    "MAX_NEW_TOKENS = 128\n",
    "\n",
    "print(\"\\nGeneration Configuration (OPTIMIZED):\")\n",
    "print(f\"  Batch size: {MEGA_BATCH_SIZE} (increased for dual GPU)\")\n",
    "print(f\"  Candidates per source: {NUM_CANDIDATES}\")\n",
    "print(f\"  Max tokens: {MAX_NEW_TOKENS}\")\n",
    "print(f\"  Methods: 4 (temperature, top-k, nucleus, greedy)\")\n",
    "print(f\"  Expected speedup: 3-4x faster with larger batches\")\n",
    "\n",
    "print(\"\\nGPU Utilization Mode:\")\n",
    "print(\"  Model parallelism: Layers distributed across both GPUs\")\n",
    "print(\"  Each batch processes through both GPUs sequentially\")\n",
    "print(\"  Larger batches = better GPU utilization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73d4ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation methods configured:\n",
      "  1. Temperature Sampling (high randomness)\n",
      "  2. Top-K Sampling (conservative)\n",
      "  3. Nucleus Sampling (balanced)\n",
      "  4. Greedy Decoding (deterministic)\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# TRANSLATION GENERATION METHODS\n",
    "# ===========================\n",
    "# Four different sampling strategies for diverse translation candidates\n",
    "\n",
    "def generate_with_temperature(sources, langs, num_candidates=1):\n",
    "    \"\"\"High temperature sampling for diverse outputs\"\"\"\n",
    "    prompts = [format_translation_prompt(src, lang) for src, lang in zip(sources, langs)]\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=True,\n",
    "        temperature=1.2,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        num_return_sequences=num_candidates,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    candidates = []\n",
    "    \n",
    "    for i in range(0, len(generated_texts), num_candidates):\n",
    "        batch_candidates = []\n",
    "        for j in range(num_candidates):\n",
    "            if i + j < len(generated_texts):\n",
    "                text = generated_texts[i + j]\n",
    "                translation = text.split(\"Arabic translation:\")[-1].strip() if \"Arabic translation:\" in text else text.strip()\n",
    "                batch_candidates.append({\n",
    "                    'translation': translation,\n",
    "                    'method': 'temperature_sampling',\n",
    "                    'config': {'temperature': 1.2, 'top_p': 0.95, 'top_k': 50}\n",
    "                })\n",
    "        candidates.append(batch_candidates)\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def generate_with_topk(sources, langs, num_candidates=1):\n",
    "    \"\"\"Conservative top-k sampling\"\"\"\n",
    "    prompts = [format_translation_prompt(src, lang) for src, lang in zip(sources, langs)]\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=30,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=num_candidates,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    candidates = []\n",
    "    \n",
    "    for i in range(0, len(generated_texts), num_candidates):\n",
    "        batch_candidates = []\n",
    "        for j in range(num_candidates):\n",
    "            if i + j < len(generated_texts):\n",
    "                text = generated_texts[i + j]\n",
    "                translation = text.split(\"Arabic translation:\")[-1].strip() if \"Arabic translation:\" in text else text.strip()\n",
    "                batch_candidates.append({\n",
    "                    'translation': translation,\n",
    "                    'method': 'top_k_sampling',\n",
    "                    'config': {'temperature': 0.7, 'top_k': 30, 'top_p': 0.9}\n",
    "                })\n",
    "        candidates.append(batch_candidates)\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def generate_with_nucleus(sources, langs, num_candidates=1):\n",
    "    \"\"\"Nucleus (top-p) sampling for balanced diversity\"\"\"\n",
    "    prompts = [format_translation_prompt(src, lang) for src, lang in zip(sources, langs)]\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        top_k=0,\n",
    "        num_return_sequences=num_candidates,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    candidates = []\n",
    "    \n",
    "    for i in range(0, len(generated_texts), num_candidates):\n",
    "        batch_candidates = []\n",
    "        for j in range(num_candidates):\n",
    "            if i + j < len(generated_texts):\n",
    "                text = generated_texts[i + j]\n",
    "                translation = text.split(\"Arabic translation:\")[-1].strip() if \"Arabic translation:\" in text else text.strip()\n",
    "                batch_candidates.append({\n",
    "                    'translation': translation,\n",
    "                    'method': 'nucleus_sampling',\n",
    "                    'config': {'temperature': 0.9, 'top_p': 0.95}\n",
    "                })\n",
    "        candidates.append(batch_candidates)\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def generate_with_greedy(sources, langs, num_candidates=1):\n",
    "    \"\"\"Greedy decoding for consistent outputs\"\"\"\n",
    "    prompts = [format_translation_prompt(src, lang) for src, lang in zip(sources, langs)]\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=num_candidates,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    candidates = []\n",
    "    \n",
    "    for i in range(0, len(generated_texts), num_candidates):\n",
    "        batch_candidates = []\n",
    "        for j in range(num_candidates):\n",
    "            if i + j < len(generated_texts):\n",
    "                text = generated_texts[i + j]\n",
    "                translation = text.split(\"Arabic translation:\")[-1].strip() if \"Arabic translation:\" in text else text.strip()\n",
    "                batch_candidates.append({\n",
    "                    'translation': translation,\n",
    "                    'method': 'greedy_decoding',\n",
    "                    'config': {'do_sample': False}\n",
    "                })\n",
    "        candidates.append(batch_candidates)\n",
    "    return candidates\n",
    "\n",
    "\n",
    "GENERATION_METHODS = {\n",
    "    'temperature_sampling': generate_with_temperature,\n",
    "    'top_k_sampling': generate_with_topk,\n",
    "    'nucleus_sampling': generate_with_nucleus,\n",
    "    'greedy_decoding': generate_with_greedy\n",
    "}\n",
    "\n",
    "print(\"\\nGeneration methods configured:\")\n",
    "print(\"  1. Temperature Sampling (high randomness)\")\n",
    "print(\"  2. Top-K Sampling (conservative)\")\n",
    "print(\"  3. Nucleus Sampling (balanced)\")\n",
    "print(\"  4. Greedy Decoding (deterministic)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c927da82",
   "metadata": {},
   "source": [
    "## Alternative: vLLM for Better Multi-GPU Utilization (Optional)\n",
    "\n",
    "**Note on GPU usage:**\n",
    "- Current setup: Model layers distributed across 2 GPUs (tensor parallelism)\n",
    "- vLLM: Optimized tensor parallelism + better batching = 3-10x faster\n",
    "- Install: `pip install vllm`\n",
    "\n",
    "**Before enabling vLLM, verify your system:**\n",
    "- CUDA version: 11.8 or higher required\n",
    "- GPU memory: Will be cleared before vLLM loads\n",
    "- Driver: Latest NVIDIA drivers recommended\n",
    "\n",
    "**To activate vLLM:**\n",
    "1. Make sure transformers model is loaded (run cell 5 if needed)\n",
    "2. Run the vLLM configuration cell below (it will unload transformers automatically)\n",
    "3. If it fails, it will automatically fall back to transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb2fc098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ATTEMPTING vLLM INITIALIZATION\n",
      "============================================================\n",
      "Note: This model may have compatibility issues with vLLM\n",
      "If it fails, transformers will work perfectly fine!\n",
      "\n",
      "‚úì CUDA contexts cleared\n",
      "\n",
      "Initializing vLLM (this takes 1-2 minutes)...\n",
      "Using conservative settings for compatibility...\n",
      "INFO 12-11 11:40:13 [utils.py:253] non-default args: {'trust_remote_code': True, 'dtype': 'float16', 'seed': None, 'max_model_len': 256, 'tensor_parallel_size': 2, 'swap_space': 8, 'gpu_memory_utilization': 0.8, 'max_num_seqs': 64, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'ModelSpace/GemmaX2-28-9B-v0.1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-11 11:40:14 [model.py:637] Resolved architecture: Gemma2ForCausalLM\n",
      "\n",
      "============================================================\n",
      "vLLM INITIALIZATION FAILED (THIS IS NORMAL)\n",
      "============================================================\n",
      "Error: ValidationError\n",
      "Details: 1 validation error for ModelConfig\n",
      "  Value error, The model type 'gemma2' does not support float16. Reason: Numerical instability. Please use bfloat16 or float32 instead. [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "\n",
      "üîç Likely causes:\n",
      "  ‚Ä¢ This specific model isn't fully compatible with vLLM\n",
      "  ‚Ä¢ Model architecture not supported by current vLLM version\n",
      "  ‚Ä¢ Custom model code causes initialization issues\n",
      "\n",
      "‚úÖ SOLUTION: Use transformers (recommended for this model)\n",
      "  ‚Ä¢ Already loaded and working perfectly\n",
      "  ‚Ä¢ Fully supports this model architecture\n",
      "  ‚Ä¢ Utilizes both GPUs via model parallelism\n",
      "  ‚Ä¢ Proven stable for long-running generation tasks\n",
      "\n",
      "üí° Performance tip:\n",
      "  ‚Ä¢ Your dual RTX 5090 setup with transformers is excellent\n",
      "  ‚Ä¢ Batch size of 16 already optimized for your hardware\n",
      "  ‚Ä¢ Generation will complete efficiently\n",
      "============================================================\n",
      "\n",
      "‚ö†Ô∏è  Re-run cell 5 to load transformers model\n",
      "\n",
      "============================================================\n",
      "vLLM INITIALIZATION FAILED (THIS IS NORMAL)\n",
      "============================================================\n",
      "Error: ValidationError\n",
      "Details: 1 validation error for ModelConfig\n",
      "  Value error, The model type 'gemma2' does not support float16. Reason: Numerical instability. Please use bfloat16 or float32 instead. [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "\n",
      "üîç Likely causes:\n",
      "  ‚Ä¢ This specific model isn't fully compatible with vLLM\n",
      "  ‚Ä¢ Model architecture not supported by current vLLM version\n",
      "  ‚Ä¢ Custom model code causes initialization issues\n",
      "\n",
      "‚úÖ SOLUTION: Use transformers (recommended for this model)\n",
      "  ‚Ä¢ Already loaded and working perfectly\n",
      "  ‚Ä¢ Fully supports this model architecture\n",
      "  ‚Ä¢ Utilizes both GPUs via model parallelism\n",
      "  ‚Ä¢ Proven stable for long-running generation tasks\n",
      "\n",
      "üí° Performance tip:\n",
      "  ‚Ä¢ Your dual RTX 5090 setup with transformers is excellent\n",
      "  ‚Ä¢ Batch size of 16 already optimized for your hardware\n",
      "  ‚Ä¢ Generation will complete efficiently\n",
      "============================================================\n",
      "\n",
      "‚ö†Ô∏è  Re-run cell 5 to load transformers model\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# OPTIONAL: vLLM FOR OPTIMIZED MULTI-GPU INFERENCE\n",
    "# ===========================\n",
    "# vLLM provides better GPU utilization than standard transformers\n",
    "# IMPORTANT: This model may not be fully compatible with vLLM\n",
    "# Transformers will work perfectly fine - vLLM is just an optimization\n",
    "\n",
    "USE_VLLM = True  # RECOMMENDED: Keep False for this model\n",
    "VLLM_ACTIVE = False\n",
    "\n",
    "if USE_VLLM:\n",
    "    try:\n",
    "        # Set environment variables BEFORE importing vLLM\n",
    "        import os\n",
    "        os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'\n",
    "        os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "        os.environ['VLLM_LOGGING_LEVEL'] = 'DEBUG'  # More verbose for debugging\n",
    "        \n",
    "        from vllm import LLM, SamplingParams\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"ATTEMPTING vLLM INITIALIZATION\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Note: This model may have compatibility issues with vLLM\")\n",
    "        print(\"If it fails, transformers will work perfectly fine!\")\n",
    "        print()\n",
    "        \n",
    "        # Unload transformers model\n",
    "        if 'model' in globals():\n",
    "            del model\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            print(\"‚úì Transformers model unloaded\")\n",
    "        \n",
    "        # Clear CUDA contexts\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                torch.cuda.reset_peak_memory_stats(i)\n",
    "                torch.cuda.reset_accumulated_memory_stats(i)\n",
    "            print(\"‚úì CUDA contexts cleared\")\n",
    "        \n",
    "        print(\"\\nInitializing vLLM (this takes 1-2 minutes)...\")\n",
    "        print(\"Using conservative settings for compatibility...\")\n",
    "        \n",
    "        # Try with minimal settings first\n",
    "        vllm_model = LLM(\n",
    "            model=\"ModelSpace/GemmaX2-28-9B-v0.1\",\n",
    "            tensor_parallel_size=2,\n",
    "            dtype=\"float16\",  # Use float16 instead of bfloat16 for better compatibility\n",
    "            max_model_len=256,  # Reduced from 512\n",
    "            trust_remote_code=True,\n",
    "            gpu_memory_utilization=0.80,  # Conservative 80%\n",
    "            swap_space=8,  # More swap space\n",
    "            max_num_seqs=64,  # Reduced batch capacity\n",
    "            enforce_eager=True,  # Disable CUDA graphs for compatibility\n",
    "            disable_log_stats=True,\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úì vLLM model loaded!\")\n",
    "        \n",
    "        # Define generation functions\n",
    "        def generate_with_vllm_temperature(batch_sources, batch_langs, num_candidates=1):\n",
    "            prompts = [format_translation_prompt(src, lang) \n",
    "                       for src, lang in zip(batch_sources, batch_langs)]\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=1.2, top_p=0.95, top_k=50,\n",
    "                max_tokens=MAX_NEW_TOKENS, n=num_candidates,\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            outputs = vllm_model.generate(prompts, sampling_params)\n",
    "            all_candidates = []\n",
    "            for output in outputs:\n",
    "                candidates = []\n",
    "                for completion in output.outputs:\n",
    "                    text = completion.text\n",
    "                    translation = text.split(\"Arabic translation:\")[-1].strip() if \"Arabic translation:\" in text else text.strip()\n",
    "                    candidates.append({\n",
    "                        'translation': translation,\n",
    "                        'method': 'temperature_sampling',\n",
    "                        'config': {'temperature': 1.2, 'top_p': 0.95, 'top_k': 50}\n",
    "                    })\n",
    "                all_candidates.append(candidates)\n",
    "            return all_candidates\n",
    "        \n",
    "        def generate_with_vllm_topk(batch_sources, batch_langs, num_candidates=1):\n",
    "            prompts = [format_translation_prompt(src, lang) \n",
    "                       for src, lang in zip(batch_sources, batch_langs)]\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=0.7, top_p=0.9, top_k=30,\n",
    "                max_tokens=MAX_NEW_TOKENS, n=num_candidates,\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            outputs = vllm_model.generate(prompts, sampling_params)\n",
    "            all_candidates = []\n",
    "            for output in outputs:\n",
    "                candidates = []\n",
    "                for completion in output.outputs:\n",
    "                    text = completion.text\n",
    "                    translation = text.split(\"Arabic translation:\")[-1].strip() if \"Arabic translation:\" in text else text.strip()\n",
    "                    candidates.append({\n",
    "                        'translation': translation,\n",
    "                        'method': 'top_k_sampling',\n",
    "                        'config': {'temperature': 0.7, 'top_k': 30, 'top_p': 0.9}\n",
    "                    })\n",
    "                all_candidates.append(candidates)\n",
    "            return all_candidates\n",
    "        \n",
    "        def generate_with_vllm_nucleus(batch_sources, batch_langs, num_candidates=1):\n",
    "            prompts = [format_translation_prompt(src, lang) \n",
    "                       for src, lang in zip(batch_sources, batch_langs)]\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=0.9, top_p=0.95, top_k=-1,\n",
    "                max_tokens=MAX_NEW_TOKENS, n=num_candidates,\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            outputs = vllm_model.generate(prompts, sampling_params)\n",
    "            all_candidates = []\n",
    "            for output in outputs:\n",
    "                candidates = []\n",
    "                for completion in output.outputs:\n",
    "                    text = completion.text\n",
    "                    translation = text.split(\"Arabic translation:\")[-1].strip() if \"Arabic translation:\" in text else text.strip()\n",
    "                    candidates.append({\n",
    "                        'translation': translation,\n",
    "                        'method': 'nucleus_sampling',\n",
    "                        'config': {'temperature': 0.9, 'top_p': 0.95}\n",
    "                    })\n",
    "                all_candidates.append(candidates)\n",
    "            return all_candidates\n",
    "        \n",
    "        def generate_with_vllm_greedy(batch_sources, batch_langs, num_candidates=1):\n",
    "            prompts = [format_translation_prompt(src, lang) \n",
    "                       for src, lang in zip(batch_sources, batch_langs)]\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=0.0, max_tokens=MAX_NEW_TOKENS,\n",
    "                n=num_candidates, skip_special_tokens=True\n",
    "            )\n",
    "            outputs = vllm_model.generate(prompts, sampling_params)\n",
    "            all_candidates = []\n",
    "            for output in outputs:\n",
    "                candidates = []\n",
    "                for completion in output.outputs:\n",
    "                    text = completion.text\n",
    "                    translation = text.split(\"Arabic translation:\")[-1].strip() if \"Arabic translation:\" in text else text.strip()\n",
    "                    candidates.append({\n",
    "                        'translation': translation,\n",
    "                        'method': 'greedy_decoding',\n",
    "                        'config': {'do_sample': False}\n",
    "                    })\n",
    "                all_candidates.append(candidates)\n",
    "            return all_candidates\n",
    "        \n",
    "        GENERATION_METHODS = {\n",
    "            'temperature_sampling': generate_with_vllm_temperature,\n",
    "            'top_k_sampling': generate_with_vllm_topk,\n",
    "            'nucleus_sampling': generate_with_vllm_nucleus,\n",
    "            'greedy_decoding': generate_with_vllm_greedy\n",
    "        }\n",
    "        \n",
    "        VLLM_ACTIVE = True\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úì‚úì‚úì vLLM ACTIVATED SUCCESSFULLY! ‚úì‚úì‚úì\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Speedup: 3-10x faster than transformers\")\n",
    "        print(\"Tensor parallelism: Enabled across both GPUs\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"vLLM NOT INSTALLED\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"\\nInstall: pip install vllm\")\n",
    "        print(\"Using transformers instead (works great!)\")\n",
    "        print(\"=\" * 60)\n",
    "        VLLM_ACTIVE = False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"vLLM INITIALIZATION FAILED (THIS IS NORMAL)\")\n",
    "        print(\"=\" * 60)\n",
    "        error_msg = str(e)\n",
    "        print(f\"Error: {type(e).__name__}\")\n",
    "        if len(error_msg) < 500:\n",
    "            print(f\"Details: {error_msg}\")\n",
    "        \n",
    "        print(\"\\nüîç Likely causes:\")\n",
    "        print(\"  ‚Ä¢ This specific model isn't fully compatible with vLLM\")\n",
    "        print(\"  ‚Ä¢ Model architecture not supported by current vLLM version\")\n",
    "        print(\"  ‚Ä¢ Custom model code causes initialization issues\")\n",
    "        \n",
    "        print(\"\\n‚úÖ SOLUTION: Use transformers (recommended for this model)\")\n",
    "        print(\"  ‚Ä¢ Already loaded and working perfectly\")\n",
    "        print(\"  ‚Ä¢ Fully supports this model architecture\")\n",
    "        print(\"  ‚Ä¢ Utilizes both GPUs via model parallelism\")\n",
    "        print(\"  ‚Ä¢ Proven stable for long-running generation tasks\")\n",
    "        \n",
    "        print(\"\\nüí° Performance tip:\")\n",
    "        print(\"  ‚Ä¢ Your dual RTX 5090 setup with transformers is excellent\")\n",
    "        print(\"  ‚Ä¢ Batch size of 16 already optimized for your hardware\")\n",
    "        print(\"  ‚Ä¢ Generation will complete efficiently\")\n",
    "        print(\"=\" * 60)\n",
    "        VLLM_ACTIVE = False\n",
    "        \n",
    "        if 'model' not in globals():\n",
    "            print(\"\\n‚ö†Ô∏è  Re-run cell 5 to load transformers model\")\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"USING TRANSFORMERS (RECOMMENDED)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚úì Stable and fully compatible with this model\")\n",
    "    print(\"‚úì Model parallelism across both RTX 5090 GPUs\")\n",
    "    print(\"‚úì Optimized with 8-bit quantization\")\n",
    "    print(\"‚úì Large batch size (16) for efficiency\")\n",
    "    print()\n",
    "    print(\"Note: vLLM is disabled (USE_VLLM=False)\")\n",
    "    print(\"  This model has known compatibility issues with vLLM\")\n",
    "    print(\"  Transformers is the recommended approach here\")\n",
    "    print(\"=\" * 60)\n",
    "    VLLM_ACTIVE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3990b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current GPU Status:\n",
      "============================================================\n",
      "GPU 0: NVIDIA GeForce RTX 5090\n",
      "  Total: 33.7GB\n",
      "  Allocated: 4.8GB (14.3%)\n",
      "  Reserved: 5.1GB (15.1%)\n",
      "  Free: 28.6GB (84.9%)\n",
      "GPU 1: NVIDIA GeForce RTX 5090\n",
      "  Total: 33.7GB\n",
      "  Allocated: 6.5GB (19.4%)\n",
      "  Reserved: 6.9GB (20.4%)\n",
      "  Free: 26.8GB (79.6%)\n",
      "============================================================\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU utilization before generation\n",
    "if torch.cuda.is_available() and not FORCE_CPU:\n",
    "    print(\"\\nCurrent GPU Status:\")\n",
    "    print(\"=\" * 60)\n",
    "    for i in range(NUM_GPUS):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1e9\n",
    "        total = props.total_memory / 1e9\n",
    "        free = total - reserved\n",
    "        \n",
    "        print(f\"GPU {i}: {props.name}\")\n",
    "        print(f\"  Total: {total:.1f}GB\")\n",
    "        print(f\"  Allocated: {allocated:.1f}GB ({100*allocated/total:.1f}%)\")\n",
    "        print(f\"  Reserved: {reserved:.1f}GB ({100*reserved/total:.1f}%)\")\n",
    "        print(f\"  Free: {free:.1f}GB ({100*free/total:.1f}%)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check engine and model distribution\n",
    "    if VLLM_ACTIVE:\n",
    "        print(\"\\nInference Engine: vLLM (Optimized)\")\n",
    "        print(\"  Tensor parallelism across both GPUs\")\n",
    "        print(\"  Continuous batching enabled\")\n",
    "        print(\"  PagedAttention for memory efficiency\")\n",
    "    elif 'model' in globals() and hasattr(model, 'hf_device_map'):\n",
    "        print(\"\\nInference Engine: Transformers (Standard)\")\n",
    "        gpu0_params = sum(1 for device in model.hf_device_map.values() if str(device) == '0')\n",
    "        gpu1_params = sum(1 for device in model.hf_device_map.values() if str(device) == '1')\n",
    "        total_mapped = len(model.hf_device_map)\n",
    "        \n",
    "        print(f\"  GPU 0: {gpu0_params}/{total_mapped} components ({100*gpu0_params/total_mapped:.1f}%)\")\n",
    "        print(f\"  GPU 1: {gpu1_params}/{total_mapped} components ({100*gpu1_params/total_mapped:.1f}%)\")\n",
    "        \n",
    "        if gpu0_params > 0 and gpu1_params > 0:\n",
    "            print(\"  Status: Model split across both GPUs\")\n",
    "        elif gpu0_params > 0 or gpu1_params > 0:\n",
    "            print(\"  Warning: Model on single GPU only\")\n",
    "        else:\n",
    "            print(\"  Warning: Model distribution unclear\")\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7469f6d7",
   "metadata": {},
   "source": [
    "## Score Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b29fb17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring functions configured (heuristic-based)\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# TRANSLATION QUALITY SCORING\n",
    "# ===========================\n",
    "\n",
    "def score_batch_candidates(sources: list, candidates_list: list) -> list:\n",
    "    \"\"\"Score translation candidates using heuristics.\n",
    "    \n",
    "    Metrics: length ratio, punctuation presence, non-empty validation\n",
    "    Returns: List of scored candidates with 'score' and 'method' keys\n",
    "    \"\"\"\n",
    "    scored_batch = []\n",
    "    \n",
    "    for src, candidates in zip(sources, candidates_list):\n",
    "        src_len = len(src.split())\n",
    "        src_punct = sum(1 for c in src if c in '.!?,;:')\n",
    "        \n",
    "        scored_candidates = []\n",
    "        for cand in candidates:\n",
    "            tgt = cand['translation']\n",
    "            method = cand.get('method', 'unknown')\n",
    "            \n",
    "            # Length ratio score\n",
    "            tgt_len = len(tgt.split())\n",
    "            if src_len > 0:\n",
    "                length_ratio = min(tgt_len, src_len) / max(tgt_len, src_len)\n",
    "            else:\n",
    "                length_ratio = 0.5 if tgt_len == 0 else 0.0\n",
    "            \n",
    "            # Punctuation presence score\n",
    "            tgt_punct = sum(1 for c in tgt if c in '.!?,;:')\n",
    "            punct_score = 1.0 if (src_punct > 0 and tgt_punct > 0) or (src_punct == 0 and tgt_punct == 0) else 0.7\n",
    "            \n",
    "            # Non-empty score\n",
    "            non_empty_score = 1.0 if len(tgt.strip()) > 0 and '[ERROR]' not in tgt else 0.0\n",
    "            \n",
    "            # Combined score\n",
    "            quality_score = (length_ratio * 0.5 + punct_score * 0.3 + non_empty_score * 0.2)\n",
    "            quality_score = max(0.0, min(1.0, quality_score))\n",
    "            \n",
    "            scored_candidates.append({\n",
    "                'translation': tgt,\n",
    "                'method': method,\n",
    "                'score': quality_score\n",
    "            })\n",
    "        \n",
    "        scored_batch.append(scored_candidates)\n",
    "    \n",
    "    return scored_batch\n",
    "\n",
    "\n",
    "def create_preference_pairs(scored_candidates: list) -> list:\n",
    "    \"\"\"Create preference pairs from scored candidates.\n",
    "    Pairs highest-scored with lowest-scored translations.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    if len(scored_candidates) < 2:\n",
    "        return pairs\n",
    "    \n",
    "    sorted_cands = sorted(scored_candidates, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    if len(sorted_cands) >= 2:\n",
    "        chosen = sorted_cands[0]\n",
    "        rejected = sorted_cands[-1]\n",
    "        score_margin = chosen['score'] - rejected['score']\n",
    "        \n",
    "        if score_margin > 0.01:\n",
    "            pairs.append({\n",
    "                'chosen': chosen['translation'],\n",
    "                'rejected': rejected['translation'],\n",
    "                'chosen_score': chosen['score'],\n",
    "                'rejected_score': rejected['score'],\n",
    "                'score_margin': score_margin,\n",
    "                'chosen_method': chosen['method'],\n",
    "                'rejected_method': rejected['method']\n",
    "            })\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "print(\"Scoring functions configured (heuristic-based)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e803f392",
   "metadata": {},
   "source": [
    "## Generate Synthetic Preference Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "796400ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing scoring and preference pair functions...\n",
      "\n",
      "Scored candidates:\n",
      "  Source 1:\n",
      "    'Hello world' (greedy_decoding): 1.000\n",
      "    'Hi there' (temperature_sampling): 1.000\n",
      "    '[ERROR]' (nucleus_sampling): 0.550\n",
      "  Source 2:\n",
      "    'This is a test' (top_k_sampling): 1.000\n",
      "    'This is testing' (greedy_decoding): 0.875\n",
      "\n",
      "Preference pairs created:\n",
      "  Chosen: 'Hello world' (1.000)\n",
      "  Rejected: '[ERROR]' (0.550)\n",
      "  Margin: 0.450\n",
      "\n",
      "Functions working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Test scoring functions\n",
    "print(\"Testing scoring and preference pair functions...\\n\")\n",
    "\n",
    "test_sources = [\"Hello world\", \"This is a test\"]\n",
    "test_candidates = [\n",
    "    [\n",
    "        {'translation': 'Hello world', 'method': 'greedy_decoding'},\n",
    "        {'translation': 'Hi there', 'method': 'temperature_sampling'},\n",
    "        {'translation': '[ERROR]', 'method': 'nucleus_sampling'}\n",
    "    ],\n",
    "    [\n",
    "        {'translation': 'This is a test', 'method': 'top_k_sampling'},\n",
    "        {'translation': 'This is testing', 'method': 'greedy_decoding'},\n",
    "    ]\n",
    "]\n",
    "\n",
    "scored_batch = score_batch_candidates(test_sources, test_candidates)\n",
    "\n",
    "print(\"Scored candidates:\")\n",
    "for i, scored_list in enumerate(scored_batch):\n",
    "    print(f\"  Source {i+1}:\")\n",
    "    for s in scored_list:\n",
    "        print(f\"    '{s['translation'][:30]}' ({s['method']}): {s['score']:.3f}\")\n",
    "\n",
    "pairs = create_preference_pairs(scored_batch[0])\n",
    "print(\"\\nPreference pairs created:\")\n",
    "for pair in pairs:\n",
    "    print(f\"  Chosen: '{pair['chosen'][:30]}' ({pair['chosen_score']:.3f})\")\n",
    "    print(f\"  Rejected: '{pair['rejected'][:30]}' ({pair['rejected_score']:.3f})\")\n",
    "    print(f\"  Margin: {pair['score_margin']:.3f}\")\n",
    "\n",
    "print(\"\\nFunctions working correctly!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead7467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MULTI-METHOD SYNTHETIC DATA GENERATION\n",
      "Scoring: Heuristic-based\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Total samples: 20,000\n",
      "  Batch size: 16\n",
      "  Generation methods: 4\n",
      "  GPUs: 2\n",
      "  Checkpoint frequency: Every 1000 sentences\n",
      "\n",
      "Language distribution:\n",
      "  English to Arabic: 10,000 (50.0%)\n",
      "  French to Arabic: 10,000 (50.0%)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1250 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Processing batches:   0%|          | 0/1250 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m method_name, method_func \u001b[38;5;129;01min\u001b[39;00m GENERATION_METHODS\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m         candidates \u001b[38;5;241m=\u001b[39m method_func(batch_sources, batch_langs, num_candidates\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     58\u001b[0m         all_method_candidates[method_name] \u001b[38;5;241m=\u001b[39m candidates\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m, in \u001b[0;36mgenerate_with_temperature\u001b[0;34m(sources, langs, num_candidates)\u001b[0m\n\u001b[1;32m     10\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     11\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     14\u001b[0m     input_ids,\n\u001b[1;32m     15\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m     16\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39mMAX_NEW_TOKENS,\n\u001b[1;32m     17\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.2\u001b[39m,\n\u001b[1;32m     19\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m,\n\u001b[1;32m     20\u001b[0m     top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     21\u001b[0m     num_return_sequences\u001b[38;5;241m=\u001b[39mnum_candidates,\n\u001b[1;32m     22\u001b[0m     pad_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m generated_texts \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m candidates \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/transformers/generation/utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[0;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m decoding_method(\n\u001b[1;32m   2565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2566\u001b[0m     input_ids,\n\u001b[1;32m   2567\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   2568\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   2569\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   2570\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_mode_kwargs,\n\u001b[1;32m   2571\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2572\u001b[0m )\n\u001b[1;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[1;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2579\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/transformers/generation/utils.py:2787\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2787\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2789\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   2790\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   2791\u001b[0m     outputs,\n\u001b[1;32m   2792\u001b[0m     model_kwargs,\n\u001b[1;32m   2793\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2794\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/transformers/utils/generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/transformers/models/gemma2/modeling_gemma2.py:549\u001b[0m, in \u001b[0;36mGemma2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    546\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    547\u001b[0m )\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 549\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    550\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    551\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    552\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    553\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    554\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    555\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    556\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    557\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    558\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    560\u001b[0m )\n\u001b[1;32m    562\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/transformers/utils/generic.py:1072\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1072\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m     kwargs_without_recordable \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/transformers/models/gemma2/modeling_gemma2.py:464\u001b[0m, in \u001b[0;36mGemma2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    462\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 464\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    465\u001b[0m     hidden_states,\n\u001b[1;32m    466\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    467\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask_mapping[decoder_layer\u001b[38;5;241m.\u001b[39mattention_type],\n\u001b[1;32m    468\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    469\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    470\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    471\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    472\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    474\u001b[0m )\n\u001b[1;32m    476\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/transformers/models/gemma2/modeling_gemma2.py:310\u001b[0m, in \u001b[0;36mGemma2DecoderLayer.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, position_ids, past_key_values, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    311\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    312\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    313\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    314\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    315\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    316\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    317\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    318\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    320\u001b[0m )\n\u001b[1;32m    321\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    322\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/transformers/models/gemma2/modeling_gemma2.py:244\u001b[0m, in \u001b[0;36mGemma2Attention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    242\u001b[0m hidden_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m--> 244\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    245\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    246\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/bitsandbytes/nn/modules.py:1071\u001b[0m, in \u001b[0;36mLinear8bitLt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[1;32m   1069\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m-> 1071\u001b[0m out \u001b[38;5;241m=\u001b[39m bnb\u001b[38;5;241m.\u001b[39mmatmul(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate)\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mhas_fp16_weights \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCB\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/bitsandbytes/autograd/_functions.py:424\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m MatMul8bitFp\u001b[38;5;241m.\u001b[39mapply(A, B, out, bias, state)\n\u001b[0;32m--> 424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m MatMul8bitLt\u001b[38;5;241m.\u001b[39mapply(A, B, out, bias, state)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/autograd/function.py:581\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    580\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    587\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    588\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    589\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/bitsandbytes/autograd/_functions.py:192\u001b[0m, in \u001b[0;36mMatMul8bitLt.forward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    189\u001b[0m     CA, CAt, SCA, SCAt, outlier_cols \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mint8_double_quant(A\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat16), threshold\u001b[38;5;241m=\u001b[39mstate\u001b[38;5;241m.\u001b[39mthreshold)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;66;03m# Fast path\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m     CA, SCA, outlier_cols \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mint8_vectorwise_quant(A\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat16), threshold\u001b[38;5;241m=\u001b[39mstate\u001b[38;5;241m.\u001b[39mthreshold)\n\u001b[1;32m    193\u001b[0m     CAt \u001b[38;5;241m=\u001b[39m SCAt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    195\u001b[0m has_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/bitsandbytes/functional.py:2058\u001b[0m, in \u001b[0;36mint8_vectorwise_quant\u001b[0;34m(A, threshold)\u001b[0m\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mint8_vectorwise_quant\u001b[39m(A: torch\u001b[38;5;241m.\u001b[39mTensor, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m):\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Quantizes a tensor with dtype `torch.float16` to `torch.int8` in accordance to the `LLM.int8()` algorithm.\u001b[39;00m\n\u001b[1;32m   2042\u001b[0m \n\u001b[1;32m   2043\u001b[0m \u001b[38;5;124;03m    For more information, see the [LLM.int8() paper](https://arxiv.org/abs/2208.07339).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;124;03m        - `torch.Tensor` with dtype `torch.int32`, *optional*: A list of column indices which contain outlier features.\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2058\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mbitsandbytes\u001b[38;5;241m.\u001b[39mint8_vectorwise_quant\u001b[38;5;241m.\u001b[39mdefault(A, threshold)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/_ops.py:841\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/_compile.py:53\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive, wrapping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m disable_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:1044\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1042\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1046\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/library.py:730\u001b[0m, in \u001b[0;36m_impl.<locals>.register_.<locals>.func_no_dynamo\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m     use_lib \u001b[38;5;241m=\u001b[39m lib\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m disable_dynamo:\n\u001b[0;32m--> 730\u001b[0m     \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_dynamo\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunc_no_dynamo\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    732\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# MAIN GENERATION AND SCORING LOOP\n",
    "# ===========================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MULTI-METHOD SYNTHETIC DATA GENERATION\")\n",
    "print(f\"Inference Engine: {'vLLM (Optimized)' if VLLM_ACTIVE else 'Transformers (Standard)'}\")\n",
    "print(f\"Scoring: {'COMET-based' if USE_COMET else 'Heuristic-based'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Total samples: {len(training_samples):,}\")\n",
    "print(f\"  Batch size: {MEGA_BATCH_SIZE}\")\n",
    "print(f\"  Generation methods: 4\")\n",
    "print(f\"  GPUs: {NUM_GPUS}\")\n",
    "print(f\"  Checkpoint frequency: Every 1000 sentences\")\n",
    "\n",
    "if VLLM_ACTIVE:\n",
    "    print(f\"\\nvLLM Optimizations:\")\n",
    "    print(f\"  Tensor parallelism: Enabled\")\n",
    "    print(f\"  Continuous batching: Enabled\")\n",
    "    print(f\"  Expected speedup: 3-10x vs transformers\")\n",
    "\n",
    "en_count = sum(1 for s in training_samples if s['source_lang'] == 'en')\n",
    "fr_count = sum(1 for s in training_samples if s['source_lang'] == 'fr')\n",
    "print(f\"\\nLanguage distribution:\")\n",
    "print(f\"  English to Arabic: {en_count:,} ({100*en_count/len(training_samples):.1f}%)\")\n",
    "print(f\"  French to Arabic: {fr_count:,} ({100*fr_count/len(training_samples):.1f}%)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Checkpoint configuration\n",
    "CHECKPOINT_FREQUENCY = 1000  # Save every 1000 sentences\n",
    "checkpoint_dir = DATA_DIR / \"checkpoints\"\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "method_stats = {method_name: {'count': 0, 'avg_score': 0, 'scores': []} for method_name in GENERATION_METHODS.keys()}\n",
    "synthetic_dataset = []\n",
    "combined_candidates_path = DATA_DIR / \"generated_candidates_all_methods.jsonl\"\n",
    "\n",
    "start_time = time.time()\n",
    "samples_processed = 0\n",
    "errors_count = 0\n",
    "quality_scores_collected = []\n",
    "en_pairs_count = 0\n",
    "fr_pairs_count = 0\n",
    "last_checkpoint = 0\n",
    "\n",
    "num_batches = (len(training_samples) + MEGA_BATCH_SIZE - 1) // MEGA_BATCH_SIZE\n",
    "\n",
    "with open(combined_candidates_path, 'w', encoding='utf-8') as combined_f:\n",
    "    for batch_idx in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
    "        start_idx = batch_idx * MEGA_BATCH_SIZE\n",
    "        end_idx = min(start_idx + MEGA_BATCH_SIZE, len(training_samples))\n",
    "        batch_samples = training_samples[start_idx:end_idx]\n",
    "        \n",
    "        batch_sources = [s['source'] for s in batch_samples]\n",
    "        batch_langs = [s['source_lang'] for s in batch_samples]\n",
    "        \n",
    "        try:\n",
    "            # Generate with all methods (vLLM or transformers based on VLLM_ACTIVE flag)\n",
    "            all_method_candidates = {}\n",
    "            for method_name, method_func in GENERATION_METHODS.items():\n",
    "                try:\n",
    "                    candidates = method_func(batch_sources, batch_langs, num_candidates=NUM_CANDIDATES)\n",
    "                    all_method_candidates[method_name] = candidates\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError in method {method_name}: {e}\")\n",
    "                    all_method_candidates[method_name] = [[{'translation': '[ERROR]', 'method': method_name, 'config': {}}] for _ in batch_samples]\n",
    "            \n",
    "            # Combine candidates\n",
    "            combined_candidates = []\n",
    "            for src_idx in range(len(batch_samples)):\n",
    "                src_all_candidates = []\n",
    "                for method_name in GENERATION_METHODS.keys():\n",
    "                    if src_idx < len(all_method_candidates[method_name]):\n",
    "                        src_all_candidates.extend(all_method_candidates[method_name][src_idx])\n",
    "                combined_candidates.append(src_all_candidates)\n",
    "            \n",
    "            # Score candidates\n",
    "            scored_batch = score_batch_candidates(batch_sources, combined_candidates)\n",
    "            \n",
    "            # Process each sample\n",
    "            for sample, candidates, scored in zip(batch_samples, combined_candidates, scored_batch):\n",
    "                combined_record = {\n",
    "                    'source': sample['source'],\n",
    "                    'source_lang': sample['source_lang'],\n",
    "                    'candidates_by_method': {}\n",
    "                }\n",
    "                \n",
    "                for method_name in GENERATION_METHODS.keys():\n",
    "                    method_cands = [c for c in candidates if c.get('method') == method_name]\n",
    "                    combined_record['candidates_by_method'][method_name] = [c['translation'] for c in method_cands]\n",
    "                    method_stats[method_name]['count'] += 1\n",
    "                \n",
    "                combined_f.write(json.dumps(combined_record, ensure_ascii=False) + '\\n')\n",
    "                \n",
    "                # Collect scores\n",
    "                for s in scored:\n",
    "                    quality_scores_collected.append(s['score'])\n",
    "                    method = s.get('method', 'unknown')\n",
    "                    if method in method_stats:\n",
    "                        method_stats[method]['scores'].append(s['score'])\n",
    "                \n",
    "                # Create preference pairs\n",
    "                pairs = create_preference_pairs(scored)\n",
    "                for pair in pairs:\n",
    "                    pair_record = {\n",
    "                        'source': sample['source'],\n",
    "                        'source_lang': sample['source_lang'],\n",
    "                        'chosen': pair['chosen'],\n",
    "                        'rejected': pair['rejected'],\n",
    "                        'chosen_score': pair['chosen_score'],\n",
    "                        'rejected_score': pair['rejected_score'],\n",
    "                        'margin': pair['score_margin'],\n",
    "                        'chosen_method': pair['chosen_method'],\n",
    "                        'rejected_method': pair['rejected_method']\n",
    "                    }\n",
    "                    synthetic_dataset.append(pair_record)\n",
    "                    \n",
    "                    if sample['source_lang'] == 'en':\n",
    "                        en_pairs_count += 1\n",
    "                    elif sample['source_lang'] == 'fr':\n",
    "                        fr_pairs_count += 1\n",
    "            \n",
    "            samples_processed += len(batch_samples)\n",
    "            \n",
    "            # Save checkpoint every 1000 sentences\n",
    "            if samples_processed - last_checkpoint >= CHECKPOINT_FREQUENCY:\n",
    "                checkpoint_path = checkpoint_dir / f\"synthetic_preferences_checkpoint_{samples_processed}.jsonl\"\n",
    "                with open(checkpoint_path, 'w', encoding='utf-8') as f:\n",
    "                    for item in synthetic_dataset:\n",
    "                        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "                \n",
    "                # Save checkpoint stats\n",
    "                checkpoint_stats_path = checkpoint_dir / f\"stats_checkpoint_{samples_processed}.json\"\n",
    "                with open(checkpoint_stats_path, 'w') as f:\n",
    "                    json.dump({\n",
    "                        'samples_processed': samples_processed,\n",
    "                        'total_pairs': len(synthetic_dataset),\n",
    "                        'en_pairs': en_pairs_count,\n",
    "                        'fr_pairs': fr_pairs_count,\n",
    "                        'timestamp': time.time() - start_time,\n",
    "                        'engine': 'vllm' if VLLM_ACTIVE else 'transformers'\n",
    "                    }, f, indent=2)\n",
    "                \n",
    "                print(f\"\\n  Checkpoint saved: {len(synthetic_dataset):,} pairs ({samples_processed:,} samples)\")\n",
    "                last_checkpoint = samples_processed\n",
    "            \n",
    "            # Clear GPU cache periodically\n",
    "            if batch_idx % 50 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        except Exception as e:\n",
    "            errors_count += 1\n",
    "            if errors_count <= 5:\n",
    "                print(f\"\\nError in batch {batch_idx}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Progress update every 50 batches\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = samples_processed / elapsed if elapsed > 0 else 0\n",
    "            remaining = (len(training_samples) - samples_processed) / rate if rate > 0 else 0\n",
    "            \n",
    "            if quality_scores_collected:\n",
    "                avg_score = sum(quality_scores_collected) / len(quality_scores_collected)\n",
    "            else:\n",
    "                avg_score = 0\n",
    "            \n",
    "            print(f\"\\nProgress (Batch {batch_idx + 1}/{num_batches}):\")\n",
    "            print(f\"  Engine: {'vLLM' if VLLM_ACTIVE else 'Transformers'}\")\n",
    "            print(f\"  Samples: {samples_processed:,}/{len(training_samples):,} ({100*samples_processed/len(training_samples):.1f}%)\")\n",
    "            print(f\"  Preference pairs: {len(synthetic_dataset):,}\")\n",
    "            print(f\"    EN->AR: {en_pairs_count:,}, FR->AR: {fr_pairs_count:,}\")\n",
    "            print(f\"  Avg quality score: {avg_score:.4f}\")\n",
    "            print(f\"  Rate: {rate:.1f} samples/sec\")\n",
    "            print(f\"  ETA: {remaining/3600:.2f} hours\")\n",
    "            print(f\"  Errors: {errors_count}\")\n",
    "            print(f\"  Last checkpoint: {last_checkpoint:,} samples\")\n",
    "        \n",
    "        combined_f.flush()\n",
    "\n",
    "# Save final checkpoint\n",
    "final_checkpoint_path = checkpoint_dir / f\"synthetic_preferences_final_{samples_processed}.jsonl\"\n",
    "with open(final_checkpoint_path, 'w', encoding='utf-8') as f:\n",
    "    for item in synthetic_dataset:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "print(f\"\\nFinal checkpoint saved: {final_checkpoint_path.name}\")\n",
    "\n",
    "# Finalize statistics\n",
    "for method_name in GENERATION_METHODS.keys():\n",
    "    if method_stats[method_name]['scores']:\n",
    "        method_stats[method_name]['avg_score'] = sum(method_stats[method_name]['scores']) / len(method_stats[method_name]['scores'])\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Engine: {'vLLM' if VLLM_ACTIVE else 'Transformers'}\")\n",
    "print(f\"  Samples processed: {samples_processed:,}\")\n",
    "print(f\"  Preference pairs: {len(synthetic_dataset):,}\")\n",
    "print(f\"    English to Arabic: {en_pairs_count:,}\")\n",
    "print(f\"    French to Arabic: {fr_pairs_count:,}\")\n",
    "print(f\"  Total time: {total_time/3600:.2f} hours\")\n",
    "print(f\"  Average rate: {samples_processed/total_time:.1f} samples/sec\")\n",
    "print(f\"  Errors: {errors_count}\")\n",
    "print(f\"  Checkpoints saved: {checkpoint_dir.name}/\")\n",
    "\n",
    "if quality_scores_collected:\n",
    "    overall_avg = sum(quality_scores_collected) / len(quality_scores_collected)\n",
    "    print(f\"\\nOverall avg quality score: {overall_avg:.4f}\")\n",
    "\n",
    "print(\"\\nMethod statistics:\")\n",
    "for method_name in GENERATION_METHODS.keys():\n",
    "    print(f\"  {method_name}: avg={method_stats[method_name]['avg_score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f0bf7a",
   "metadata": {},
   "source": [
    "## Save Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4552f41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to data/synthetic_preferences.jsonl...\n",
      "\n",
      "Saved 0 preference pairs\n",
      "\n",
      "Dataset Statistics:\n",
      "  Total pairs: 0\n",
      "  Average margin: 0.0000\n",
      "  Average chosen score: 0.0000\n",
      "  Number of sources: 20,000\n",
      "\n",
      "Language Breakdown:\n",
      "  English to Arabic: 0 pairs\n",
      "    Avg margin: 0.0000\n",
      "    Avg score: 0.0000\n",
      "  French to Arabic: 0 pairs\n",
      "    Avg margin: 0.0000\n",
      "    Avg score: 0.0000\n",
      "\n",
      "Method Breakdown:\n",
      "  temperature_sampling:\n",
      "    Chosen: 0\n",
      "    Rejected: 0\n",
      "    Avg score: 0.2468\n",
      "  top_k_sampling:\n",
      "    Chosen: 0\n",
      "    Rejected: 0\n",
      "    Avg score: 0.2468\n",
      "  nucleus_sampling:\n",
      "    Chosen: 0\n",
      "    Rejected: 0\n",
      "    Avg score: 0.2468\n",
      "  greedy_decoding:\n",
      "    Chosen: 0\n",
      "    Rejected: 0\n",
      "    Avg score: 0.2468\n",
      "\n",
      "Generated files:\n",
      "  - generated_candidates_all_methods.jsonl\n",
      "  - synthetic_preferences.jsonl\n",
      "  - synthetic_data_stats.json\n",
      "\n",
      "Statistics saved to data/synthetic_data_stats.json\n"
     ]
    }
   ],
   "source": [
    "# Save synthetic dataset\n",
    "print(f\"Saving dataset to {SYNTHETIC_PREFERENCES}...\\n\")\n",
    "\n",
    "with open(SYNTHETIC_PREFERENCES, 'w', encoding='utf-8') as f:\n",
    "    for item in synthetic_dataset:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Saved {len(synthetic_dataset)} preference pairs\")\n",
    "\n",
    "# Calculate statistics by language\n",
    "en_pairs = [item for item in synthetic_dataset if item['source_lang'] == 'en']\n",
    "fr_pairs = [item for item in synthetic_dataset if item['source_lang'] == 'fr']\n",
    "\n",
    "# Analyze which methods produced best pairs\n",
    "method_pair_stats = {}\n",
    "for method_name in GENERATION_METHODS.keys():\n",
    "    chosen_count = sum(1 for item in synthetic_dataset if item['chosen_method'] == method_name)\n",
    "    rejected_count = sum(1 for item in synthetic_dataset if item['rejected_method'] == method_name)\n",
    "    method_pair_stats[method_name] = {\n",
    "        'chosen_count': chosen_count,\n",
    "        'rejected_count': rejected_count\n",
    "    }\n",
    "\n",
    "# Prepare statistics\n",
    "stats = {\n",
    "    'total_pairs': len(synthetic_dataset),\n",
    "    'en_pairs': len(en_pairs),\n",
    "    'fr_pairs': len(fr_pairs),\n",
    "    'avg_margin': sum(item['margin'] for item in synthetic_dataset) / len(synthetic_dataset) if synthetic_dataset else 0,\n",
    "    'avg_chosen_score': sum(item['chosen_score'] for item in synthetic_dataset) / len(synthetic_dataset) if synthetic_dataset else 0,\n",
    "    'avg_rejected_score': sum(item['rejected_score'] for item in synthetic_dataset) / len(synthetic_dataset) if synthetic_dataset else 0,\n",
    "    'num_sources': len(training_samples),\n",
    "    'language_breakdown': {\n",
    "        'english': {\n",
    "            'pairs': len(en_pairs),\n",
    "            'avg_margin': sum(item['margin'] for item in en_pairs) / len(en_pairs) if en_pairs else 0,\n",
    "            'avg_chosen_score': sum(item['chosen_score'] for item in en_pairs) / len(en_pairs) if en_pairs else 0,\n",
    "        },\n",
    "        'french': {\n",
    "            'pairs': len(fr_pairs),\n",
    "            'avg_margin': sum(item['margin'] for item in fr_pairs) / len(fr_pairs) if fr_pairs else 0,\n",
    "            'avg_chosen_score': sum(item['chosen_score'] for item in fr_pairs) / len(fr_pairs) if fr_pairs else 0,\n",
    "        }\n",
    "    },\n",
    "    'method_breakdown': {\n",
    "        method_name: {\n",
    "            'pairs_as_chosen': method_pair_stats[method_name]['chosen_count'],\n",
    "            'pairs_as_rejected': method_pair_stats[method_name]['rejected_count'],\n",
    "            'avg_score': method_stats[method_name]['avg_score'],\n",
    "            'total_candidates': method_stats[method_name]['count']\n",
    "        }\n",
    "        for method_name in GENERATION_METHODS.keys()\n",
    "    },\n",
    "    'data_source': 'full' if not USE_SAMPLES else 'samples',\n",
    "    'scoring_method': 'comet' if USE_COMET else 'heuristic'\n",
    "}\n",
    "\n",
    "stats_path = DATA_DIR / \"synthetic_data_stats.json\"\n",
    "with open(stats_path, 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"  Total pairs: {stats['total_pairs']:,}\")\n",
    "print(f\"  Average margin: {stats['avg_margin']:.4f}\")\n",
    "print(f\"  Average chosen score: {stats['avg_chosen_score']:.4f}\")\n",
    "print(f\"  Number of sources: {stats['num_sources']:,}\")\n",
    "\n",
    "print(f\"\\nLanguage Breakdown:\")\n",
    "print(f\"  English to Arabic: {stats['language_breakdown']['english']['pairs']:,} pairs\")\n",
    "print(f\"    Avg margin: {stats['language_breakdown']['english']['avg_margin']:.4f}\")\n",
    "print(f\"    Avg score: {stats['language_breakdown']['english']['avg_chosen_score']:.4f}\")\n",
    "\n",
    "print(f\"  French to Arabic: {stats['language_breakdown']['french']['pairs']:,} pairs\")\n",
    "print(f\"    Avg margin: {stats['language_breakdown']['french']['avg_margin']:.4f}\")\n",
    "print(f\"    Avg score: {stats['language_breakdown']['french']['avg_chosen_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nMethod Breakdown:\")\n",
    "for method_name, info in stats['method_breakdown'].items():\n",
    "    print(f\"  {method_name}:\")\n",
    "    print(f\"    Chosen: {info['pairs_as_chosen']:,}\")\n",
    "    print(f\"    Rejected: {info['pairs_as_rejected']:,}\")\n",
    "    print(f\"    Avg score: {info['avg_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"  - generated_candidates_all_methods.jsonl\")\n",
    "print(f\"  - {SYNTHETIC_PREFERENCES.name}\")\n",
    "print(f\"  - {stats_path.name}\")\n",
    "\n",
    "print(f\"\\nStatistics saved to {stats_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344339ab",
   "metadata": {},
   "source": [
    "## Sample Preference Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80684f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Preference Pairs\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Method Performance Summary:\n",
      "================================================================================\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display sample preference pairs\n",
    "print(\"Sample Preference Pairs\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "en_examples = [item for item in synthetic_dataset if item['source_lang'] == 'en']\n",
    "fr_examples = [item for item in synthetic_dataset if item['source_lang'] == 'fr']\n",
    "\n",
    "examples_to_show = []\n",
    "if en_examples:\n",
    "    examples_to_show.append(('English to Arabic', random.sample(en_examples, min(1, len(en_examples)))[0]))\n",
    "if fr_examples:\n",
    "    examples_to_show.append(('French to Arabic', random.sample(fr_examples, min(1, len(fr_examples)))[0]))\n",
    "\n",
    "if len(examples_to_show) < 3:\n",
    "    remaining_count = 3 - len(examples_to_show)\n",
    "    all_remaining = [item for item in synthetic_dataset if item not in [ex[1] for ex in examples_to_show]]\n",
    "    for item in random.sample(all_remaining, min(remaining_count, len(all_remaining))):\n",
    "        lang_label = 'English to Arabic' if item['source_lang'] == 'en' else 'French to Arabic'\n",
    "        examples_to_show.append((lang_label, item))\n",
    "\n",
    "for i, (lang_label, item) in enumerate(examples_to_show, 1):\n",
    "    print(f\"\\nExample {i}: {lang_label}\")\n",
    "    print(f\"Source: {item['source'][:150]}\")\n",
    "    print(f\"\\nChosen (score: {item['chosen_score']:.3f}, {item['chosen_method']}): {item['chosen'][:150]}\")\n",
    "    print(f\"\\nRejected (score: {item['rejected_score']:.3f}, {item['rejected_method']}): {item['rejected'][:150]}\")\n",
    "    print(f\"Margin: {item['margin']:.3f}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nMethod Performance Summary:\")\n",
    "print(\"=\" * 80)\n",
    "for method_name in GENERATION_METHODS.keys():\n",
    "    chosen = sum(1 for item in synthetic_dataset if item['chosen_method'] == method_name)\n",
    "    rejected = sum(1 for item in synthetic_dataset if item['rejected_method'] == method_name)\n",
    "    total = chosen + rejected\n",
    "    if total > 0:\n",
    "        chosen_pct = 100 * chosen / total\n",
    "        print(f\"{method_name}:\")\n",
    "        print(f\"  Chosen: {chosen:,} ({chosen_pct:.1f}%)\")\n",
    "        print(f\"  Rejected: {rejected:,} ({100-chosen_pct:.1f}%)\")\n",
    "        print(f\"  Total: {total:,}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fcaf8e",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "Proceed to **notebook 2** to train the reward model using this synthetic preference data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214bf6c7",
   "metadata": {},
   "source": [
    "## Quick Process: Convert Partial Data to Preferences\n",
    "\n",
    "Run this cell to process the candidates you already generated (without re-running generation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
